<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 61]
- [quant-ph](#quant-ph) [Total: 43]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 4]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.AI](#cs.AI) [Total: 42]
- [nlin.CD](#nlin.CD) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 20]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression](https://arxiv.org/abs/2512.15721)
*Sveinung Myhre*

Main category: cs.LG

TL;DR: DiscoverDCP框架结合符号回归与DCP规则集进行系统辨识，确保发现的模型表达式全局凸，避免后验凸性验证的计算负担


<details>
  <summary>Details</summary>
Motivation: 传统固定参数凸表达式（如二次函数）在准确性和灵活性方面受限，而系统辨识中发现的模型需要进行凸性验证，这一过程计算复杂且难以保证全局凸性

Method: 将符号回归与Disciplined Convex Programming（DCP）规则集集成，强制所有候选模型表达式遵守DCP组合规则，确保输出表达式在构造上就是全局凸的

Result: 该方法能够发现比传统固定参数凸表达式更宽松、更准确的凸替代模型，产生可解释、可验证且灵活的凸模型

Conclusion: DiscoverDCP框架为安全关键控制和优化任务提供了有效的系统辨识方法，通过构造保证凸性，避免了后验验证的计算复杂性

Abstract: We propose DiscoverDCP, a data-driven framework that integrates symbolic regression with the rule sets of Disciplined Convex Programming (DCP) to perform system identification. By enforcing that all discovered candidate model expressions adhere to DCP composition rules, we ensure that the output expressions are globally convex by construction, circumventing the computationally intractable process of post-hoc convexity verification. This approach allows for the discovery of convex surrogates that exhibit more relaxed and accurate functional forms than traditional fixed-parameter convex expressions (e.g., quadratic functions). The proposed method produces interpretable, verifiable, and flexible convex models suitable for safety-critical control and optimization tasks.

</details>


### [2] [KAN-Matrix: Visualizing Nonlinear Pairwise and Multivariate Contributions for Physical Insight](https://arxiv.org/abs/2512.15755)
*Luis A. De la Fuente,Hernan A. Moreno,Laura V. Alvarez,Hoshin V. Gupta*

Main category: cs.LG

TL;DR: 该论文提出使用Kolmogorov-Arnold Networks (KANs) 来增强高维数据集的解释性，开发了两种可视化工具：PKAN用于分析变量间的非线性关联，MKAN用于量化输入变量对目标变量的贡献度。


<details>
  <summary>Details</summary>
Motivation: 科学家在处理复杂数据集时面临高维度和变量共线性的挑战，传统相关性分析方法在解释性和简约性方面存在局限，需要更强大的工具来揭示变量间的非线性关系和相对重要性。

Method: 提出基于Kolmogorov-Arnold Networks (KANs) 的两种可视化工具：Pairwise KAN Matrix (PKAN) 用于表征变量对之间的非线性关联，Multivariate KAN Contribution Matrix (MKAN) 作为非线性特征排序工具，量化输入变量在预测目标变量时的相对贡献。

Result: 通过实验比较，PKAN和MKAN比Pearson相关性和互信息方法产生更稳健和信息丰富的结果，能够捕捉关系的强度和函数形式，有助于发现隐藏的物理模式。

Conclusion: PKAN和MKAN工具支持模型开发工作流程中的预处理和后处理任务，促进基于领域知识的模型开发，为复杂数据集的解释提供了更强大的分析框架。

Abstract: Interpreting complex datasets remains a major challenge for scientists, particularly due to high dimensionality and collinearity among variables. We introduce a novel application of Kolmogorov-Arnold Networks (KANs) to enhance interpretability and parsimony beyond what traditional correlation analyses offer. We present two interpretable, color-coded visualization tools: the Pairwise KAN Matrix (PKAN) and the Multivariate KAN Contribution Matrix (MKAN). PKAN characterizes nonlinear associations between pairs of variables, while MKAN serves as a nonlinear feature-ranking tool that quantifies the relative contributions of inputs in predicting a target variable. These tools support pre-processing (e.g., feature selection, redundancy analysis) and post-processing (e.g., model explanation, physical insights) in model development workflows. Through experimental comparisons, we demonstrate that PKAN and MKAN yield more robust and informative results than Pearson Correlation and Mutual Information. By capturing the strength and functional forms of relationships, these matrices facilitate the discovery of hidden physical patterns and promote domain-informed model development.

</details>


### [3] [Hybrid Quantum-Classical Ensemble Learning for S\&P 500 Directional Prediction](https://arxiv.org/abs/2512.15738)
*Abraham Itzhak Weinberg*

Main category: cs.LG

TL;DR: 本文提出了一种结合量子情感分析、决策变换器和策略模型选择的混合集成框架，在S&P 500预测中实现了60.14%的方向准确率，比单个模型提高了3.10%。


<details>
  <summary>Details</summary>
Motivation: 金融市场预测是机器学习的一个挑战性应用，由于高噪声、非平稳性和市场效率，大多数模型难以超过55-57%的准确率。现有方法存在三个主要限制：架构多样性不足、情感分析能力有限、以及弱预测器的负面影响。

Method: 1. 混合集成框架结合量子情感分析（4量子比特变分量子电路）、决策变换器架构和策略模型选择
2. 架构多样性策略：结合不同学习算法（LSTM、决策变换器、XGBoost、随机森林、逻辑回归）在同一数据上
3. 智能过滤：排除准确率低于52%的弱预测器
4. 使用Top-7模型进行集成，而非全部35个模型

Result: 1. 在S&P 500预测中达到60.14%的方向准确率，比单个模型提高3.10%
2. 架构多样性优于数据集多样性：不同算法在同一数据上（60.14%）优于相同架构在多个数据集上（52.80%）
3. 量子情感分析为每个模型提供+0.8%到+1.5%的增益
4. 智能过滤显著提升性能：Top-7模型60.14% vs 全部35个模型51.2%
5. McNemar检验证实统计显著性（p<0.05）
6. 初步回测显示夏普比率1.2 vs 买入持有策略0.8

Conclusion: 该混合集成框架通过架构多样性、量子增强情感分析和智能模型选择，显著提升了金融市场预测的准确率，具有实际交易潜力。架构多样性比数据集多样性更为重要，量子计算在金融情感分析中显示出实用价值。

Abstract: Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\% directional accuracy on S\&P 500 prediction, a 3.10\% improvement over individual models.
  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\% vs.\ 52.80\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\% to +1.5\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\%$), improving ensemble performance (Top-7 models: 60.14\% vs.\ all 35 models: 51.2\%).
  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.

</details>


### [4] [Pattern recognition in complex systems via vector-field representations of spatio-temporal data](https://arxiv.org/abs/2512.16763)
*Ingrid Amaranta Membrillo Solis,Maria van Rossem,Tristan Madeleine,Tetiana Orlova,Nina Podoliak,Giampaolo D'Alessandro,Jacek Brodzki,Malgosia Kaczmarek*

Main category: cs.LG

TL;DR: 该论文提出了一种基于离散测度空间上向量场理论的几何框架，用于分析复杂系统的时空数据，通过引入双参数度量族解决了传统方法在处理高维非线性数据时的局限性。


<details>
  <summary>Details</summary>
Motivation: 复杂系统（如大脑、细胞、气候、经济等）具有高维非线性动态特性，传统建模方法难以处理。虽然信息技术进步使得数据驱动方法成为可能，但时空数据的庞大和复杂性阻碍了传统降维、相空间重构和吸引子表征等方法的有效性。

Method: 提出基于离散测度空间上向量场理论的几何框架，引入适用于数据分析和机器学习应用的双参数度量族。该框架支持时间相关图像、图像梯度以及定义在图和单纯复形上的实值或向量值函数。

Result: 使用生物和物理系统在平坦和弯曲域上的数值模拟数据进行验证，结果表明所提出的度量结合多维尺度分析能有效解决关键分析挑战，实现降维、模态分解、相空间重构和吸引子表征。

Conclusion: 该框架为理解复杂动力系统提供了稳健途径，特别是在传统建模不切实际但实验数据丰富的背景下，为复杂系统分析提供了新的工具和方法。

Abstract: A complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth's climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.

</details>


### [5] [SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference](https://arxiv.org/abs/2512.15742)
*Jeff Smith*

Main category: cs.LG

TL;DR: SHARe-KAN框架通过增益-形状-偏置向量量化解决KANs内存墙问题，实现88倍运行时内存减少并保持基线精度


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KANs)面临基本的内存墙问题：其学习的基础函数导致参数数量激增，带来极高的带宽需求，阻碍了在内存受限环境中的部署。传统剪枝方法在Vision KANs上失效，因为其全息拓扑结构使得信息分布在样条干涉中而非局部化到特定边。

Method: 提出SHARe-KAN框架，采用增益-形状-偏置向量量化(Gain-Shape-Bias Vector Quantization)来利用函数冗余同时保持密集拓扑结构。配合LUTHAM硬件感知编译器进行静态内存规划。

Result: 在PASCAL VOC数据集上实现88倍运行时内存减少（从1.13GB降至12.91MB），同时匹配未压缩基线精度。在NVIDIA Ampere架构上分析显示>90%的L2缓存驻留率，证明工作负载已从基于样条架构固有的DRAM带宽约束中解耦。

Conclusion: SHARe-KAN框架成功解决了KANs的内存墙问题，通过向量量化技术有效压缩模型，结合硬件感知编译器优化内存使用，使得KANs能够在内存受限环境中部署，同时保持原始精度。

Abstract: Kolmogorov-Arnold Networks (KANs) face a fundamental memory wall: their learned basis functions create parameter counts that impose extreme bandwidth demands, hindering deployment in memory-constrained environments. We show that Vision KANs exhibit a holographic topology, where information is distributed across the interference of splines rather than localized to specific edges. Consequently, traditional pruning fails (10% sparsity degrades mAP from 85.23% to 45%, a $\sim$40-point drop). To address this, we present SHARe-KAN, a framework utilizing Gain-Shape-Bias Vector Quantization to exploit functional redundancy while preserving the dense topology. Coupled with LUTHAM, a hardware-aware compiler with static memory planning, we achieve $88\times$ runtime memory reduction (1.13 GB $\to$ 12.91 MB) and match uncompressed baseline accuracy on PASCAL VOC. Profiling on NVIDIA Ampere architecture confirms $>90\%$ L2 cache residency, demonstrating that the workload is decoupled from DRAM bandwidth constraints inherent to spline-based architectures.

</details>


### [6] [LLaDA2.0: Scaling Up Diffusion Language Models to 100B](https://arxiv.org/abs/2512.15745)
*Tiwei Bie,Maosong Cao,Kun Chen,Lun Du,Mingliang Gong,Zhuochen Gong,Yanmei Gu,Jiaqi Hu,Zenan Huang,Zhenzhong Lan,Chengxi Li,Chongxuan Li,Jianguo Li,Zehuan Li,Huabin Liu,Ling Liu,Guoshan Lu,Xiaocheng Lu,Yuxin Ma,Jianfeng Tan,Lanning Wei,Ji-Rong Wen,Yipeng Xing,Xiaolu Zhang,Junbo Zhao,Da Zheng,Jun Zhou,Junlin Zhou,Zhanchao Zhou,Liwang Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: LLaDA2.0是一个基于离散扩散的大型语言模型系列，通过系统化转换自回归模型实现，包含16B和100B参数的MoE变体，支持并行解码，性能优异且已开源。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型解码效率低，而从头训练扩散模型成本高昂。本文旨在通过转换预训练AR模型为离散扩散模型，实现知识继承、渐进适应和效率优化的前沿部署。

Method: 采用3阶段块级WSD训练方案：1)渐进增加块大小的块扩散预热；2)大规模全序列扩散稳定训练；3)回归紧凑块大小扩散衰减。配合SFT和DPO后训练对齐，生成16B和100B参数的MoE变体。

Result: 成功开发出LLaDA2.0-mini（16B）和LLaDA2.0-flash（100B）两个指令调优MoE变体，保留了并行解码优势，在前沿规模上实现了卓越的性能和效率，模型已开源。

Conclusion: LLaDA2.0建立了一个新的前沿规模部署范式，通过系统化转换AR模型为离散扩散模型，实现了知识继承、效率优化和性能提升，为大规模语言模型部署提供了高效解决方案。

Abstract: This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.

</details>


### [7] [A Unified Generative-Predictive Framework for Deterministic Inverse Design](https://arxiv.org/abs/2512.15746)
*Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: Janus框架：统一生成-预测架构，用于异质材料微结构的逆设计，通过解耦潜空间实现高效物理解释性生成


<details>
  <summary>Details</summary>
Motivation: 异质材料微结构逆设计是病态且计算昂贵的问题，现有生成模型难以支持快速、稳定的确定性物理解释性逆推

Method: Janus框架：深度编码器-解码器架构耦合可分离的KHRONOS预测头，学习同时适用于生成逆推和物理预测的等距潜流形，通过联合目标诱导潜空间解耦

Result: MNIST验证：高保真重建、准确分类和多样化生成逆推；热导率标记微结构：前向预测R²=0.98，重建误差<5%，逆解满足目标属性1%误差，潜流形平滑遍历

Conclusion: Janus通过统一预测和生成于单一潜空间，实现实时、物理解释性逆微结构生成，计算成本低于传统优化方法

Abstract: Inverse design of heterogeneous material microstructures is a fundamentally ill-posed and famously computationally expensive problem. This is exacerbated by the high-dimensional design spaces associated with finely resolved images, multimodal input property streams, and a highly nonlinear forward physics. Whilst modern generative models excel at accurately modeling such complex forward behavior, most of them are not intrinsically structured to support fast, stable \emph{deterministic} inversion with a physics-informed bias. This work introduces Janus, a unified generative-predictive framework to address this problem. Janus couples a deep encoder-decoder architecture with a predictive KHRONOS head, a separable neural architecture. Topologically speaking, Janus learns a latent manifold simultaneously isometric for generative inversion and pruned for physical prediction; the joint objective inducing \emph{disentanglement} of the latent space. Janus is first validated on the MNIST dataset, demonstrating high-fidelity reconstruction, accurate classification and diverse generative inversion of all ten target classes. It is then applied to the inverse design of heterogeneous microstructures labeled with thermal conductivity. It achieves a forward prediction accuracy $R^2=0.98$ (2\% relative error) and sub-5\% pixelwise reconstruction error. Inverse solutions satisfy target properties to within $1\%$ relative error. Inverting a sweep through properties reveal smooth traversal of the latent manifold, and UMAP visualization confirms the emergence of a low-dimensional, disentangled manifold. By unifying prediction and generation within a single latent space, Janus enables real-time, physics-informed inverse microstructure generation at a lower computational cost typically associated with classical optimization-based approaches.

</details>


### [8] [D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models](https://arxiv.org/abs/2512.15747)
*Javon Hickmon*

Main category: cs.LG

TL;DR: 本文提出D3G方法，一种无需训练、零样本的技术，通过生成多样化人口统计数据来提升CLIP等预训练多模态模型的图像分类准确性，同时减少人口统计偏见。


<details>
  <summary>Details</summary>
Motivation: 图像分类任务中，低容量模型容易欠拟合，在细粒度分类上表现不佳。同时，数据集通常缺乏平衡的人口统计分布，导致模型预测偏向多数类别，产生有害偏见。现有方法需要高质量、多样化的跨模态数据，但这往往难以生成。

Method: 提出Diverse Demographic Data Generation (D3G)方法，这是一种无需训练、零样本的方法。使用CLIP作为基础多模态模型，Stable Diffusion XL作为生成模型，在推理时生成多样化的人口统计数据来提升分类准确性并减少偏见。

Result: 实验证明，在推理时提供多样化人口统计数据能够提升模型性能。研究还探讨了不同人口统计特征对最终准确率指标的影响，验证了该方法在减少偏见方面的有效性。

Conclusion: D3G方法能够有效提升预训练多模态模型的图像分类准确性，同时减少人口统计偏见，为解决零样本图像分类中的偏见问题提供了一种无需训练的有效解决方案。

Abstract: Image classification is a task essential for machine perception to achieve human-level image understanding. Multimodal models such as CLIP have been able to perform well on this task by learning semantic similarities across vision and language; however, despite these advances, image classification is still a challenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal representations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will be biased toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model and Stable Diffusion XL as our generative model. We demonstrate that providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric.

</details>


### [9] [GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction](https://arxiv.org/abs/2512.15751)
*Wei Guan,Jian Cao,Jinyu Cai,Qiqi Cai,Jianqi Gao,See-Kiong Ng*

Main category: cs.LG

TL;DR: GLOW：结合GNN图结构建模与LLM语义推理的统一框架，用于预测Agentic Workflows性能，解决现有方法无法同时捕捉拓扑依赖和深层语义逻辑的问题。


<details>
  <summary>Details</summary>
Motivation: Agentic Workflows（AWs）在解决复杂任务方面很有前景，但其自动化生成的可扩展性受到执行评估高成本和延迟的严重限制。现有AW性能预测方法作为替代方案，但无法同时捕捉AWs中复杂的拓扑依赖关系和深层语义逻辑。

Method: 提出GLOW统一框架，结合GNN的图结构建模能力和LLM的推理能力。具体包括：1）引入经过图任务指令调优的图导向LLM，提取拓扑感知的语义特征；2）将这些特征与GNN编码的结构表示融合；3）采用对比对齐策略进一步优化潜在空间以区分高质量AWs。

Result: 在FLORA-Bench上的大量实验表明，GLOW在预测准确性和排序效用方面优于最先进的基线方法。

Conclusion: GLOW通过统一GNN和LLM的优势，有效解决了AW性能预测中同时捕捉拓扑依赖和语义逻辑的挑战，为AW自动化生成提供了更高效的评估替代方案。

Abstract: Agentic Workflows (AWs) have emerged as a promising paradigm for solving complex tasks. However, the scalability of automating their generation is severely constrained by the high cost and latency of execution-based evaluation. Existing AW performance prediction methods act as surrogates but fail to simultaneously capture the intricate topological dependencies and the deep semantic logic embedded in AWs. To address this limitation, we propose GLOW, a unified framework for AW performance prediction that combines the graph-structure modeling capabilities of GNNs with the reasoning power of LLMs. Specifically, we introduce a graph-oriented LLM, instruction-tuned on graph tasks, to extract topologically aware semantic features, which are fused with GNN-encoded structural representations. A contrastive alignment strategy further refines the latent space to distinguish high-quality AWs. Extensive experiments on FLORA-Bench show that GLOW outperforms state-of-the-art baselines in prediction accuracy and ranking utility.

</details>


### [10] [ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning](https://arxiv.org/abs/2512.15756)
*Yoonpyo Lee*

Main category: cs.LG

TL;DR: ReactorFold：将核反应堆燃料组件设计重新定义为语言模型的序列建模问题，通过生成式方法超越传统固定设计空间的限制


<details>
  <summary>Details</summary>
Motivation: 传统核反应堆堆芯设计方法（确定性、元启发式、机器学习辅助）在固定的人工定义配置空间中搜索，限制了发现全新设计拓扑结构的能力，需要突破人类强加的设计约束

Method: 引入ReactorFold生成框架，将燃料组件设计重新定义为语言模型的序列建模问题；使用蒙特卡洛数据、参数高效微调和直接偏好优化（DPO）训练模型学习压水堆组件的潜在结构，并通过单次前向传递生成候选布局

Result: DPO对齐的模型展现出新兴的设计空间扩展能力：尽管仅在固定数量钆可燃吸收体（Gd）棒配置上训练，却能自主调整Gd库存以满足严格的功率峰值约束；模型还发现了高性能的非对称配置，挑战了传统的对称装载启发式方法，访问了传统搜索方法无法达到的设计区域

Conclusion: 语言模型能够内化因果物理关系并超越人类强加的设计约束，为核反应堆设计开辟了新的可能性，展示了生成式方法在复杂工程问题中的潜力

Abstract: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.

</details>


### [11] [Twin Restricted Kernel Machines for Multiview Classification](https://arxiv.org/abs/2512.15757)
*A. Quadir,M. Sajid,Mushir Akhtar,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了一种新的多视图双限制核机(TMvRKM)，通过正则化最小二乘方法替代传统二次规划，提高了计算效率和分类性能，在多视图数据集上表现出优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统多视图支持向量机方法面临高维空间决策边界捕捉困难、容易出错、视图不一致等问题，且依赖大规模二次规划求解，计算效率低下。需要一种既能有效利用多视图互补信息，又能克服传统核方法计算和泛化挑战的新模型。

Method: 提出TMvRKM模型，将核机与多视图框架结合，采用正则化最小二乘方法确定最优分离超平面，避免大规模二次规划求解。模型中包含耦合项平衡多视图误差，结合早期和晚期融合策略，在训练时利用所有视图的集体信息，同时保持对单个视图特定变化的灵活性。

Result: 在UCI、KEEL和AwA基准数据集上进行严格测试，实验结果表明TMvRKM在所有场景中都优于基线模型，统计分析一致显示其具有卓越的泛化性能。

Conclusion: TMvRKM模型成功解决了传统多视图核方法的计算效率和泛化挑战，通过创新的正则化最小二乘方法和多视图融合策略，在多视图学习任务中实现了优异的性能表现。

Abstract: Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating significant success. Moreover, these models face challenges in effectively capturing decision boundaries in high-dimensional spaces using the kernel trick. They are also prone to errors and struggle with view inconsistencies, which are common in multi-view datasets. In this work, we introduce the multiview twin restricted kernel machine (TMvRKM), a novel model that integrates the strengths of kernel machines with the multiview framework, addressing key computational and generalization challenges associated with traditional kernel-based approaches. Unlike traditional methods that rely on solving large quadratic programming problems (QPPs), the proposed TMvRKM efficiently determines an optimal separating hyperplane through a regularized least squares approach, enhancing both computational efficiency and classification performance. The primal objective of TMvRKM includes a coupling term designed to balance errors across multiple views effectively. By integrating early and late fusion strategies, TMvRKM leverages the collective information from all views during training while remaining flexible to variations specific to individual views. The proposed TMvRKM model is rigorously tested on UCI, KEEL, and AwA benchmark datasets. Both experimental results and statistical analyses consistently highlight its exceptional generalization performance, outperforming baseline models in every scenario.

</details>


### [12] [Yantra AI -- An intelligence platform which interacts with manufacturing operations](https://arxiv.org/abs/2512.15758)
*Varshini Krishnamurthy*

Main category: cs.LG

TL;DR: 该论文开发了一个用于XRIT的智能生产系统，集成了机器学习模型进行预测性维护和异常检测，使用Streamlit实现实时数据可视化，并整合GPT-4虚拟助手提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 工业4.0快速发展改变了智能生产，需要解决能源管理、预测性维护和AI驱动决策支持等关键问题，以提高运营效率和减少停机时间。

Method: 开发智能生产系统，集成随机森林分类器进行主动维护，隔离森林进行异常检测，使用Streamlit实现实时数据可视化仪表板，并整合GPT-4虚拟助手提供实时决策支持。

Result: 系统使用模拟数据进行测试，显著提高了工作效率、能源管理和维修规划能力，系统设计为可扩展，适合XRIT生产环境的实时应用。

Conclusion: 开发的智能生产系统有效解决了工业4.0环境下的关键生产问题，未来工作将集中在实时数据集成和系统优化改进方面。

Abstract: Industry 4.0 is growing quickly, which has changed smart production by encouraging the use of real-time tracking, machine learning, and AI-driven systems to make operations run more smoothly. The main focus of this dissertation is on creating and testing an intelligent production system for XRIT that solves important problems like energy management, predictive maintenance, and AI-powered decision support. Machine learning models are built into the system, such as the Random Forest Classifier for proactive maintenance and the Isolation Forest for finding outliers. These models help with decision-making and reducing downtime. Streamlit makes real-time data visualisation possible, giving workers access to dashboards that they can interact with and see real-time observations.The system was tested with fake data and is made to be scalable, so it can be used in real time in XRIT's production setting. Adding an AI-powered virtual assistant made with GPT-4 lets workers get real-time, useful information that makes complicated questions easier to answer and improves operational decisions. The testing shows that the system makes working efficiency, energy management, and the ability to plan repairs much better. Moving the system to real-time data merging and looking for other ways to make it better will be the main focus of future work.

</details>


### [13] [Semantic-Constrained Federated Aggregation: Convergence Theory and Privacy-Utility Bounds for Knowledge-Enhanced Distributed Learning](https://arxiv.org/abs/2512.15759)
*Jahidul Arafat*

Main category: cs.LG

TL;DR: 提出语义约束联邦聚合框架，通过领域知识约束解决非独立同分布数据下的收敛问题，理论证明收敛速率，实验验证在制造预测性维护中提升22%收敛速度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据条件下收敛缓慢，现有解决方案对所有客户端更新采用相同处理方式，忽略了语义有效性。需要将领域知识约束融入分布式优化，改善收敛性能。

Method: 提出语义约束联邦聚合框架，将领域知识约束纳入分布式优化。使用知识图谱编码约束（基于ISA-95和MASON本体构建3000个约束），理论证明收敛速率O(1/sqrt(T) + rho)，其中rho为约束违反率。

Result: 约束减少41%有效数据异质性，假设空间减少因子θ=0.37。在ε=10的差分隐私下，约束正则化保持效用仅下降3.7%（标准联邦学习下降12.1%）。实验显示22%更快收敛，41.3%模型发散减少，约束违反率ρ<0.05时保持90%最优性能。

Conclusion: 语义约束联邦聚合框架有效解决非独立同分布联邦学习的收敛问题，理论预测与实证结果高度一致（R²>0.90），为约束型联邦学习建立了首个收敛理论，显著改善隐私-效用权衡。

Abstract: Federated learning enables collaborative model training across distributed data sources but suffers from slow convergence under non-IID data conditions. Existing solutions employ algorithmic modifications treating all client updates identically, ignoring semantic validity. We introduce Semantic-Constrained Federated Aggregation (SCFA), a theoretically-grounded framework incorporating domain knowledge constraints into distributed optimization. We prove SCFA achieves convergence rate O(1/sqrt(T) + rho) where rho represents constraint violation rate, establishing the first convergence theory for constraint-based federated learning. Our analysis shows constraints reduce effective data heterogeneity by 41% and improve privacy-utility tradeoffs through hypothesis space reduction by factor theta=0.37. Under (epsilon,delta)-differential privacy with epsilon=10, constraint regularization maintains utility within 3.7% of non-private baseline versus 12.1% degradation for standard federated learning, representing 2.7x improvement. We validate our framework on manufacturing predictive maintenance using Bosch production data with 1.18 million samples and 968 sensor features, constructing knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Experiments demonstrate 22% faster convergence, 41.3% model divergence reduction, and constraint violation thresholds where rho<0.05 maintains 90% optimal performance while rho>0.18 causes catastrophic failure. Our theoretical predictions match empirical observations with R^2>0.90 across convergence, privacy, and violation-performance relationships.

</details>


### [14] [Machine Learning Framework for Thrombosis Risk Prediction in Rotary Blood Pumps](https://arxiv.org/abs/2512.15761)
*Christopher Blum,Michael Neidlin*

Main category: cs.LG

TL;DR: 提出一个基于计算流体力学特征的可解释机器学习框架，用于空间血栓风险评估，通过逻辑回归和特征选择管道识别与血栓风险相关的关键血流特征。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型难以将旋转血泵中的复杂流动条件转化为可靠且可解释的血栓风险预测，这反映了对特定血流特征如何促进血栓形成和生长的理解不足。

Method: 引入基于计算流体力学血流特征的可解释机器学习框架，使用逻辑回归模型结合结构化特征选择管道，推导出紧凑且物理可解释的特征集（包括非线性特征组合），基于已验证的宏观血栓模型的空间风险模式进行训练。

Result: 模型成功重现了标记的风险分布，识别出与血栓风险增加相关的不同血流特征集。即使仅在单个轴向泵运行点上训练，当应用于离心泵时，模型也能预测合理的血栓易发区域。

Conclusion: 可解释机器学习能够将局部血流特征与血栓风险联系起来，同时保持计算效率和机制透明度。低计算成本支持快速血栓形成性筛查，该框架补充了基于物理的血栓建模，为将可解释机器学习集成到CFD驱动的血栓分析和设备设计工作流程提供了方法学基础。

Abstract: Thrombosis in rotary blood pumps arises from complex flow conditions that remain difficult to translate into reliable and interpretable risk predictions using existing computational models. This limitation reflects an incomplete understanding of how specific flow features contribute to thrombus initiation and growth. This study introduces an interpretable machine learning framework for spatial thrombosis assessment based directly on computational fluid dynamics-derived flow features. A logistic regression (LR) model combined with a structured feature-selection pipeline is used to derive a compact and physically interpretable feature set, including nonlinear feature combinations. The framework is trained using spatial risk patterns from a validated, macro-scale thrombosis model for two representative scenarios. The model reproduces the labeled risk distributions and identifies distinct sets of flow features associated with increased thrombosis risk. When applied to a centrifugal pump, despite training on a single axial pump operating point, the model predicts plausible thrombosis-prone regions. These results show that interpretable machine learning can link local flow features to thrombosis risk while remaining computationally efficient and mechanistically transparent. The low computational cost enables rapid thrombogenicity screening without repeated or costly simulations. The proposed framework complements physics-based thrombosis modeling and provides a methodological basis for integrating interpretable machine learning into CFD-driven thrombosis analysis and device design workflows.

</details>


### [15] [Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction](https://arxiv.org/abs/2512.15762)
*Kanxue Li,Yibing Zhan,Hua Jin,Chongchong Qi,Xu Lin,Baosheng Yu*

Main category: cs.LG

TL;DR: CSA-TTA：一种跨样本增强的测试时适应框架，通过整合其他患者的低血压事件来提升术中低血压的个性化预测性能


<details>
  <summary>Details</summary>
Motivation: 术中低血压预测面临患者特异性变异和事件稀少性的挑战，传统测试时适应方法因低血压事件罕见而导致训练不可靠

Method: 提出CSA-TTA框架：1）构建跨样本库，将历史数据分为低血压和非低血压样本；2）采用粗到细检索策略，先通过K-Shape聚类识别代表性簇中心，再基于当前患者信号检索语义相似样本；3）整合自监督掩码重建和回顾性序列预测信号来增强模型适应性

Result: 在VitalDB数据集和真实医院数据集上评估，与TimesFM和UniTS等先进时序预测模型集成，CSA-TTA在不同设置下均提升性能：在VitalDB上，微调场景下Recall和F1分别提升+1.33%和+1.13%，零样本场景下提升+7.46%和+5.07%

Conclusion: CSA-TTA通过跨样本增强有效解决了术中低血压预测中测试时训练不可靠的问题，展示了强大的鲁棒性和泛化能力

Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.

</details>


### [16] [Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic](https://arxiv.org/abs/2512.15765)
*Mélissa Tamine,Otmane Sakhi,Benjamin Heymann*

Main category: cs.LG

TL;DR: 本文提出利用DPO（直接偏好优化）的数学结构简化LLM数据价值评估中的Shapley值计算，解决传统方法计算成本过高的问题。


<details>
  <summary>Details</summary>
Motivation: 数据是训练大语言模型的关键资产，但数据价值评估面临计算挑战。传统基于合作博弈论的Shapley值计算方法需要大量模型重训练，对于大型模型计算成本过高。需要找到更高效的数据价值评估方法。

Method: 利用DPO（直接偏好优化）训练LLM的特殊数学结构，开发可扩展的Shapley值计算方法。DPO的特定形式使得数据贡献的边际效应计算更加高效，避免了传统方法需要的大量模型重训练。

Result: 证明了DPO的数学结构能够显著简化Shapley值计算，使数据价值评估在LLM场景下变得可行。这种方法大幅降低了计算成本，为数据价值评估与LLM的交叉应用打开了可能性。

Conclusion: DPO训练方法为LLM数据价值评估提供了高效的Shapley值计算途径，解决了传统方法的计算瓶颈，有望促进数据所有者之间的协作和数据投资决策。

Abstract: Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.

</details>


### [17] [TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration](https://arxiv.org/abs/2512.15773)
*Ye Li,Jiahe Feng,Yuan Meng,Kangye Ji,Chen Tang,Xinwan Wen,Shutao Xia,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: TS-DP提出首个用于扩散策略的推测解码框架，通过Transformer草稿器和RL调度器实现时间自适应，在保持性能的同时将推理速度提升4.17倍，达到25Hz实时控制频率。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在具身控制中表现出色，但存在推理延迟高、计算成本大的问题。现有静态加速方法无法处理动态具身任务，而推测解码作为无损自适应方案在扩散策略中尚未充分探索。

Method: 1) 使用Transformer草稿器模仿基础模型，替代昂贵的去噪调用；2) 基于强化学习的调度器根据任务难度动态调整推测参数，在保持准确性的同时提高效率。

Result: 在多种具身环境中，TS-DP实现了高达4.17倍的推理加速，草稿接受率超过94%，推理频率达到25Hz，实现了无性能下降的实时扩散控制。

Conclusion: TS-DP是首个为扩散策略设计的推测解码框架，通过时间自适应机制成功解决了动态具身任务中的计算效率问题，实现了高性能实时控制。

Abstract: Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.

</details>


### [18] [Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence](https://arxiv.org/abs/2512.15780)
*Samruddhi Baviskar*

Main category: cs.LG

TL;DR: 评估金融决策中表格机器学习模型的对抗鲁棒性，发现在小扰动下性能显著下降，对抗训练可部分恢复


<details>
  <summary>Details</summary>
Motivation: 金融决策中的表格机器学习模型（如信用评分和欺诈检测）对对抗攻击的鲁棒性尚未充分研究，需要评估这些模型在对抗扰动下的表现，特别是对歧视性、校准性和金融风险指标的影响

Method: 使用信用评分和欺诈检测数据，应用基于梯度的对抗攻击方法，测量模型在歧视性、校准性和金融风险指标方面的变化

Result: 结果显示，在小扰动下模型性能显著下降，通过对抗训练可以部分恢复模型性能

Conclusion: 金融决策中的表格机器学习模型对对抗攻击脆弱，需要加强鲁棒性保护措施，对抗训练是有效的缓解策略但只能部分恢复性能

Abstract: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.

</details>


### [19] [Boosting t-SNE Efficiency for Sequencing Data: Insights from Kernel Selection](https://arxiv.org/abs/2512.15900)
*Avais Jan,Prakash Chourasia,Sarwan Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 研究评估了9种不同核函数在t-SNE降维中的表现，发现余弦相似度核在生物序列数据分析中优于传统高斯核和隔离核，具有更好的运行效率和距离保持能力。


<details>
  <summary>Details</summary>
Motivation: 传统t-SNE使用的高斯核缺乏数据依赖性且计算开销大，限制了其在分类生物序列分析中的可扩展性和有效性。虽然已有研究提出隔离核作为替代，但它可能无法最优地捕捉序列相似性。

Method: 研究评估了9种不同核函数在t-SNE中的应用，使用三种嵌入方法：One-Hot编码、Spike2Vec和minimizers。通过主观可视化和客观指标（包括邻域保持分数）进行评估，并在六个不同的生物数据集上进行分类和聚类实验验证。

Result: 余弦相似度核总体上优于其他核函数（包括高斯核和隔离核），在运行效率和低维空间中的成对距离保持方面表现更优。核选择不仅影响可视化质量，还显著影响下游分析任务。

Conclusion: 余弦相似度核在不同数据类型和嵌入策略中提供最稳健的性能，特别适合大规模生物序列分析。核函数选择对t-SNE在生物序列数据分析中的效果有重要影响。

Abstract: Dimensionality reduction techniques are essential for visualizing and analyzing high-dimensional biological sequencing data. t-distributed Stochastic Neighbor Embedding (t-SNE) is widely used for this purpose, traditionally employing the Gaussian kernel to compute pairwise similarities. However, the Gaussian kernel's lack of data-dependence and computational overhead limit its scalability and effectiveness for categorical biological sequences. Recent work proposed the isolation kernel as an alternative, yet it may not optimally capture sequence similarities. In this study, we comprehensively evaluate nine different kernel functions for t-SNE applied to molecular sequences, using three embedding methods: One-Hot Encoding, Spike2Vec, and minimizers. Through both subjective visualization and objective metrics (including neighborhood preservation scores), we demonstrate that the cosine similarity kernel in general outperforms other kernels, including Gaussian and isolation kernels, achieving superior runtime efficiency and better preservation of pairwise distances in low-dimensional space. We further validate our findings through extensive classification and clustering experiments across six diverse biological datasets (Spike7k, Host, ShortRead, Rabies, Genome, and Breast Cancer), employing multiple machine learning algorithms and evaluation metrics. Our results show that kernel selection significantly impacts not only visualization quality but also downstream analytical tasks, with the cosine similarity kernel providing the most robust performance across different data types and embedding strategies, making it particularly suitable for large-scale biological sequence analysis.

</details>


### [20] [A Unification of Discrete, Gaussian, and Simplicial Diffusion](https://arxiv.org/abs/2512.15923)
*Nuria Alina Chandra,Yucen Lily Li,Alan N. Amin,Alex Ali,Joshua Rollins,Sebastian W. Ober,Aniruddh Raghu,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，将离散扩散、高斯扩散和单纯形扩散三种离散序列建模方法统一为同一基础过程的不同参数化形式，即Wright-Fisher种群遗传模型，并展示了它们之间的数学联系。


<details>
  <summary>Details</summary>
Motivation: 当前离散序列建模（如DNA、蛋白质、语言）存在三种主要扩散方法：离散空间扩散、欧几里得空间的高斯扩散和单纯形扩散。这些方法各有优缺点：离散扩散领域最自然，高斯扩散算法更成熟，单纯形扩散理论上结合了两者优点但实践中数值不稳定。缺乏统一框架使得研究者难以在不同方法间切换。

Method: 构建理论框架，将三种离散扩散方法统一为Wright-Fisher种群遗传模型的不同参数化形式。发现单纯形扩散和高斯扩散是该模型的两个大种群极限。利用数十年数学遗传学文献开发稳定的单纯形扩散方法，并训练单一模型在测试时能在三个领域中进行扩散。

Result: Wright-Fisher单纯形扩散比先前单纯形扩散模型更稳定，在条件DNA生成任务中表现更好。能够训练同时在多个领域工作的模型，其性能与在单个领域训练的模型相当。实现了单一模型在测试时可在三个扩散领域间切换。

Conclusion: 通过Wright-Fisher种群遗传模型统一了三种离散扩散方法，建立了它们之间的数学联系，解决了单纯形扩散的稳定性问题，并实现了单一模型的多领域扩散能力，为离散序列建模提供了更灵活和强大的框架。

Abstract: To model discrete sequences such as DNA, proteins, and language using diffusion, practitioners must choose between three major methods: diffusion in discrete space, Gaussian diffusion in Euclidean space, or diffusion on the simplex. Despite their shared goal, these models have disparate algorithms, theoretical structures, and tradeoffs: discrete diffusion has the most natural domain, Gaussian diffusion has more mature algorithms, and diffusion on the simplex in principle combines the strengths of the other two but in practice suffers from a numerically unstable stochastic processes. Ideally we could see each of these models as instances of the same underlying framework, and enable practitioners to switch between models for downstream applications. However previous theories have only considered connections in special cases. Here we build a theory unifying all three methods of discrete diffusion as different parameterizations of the same underlying process: the Wright-Fisher population genetics model. In particular, we find simplicial and Gaussian diffusion as two large-population limits. Our theory formally connects the likelihoods and hyperparameters of these models and leverages decades of mathematical genetics literature to unlock stable simplicial diffusion. Finally, we relieve the practitioner of balancing model trade-offs by demonstrating it is possible to train a single model that can perform diffusion in any of these three domains at test time. Our experiments show that Wright-Fisher simplicial diffusion is more stable and outperforms previous simplicial diffusion models on conditional DNA generation. We also show that we can train models on multiple domains at once that are competitive with models trained on any individual domain.

</details>


### [21] [DSO: Direct Steering Optimization for Bias Mitigation](https://arxiv.org/abs/2512.15926)
*Lucas Monteiro Paes,Nivedha Sivakumar,Yinong Oliver Wang,Masha Fedzechkina Donaldson,Luca Zappella,Nicholas Apostoloff*

Main category: cs.LG

TL;DR: 本文提出DSO方法，通过强化学习优化激活向量线性变换，在推理时可控地减少视觉语言模型和大型语言模型中的偏见，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在为用户做决策时（如视觉语言模型识别医生），会受到输入人物人口统计属性的影响而产生偏见（如无法识别女性为医生）。现有激活导向方法难以纠正需要跨人口群体等概率结果的偏见，且用户对偏见缓解与模型性能的平衡需求各异，需要推理时可控制的偏见减少方法。

Method: 提出直接导向优化（DSO）方法，使用强化学习寻找激活向量的线性变换，专门针对缓解偏见而设计，同时保持对模型性能的控制。该方法在推理时提供对公平性与能力权衡的控制。

Result: DSO在视觉语言模型和大型语言模型上实现了公平性与模型能力之间的最优权衡，为实践者提供了推理时对权衡的控制能力。

Conclusion: 直接针对控制模型行为进行优化的导向策略设计具有优势，比依赖预定义启发式方法进行可控性的方法能提供更有效的偏见干预。

Abstract: Generative models are often deployed to make decisions on behalf of users, such as vision-language models (VLMs) identifying which person in a room is a doctor to help visually impaired individuals. Yet, VLM decisions are influenced by the perceived demographic attributes of people in the input, which can lead to biased outcomes like failing to identify women as doctors. Moreover, when reducing bias leads to performance loss, users may have varying needs for balancing bias mitigation with overall model capabilities, highlighting the demand for methods that enable controllable bias reduction during inference. Activation steering is a popular approach for inference-time controllability that has shown potential in inducing safer behavior in large language models (LLMs). However, we observe that current steering methods struggle to correct biases, where equiprobable outcomes across demographic groups are required. To address this, we propose Direct Steering Optimization (DSO) which uses reinforcement learning to find linear transformations for steering activations, tailored to mitigate bias while maintaining control over model performance. We demonstrate that DSO achieves state-of-the-art trade-off between fairness and capabilities on both VLMs and LLMs, while offering practitioners inference-time control over the trade-off. Overall, our work highlights the benefit of designing steering strategies that are directly optimized to control model behavior, providing more effective bias intervention than methods that rely on pre-defined heuristics for controllability.

</details>


### [22] [BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research](https://arxiv.org/abs/2512.15931)
*Tiancheng Gao,Scott C. Lowe,Brendan Furneaux,Angel X Chang,Graham W. Taylor*

Main category: cs.LG

TL;DR: BarcodeMamba+是一个基于状态空间模型架构的真菌DNA条形码分类基础模型，采用预训练-微调范式，在数据稀疏的真菌分类任务中显著优于传统监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 真菌DNA条形码分类面临标签稀疏和长尾分布等极端挑战，传统监督学习方法难以泛化到未见物种且无法有效捕捉数据的层次结构特性。

Method: 提出BarcodeMamba+基础模型，采用预训练-微调范式利用部分标记数据。在微调阶段系统整合了层次标签平滑、加权损失函数和MycoAI的多头输出层等增强技术。

Result: 在具有明显分类分布偏移的真菌分类基准测试中，最终模型在所有分类级别上都优于现有方法，每个增强组件都带来了显著的性能提升。

Conclusion: 该工作为基于基因组学的生物多样性研究提供了强大新工具，并为这一挑战性领域建立了有效且可扩展的训练范式。

Abstract: Accurate taxonomic classification from DNA barcodes is a cornerstone of global biodiversity monitoring, yet fungi present extreme challenges due to sparse labelling and long-tailed taxa distributions. Conventional supervised learning methods often falter in this domain, struggling to generalize to unseen species and to capture the hierarchical nature of the data. To address these limitations, we introduce BarcodeMamba+, a foundation model for fungal barcode classification built on a powerful and efficient state-space model architecture. We employ a pretrain and fine-tune paradigm, which utilizes partially labelled data and we demonstrate this is substantially more effective than traditional fully-supervised methods in this data-sparse environment. During fine-tuning, we systematically integrate and evaluate a suite of enhancements--including hierarchical label smoothing, a weighted loss function, and a multi-head output layer from MycoAI--to specifically tackle the challenges of fungal taxonomy. Our experiments show that each of these components yields significant performance gains. On a challenging fungal classification benchmark with distinct taxonomic distribution shifts from the broad training set, our final model outperforms a range of existing methods across all taxonomic levels. Our work provides a powerful new tool for genomics-based biodiversity research and establishes an effective and scalable training paradigm for this challenging domain. Our code is publicly available at https://github.com/bioscan-ml/BarcodeMamba.

</details>


### [23] [In-Context Semi-Supervised Learning](https://arxiv.org/abs/2512.15934)
*Jiashuo Fan,Paul Rosu,Aaron T. Wang,Michael Li,Lawrence Carin,Xiang Cheng*

Main category: cs.LG

TL;DR: 该论文研究了Transformer在上下文学习中的半监督学习能力，探索了如何利用未标记数据提升在低标签环境下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前关于Transformer上下文学习能力的理论研究主要集中在有监督的显式标签对场景，但在实践中，即使标签稀疏或缺失时Transformer仍表现良好，这表明未标记的上下文演示中蕴含着重要结构。

Method: 引入并研究了上下文半监督学习（IC-SSL），其中少量标记示例伴随大量未标记点，展示了Transformer可以利用未标记上下文学习鲁棒的、上下文相关的表示。

Result: Transformer能够利用未标记上下文学习到鲁棒的上下文相关表示，这种表示能够实现准确预测，并在低标签环境下显著提升性能。

Conclusion: 该研究为理解Transformer如何在上下文学习框架中利用未标记上下文进行表示学习提供了基础性见解。

Abstract: There has been significant recent interest in understanding the capacity of Transformers for in-context learning (ICL), yet most theory focuses on supervised settings with explicitly labeled pairs. In practice, Transformers often perform well even when labels are sparse or absent, suggesting crucial structure within unlabeled contextual demonstrations. We introduce and study in-context semi-supervised learning (IC-SSL), where a small set of labeled examples is accompanied by many unlabeled points, and show that Transformers can leverage the unlabeled context to learn a robust, context-dependent representation. This representation enables accurate predictions and markedly improves performance in low-label regimes, offering foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework.

</details>


### [24] [Tracking Wildfire Assets with Commodity RFID and Gaussian Process Modeling](https://arxiv.org/abs/2512.15956)
*John Hateley,Sriram Narasimhan,Omid Abari*

Main category: cs.LG

TL;DR: 提出一种基于商品RFID的森林资产追踪方法，无需预先标记已知位置即可实现GPS级别的定位精度，用于野火响应应用。


<details>
  <summary>Details</summary>
Motivation: 传统商品RFID系统在森林环境中因信号衰减、多径效应和环境变化导致定位性能差。现有指纹识别方法需要预先在已知位置部署标签，这在野火响应等实际应用中难以实现。

Method: 使用高斯过程建模不同环境的RF信号响应特征，构建环境模型字典。提出加权对数似然方法将未知环境与字典中最接近的环境模型匹配，实现无需GPS或摄像头等额外传感器的RFID标签定位。

Result: 能够实现GPS级别的定位精度（米级），可同时追踪数十个野火响应资产，无需预先标记已知位置，成本远低于GPS系统。

Conclusion: 该方法为森林环境中的资产追踪提供了一种经济高效、可扩展的解决方案，特别适用于野火响应等需要快速部署的应用场景。

Abstract: This paper presents a novel, cost-effective, and scalable approach to track numerous assets distributed in forested environments using commodity Radio Frequency Identification (RFID) targeting wildfire response applications. Commodity RFID systems suffer from poor tag localization when dispersed in forested environments due to signal attenuation, multi-path effects and environmental variability. Current methods to address this issue via fingerprinting rely on dispersing tags at known locations {\em a priori}. In this paper, we address the case when it is not possible to tag known locations and show that it is possible to localize tags to accuracies comparable to global positioning systems (GPS) without such a constraint. For this, we propose Gaussian Process to model various environments solely based on RF signal response signatures and without the aid of additional sensors such as global positioning GPS or cameras, and match an unknown RF to the closest match in a model dictionary. We utilize a new weighted log-likelihood method to associate an unknown environment with the closest environment in a dictionary of previously modeled environments, which is a crucial step in being able to use our approach. Our results show that it is possible to achieve localization accuracies of the order of GPS, but with passive commodity RFID, which will allow the tracking of dozens of wildfire assets within the vicinity of mobile readers at-a-time simultaneously, does not require known positions to be tagged {\em a priori}, and can achieve localization at a fraction of the cost compared to GPS.

</details>


### [25] [Provably Extracting the Features from a General Superposition](https://arxiv.org/abs/2512.15987)
*Allen Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种从叠加特征中学习特征的算法，通过噪声查询访问函数f，能够识别所有非退化特征方向并重建函数f，适用于任意叠加和一般响应函数。


<details>
  <summary>Details</summary>
Motivation: 复杂机器学习模型通常通过线性表示编码特征，但这些特征存在于叠加状态中难以恢复。特别是在过完备机制下（特征数量大于维度），现有算法面临挑战。需要一种能够处理任意叠加和一般响应函数的有效算法。

Method: 提出一种高效的查询算法，通过噪声oracle访问函数f，在傅里叶空间中迭代细化搜索空间来定位隐藏的特征方向。算法要求特征方向不近似相同，但允许任意响应函数。

Result: 算法能够识别所有非退化特征方向并重建函数f，适用于比所有相关先前结果更一般的设置，仅要求特征方向不近似相同。

Conclusion: 该研究为从叠加特征中学习特征提供了有效的算法框架，突破了过完备机制下的算法挑战，适用于广泛的叠加设置和一般响应函数。

Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,σ_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $σ_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n > d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $σ_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.

</details>


### [26] [Higher-Order LaSDI: Reduced Order Modeling with Multiple Time Derivatives](https://arxiv.org/abs/2512.15997)
*Robert Stephany,William Michael Anderson,Youngsoo Choi*

Main category: cs.LG

TL;DR: 该论文提出了一种结合高阶有限差分格式和Rollout损失函数的方法，用于提升降阶模型在长时间尺度上的预测精度，并在2D Burgers方程上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂偏微分方程通常需要计算昂贵的数值方法，降阶模型虽然能通过降维创建快速近似，但在长时间尺度上的预测能力会显著下降。

Method: 1. 引入灵活、高阶且计算成本低的有限差分格式；2. 提出Rollout损失函数，训练降阶模型在任意时间尺度上做出准确预测。

Result: 在2D Burgers方程上验证了所提方法的有效性，展示了在长时间尺度上保持预测精度的能力。

Conclusion: 通过结合高阶有限差分格式和Rollout损失函数，成功提升了降阶模型在长时间预测中的性能，为解决复杂偏微分方程的长时间尺度计算问题提供了有效方案。

Abstract: Solving complex partial differential equations is vital in the physical sciences, but often requires computationally expensive numerical methods. Reduced-order models (ROMs) address this by exploiting dimensionality reduction to create fast approximations. While modern ROMs can solve parameterized families of PDEs, their predictive power degrades over long time horizons. We address this by (1) introducing a flexible, high-order, yet inexpensive finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to make accurate predictions over arbitrary time horizons. We demonstrate our approach on the 2D Burgers equation.

</details>


### [27] [Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results](https://arxiv.org/abs/2512.16013)
*Ruolei Zeng,Arun Sharma,Shuai An,Mingzhou Yang,Shengya Zhang,Licheng Liu,David Mulla,Shashi Shekhar*

Main category: cs.LG

TL;DR: 提出FTBSC-KGML框架，通过预训练-微调过程和站点特定参数，结合知识引导机器学习，改进农业生态系统碳循环的量化，更好地利用迁移学习和空间异质性。


<details>
  <summary>Details</summary>
Motivation: 在决策相关尺度上准确且经济地量化农业生态系统碳循环对气候缓解和可持续农业至关重要。传统方法依赖位置无关的参数化和独立训练，未能充分利用迁移学习和输入数据的空间异质性，限制了在具有显著变异性的区域的适用性。

Method: 提出FTBSC-KGML框架，在KGML-ag基础上增加预训练-微调过程和站点特定参数。使用多站点遥感GPP、气候和土壤协变量数据进行预训练-微调，估计土地排放。核心是空间异质性感知的迁移学习方案：全局预训练模型在各州或站点微调，学习位置感知表示，在有限数据下提高局部准确性而不牺牲可解释性。

Result: 实证表明，FTBSC-KGML相比纯全局模型具有更低的验证误差和更高的一致性解释能力，能更好地捕捉跨州的空间变异性。

Conclusion: 该工作扩展了先前的SDSA-KGML框架，提出的FTBSC-KGML框架通过结合预训练-微调过程和空间异质性感知的迁移学习，改进了农业生态系统碳循环的量化，在保持可解释性的同时提高了局部准确性。

Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.

</details>


### [28] [CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting](https://arxiv.org/abs/2512.16046)
*Shu Wan,Reepal Shah,John Sabo,Huan Liu,K. Selçuk Candan*

Main category: cs.LG

TL;DR: CauStream是一个用于因果时空径流预测的统一框架，通过联合学习径流因果图和路由图，在保持物理可解释性的同时提升预测性能


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在径流预测中忽视物理过程，限制可解释性和泛化能力；现有因果学习方法依赖固定因果图，无法适应数据变化

Method: 提出CauStream框架，联合学习气象强迫与径流之间的因果图，以及捕捉站点间动态依赖关系的路由图，并在非参数设置下建立因果结构可识别性条件

Result: 在三个美国主要河流流域和三个预测时间尺度上，CauStream持续优于现有最先进方法，预测窗口越长性能优势越明显；学习到的因果图与领域知识高度一致

Conclusion: CauStream为因果时空建模提供了理论基础，能够扩展到广泛的科学和环境应用中，在保持物理可解释性的同时实现更好的预测性能

Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.

</details>


### [29] [Privacy Blur: Quantifying Privacy and Utility for Image Data Release](https://arxiv.org/abs/2512.16086)
*Saeed Mahloujifar,Narine Kokhlikyan,Chuan Guo,Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: 研究发现高斯模糊在实际低精度实现中隐私保护不足，提出像素化和像素化加噪声的方法在适当粒度下能更好平衡隐私与实用性，并开发了Privacy Blur软件包。


<details>
  <summary>Details</summary>
Motivation: 图像数据常包含人脸、车牌等隐私信息，现有高斯模糊方法在实际应用中隐私保护不足，需要寻找更好的隐私保护与模型训练实用性平衡的方法。

Method: 比较四种图像模糊化算法：高斯模糊、像素化、像素化加噪声（DP-Pix）和裁剪。通过反转攻击和识别攻击评估隐私性，通过模型训练效果评估实用性。

Result: 高斯模糊在实际低精度实现中最不隐私，易受反转攻击。像素化和像素化加噪声在适当粒度下能同时提供隐私保护和模型训练实用性。

Conclusion: 像素化和像素化加噪声是比高斯模糊更好的隐私保护方法，开发了Privacy Blur软件包提供建议参数，帮助实现隐私与实用性的平衡。

Abstract: Image data collected in the wild often contains private information such as faces and license plates, and responsible data release must ensure that this information stays hidden. At the same time, released data should retain its usefulness for model-training. The standard method for private information obfuscation in images is Gaussian blurring. In this work, we show that practical implementations of Gaussian blurring are reversible enough to break privacy. We then take a closer look at the privacy-utility tradeoffs offered by three other obfuscation algorithms -- pixelization, pixelization and noise addition (DP-Pix), and cropping. Privacy is evaluated by reversal and discrimination attacks, while utility by the quality of the learnt representations when the model is trained on data with obfuscated faces. We show that the most popular industry-standard method, Gaussian blur is the least private of the four -- being susceptible to reversal attacks in its practical low-precision implementations. In contrast, pixelization and pixelization plus noise addition, when used at the right level of granularity, offer both privacy and utility for a number of computer vision tasks. We make our proposed methods together with suggested parameters available in a software package called Privacy Blur.

</details>


### [30] [AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation](https://arxiv.org/abs/2512.16103)
*Sandeep Neela*

Main category: cs.LG

TL;DR: AIMM是一个AI驱动的框架，通过融合Reddit活动、机器人协调指标和市场数据，为每个股票代码生成每日操纵风险评分，用于检测社交媒体驱动的市场操纵。


<details>
  <summary>Details</summary>
Motivation: 市场操纵现在通常源自协调的社交媒体活动，而非孤立的交易。散户投资者、监管机构和经纪商需要能够将在线叙事和协调模式与市场行为联系起来的工具。

Method: 构建AIMM框架，融合Reddit活动、机器人和协调指标以及OHLCV市场特征，生成每日AIMM操纵风险评分。由于Reddit API限制，使用校准的合成社交特征匹配事件特征；市场数据使用真实历史数据。采用parquet原生管道和Streamlit仪表板。

Result: 1) 构建了AIMM-GT数据集：33个标记的股票-日，涵盖8只股票；2) 实现了前向评估和预测日志记录；3) AIMM在2021年1月GME挤压峰值前22天发出预警。虽然标记集较小，但显示了初步的判别能力和早期预警能力。

Conclusion: AIMM框架展示了检测社交媒体驱动市场操纵的潜力，特别是在GME事件中提前发出预警。发布代码、数据集架构和仪表板设计以支持社交媒体驱动的市场监控研究。

Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.

</details>


### [31] [BUILD with Precision: Bottom-Up Inference of Linear DAGs](https://arxiv.org/abs/2512.16111)
*Hamed Ajorlou,Samuel Rey,Gonzalo Mateos,Geert Leus,Antonio G. Marques*

Main category: cs.LG

TL;DR: BUILD算法利用观测数据的精度矩阵结构，通过自底向上逐步识别叶节点和父节点来重建线性高斯SEM下的DAG结构


<details>
  <summary>Details</summary>
Motivation: 从观测数据中学习有向无环图结构是因果发现、统计信号处理和机器学习中的核心问题。在线性高斯结构方程模型且噪声方差相等的条件下，该问题是可识别的，需要开发有效的DAG重建算法。

Method: 提出BUILD算法，利用观测数据精度矩阵的独特结构，通过确定性逐步算法：1) 识别叶节点及其父节点；2) 通过移除入射边修剪叶节点；3) 重复此过程直到完全重建DAG。针对有限数据估计精度矩阵的问题，采用周期性重新估计策略以增强鲁棒性。

Result: 在具有挑战性的合成基准测试中，BUILD算法与最先进的DAG学习算法相比表现优越，同时提供了明确的复杂度控制。

Conclusion: BUILD算法能够有效利用线性高斯SEM下精度矩阵的结构特性，通过自底向上的确定性方法准确重建DAG结构，在有限数据条件下通过重新估计策略保持鲁棒性，为DAG学习提供了新的有效方法。

Abstract: Learning the structure of directed acyclic graphs (DAGs) from observational data is a central problem in causal discovery, statistical signal processing, and machine learning. Under a linear Gaussian structural equation model (SEM) with equal noise variances, the problem is identifiable and we show that the ensemble precision matrix of the observations exhibits a distinctive structure that facilitates DAG recovery. Exploiting this property, we propose BUILD (Bottom-Up Inference of Linear DAGs), a deterministic stepwise algorithm that identifies leaf nodes and their parents, then prunes the leaves by removing incident edges to proceed to the next step, exactly reconstructing the DAG from the true precision matrix. In practice, precision matrices must be estimated from finite data, and ill-conditioning may lead to error accumulation across BUILD steps. As a mitigation strategy, we periodically re-estimate the precision matrix (with less variables as leaves are pruned), trading off runtime for enhanced robustness. Reproducible results on challenging synthetic benchmarks demonstrate that BUILD compares favorably to state-of-the-art DAG learning algorithms, while offering an explicit handle on complexity.

</details>


### [32] [Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure](https://arxiv.org/abs/2512.16126)
*Lulu Xue,Shengshan Hu,Linqiang Qian,Peijin Guo,Yechao Zhang,Minghui Li,Yanjun Zhang,Dayong Ye,Leo Yu Zhang*

Main category: cs.LG

TL;DR: 该论文首次揭示了机器学习遗忘技术中保留数据的隐私风险，提出了双视图推理攻击DVIA，证明攻击者通过查询原始模型和遗忘模型可以获得比单独查询任一模型更多的隐私信息。


<details>
  <summary>Details</summary>
Motivation: 机器学习遗忘技术虽然能删除特定训练数据以满足数据删除请求，但现有研究主要关注已删除数据的隐私风险，而保留数据的隐私风险尚未充分探索。论文旨在填补这一空白，揭示在双视图设置下（攻击者可同时查询原始模型和遗忘模型）保留数据面临的隐私威胁。

Method: 从信息论角度引入"隐私知识增益"概念，证明双视图设置使攻击者能获得比单独查询任一模型更多的信息。提出DVIA（双视图推理攻击）方法，通过黑盒查询两个模型，使用轻量级似然比推理模块提取保留数据的成员信息，无需训练攻击模型。

Result: 在不同数据集和模型架构上的实验验证了DVIA的有效性，表明双视图设置确实放大了隐私泄露风险，攻击者能更准确地推断保留数据的成员信息。

Conclusion: 机器学习遗忘技术在双视图设置下会引入新的隐私风险，保留数据的隐私可能受到威胁。研究揭示了这一被忽视的安全问题，为未来开发更安全的遗忘机制提供了重要参考。

Abstract: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.

</details>


### [33] [INTELLECT-3: Technical Report](https://arxiv.org/abs/2512.16144)
*Prime Intellect Team,Mika Senghaas,Fares Obeid,Sami Jaghouar,William Brown,Jack Min Ong,Daniel Auras,Matej Sirovatka,Jannik Straube,Andrew Baker,Sebastian Müller,Justus Mattern,Manveer Basra,Aiman Ismail,Dominik Scherm,Cooper Miller,Ameen Patel,Simon Kirsten,Mario Sieg,Christian Reetz,Kemal Erdem,Vincent Weisser,Johannes Hagemann*

Main category: cs.LG

TL;DR: INTELLECT-3是一个106B参数的MoE模型，通过大规模强化学习训练，在数学、代码、科学和推理基准测试中达到同尺寸模型的最先进性能，并开源了完整的RL基础设施栈。


<details>
  <summary>Details</summary>
Motivation: 开发一个在多个领域（数学、代码、科学、推理）都能达到最先进性能的高效大语言模型，同时构建可扩展的强化学习基础设施，促进开源社区的发展。

Method: 使用混合专家架构（106B参数，12B激活），基于GLM-4.5-Air-Base模型，通过大规模强化学习训练。开发了prime-rl框架支持大规模异步RL训练，从单节点扩展到数千GPU，支持多轮交互和工具使用。使用verifiers库构建了训练和评估环境。

Result: INTELLECT-3在其尺寸范围内，在数学、代码、科学和推理基准测试中达到了最先进的性能，超越了多个更大的前沿模型。RL训练扩展到512个H200 GPU，保持了高训练效率。

Conclusion: INTELLECT-3展示了通过大规模强化学习训练MoE模型的有效性，同时开源的基础设施栈为社区提供了完整的训练框架和工具，推动了强化学习在大语言模型训练中的应用。

Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.

</details>


### [34] [Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions](https://arxiv.org/abs/2512.16200)
*Haishan Ye*

Main category: cs.LG

TL;DR: 该论文首次为基于排序的零阶优化算法建立了显式的非渐进收敛率，填补了该领域理论分析的空白。


<details>
  <summary>Details</summary>
Motivation: 基于排序的零阶优化方法（如CMA-ES、自然进化策略等）在实践中广泛应用且表现优异，但现有理论分析仅提供渐进性见解，缺乏显式的收敛速率分析，特别是对于选择top-k方向的算法。本文旨在填补这一理论空白。

Method: 分析一个简单的基于排序的零阶优化算法，采用新颖的分析方法，避免了传统的漂移和信息几何技术，首次建立了显式的非渐进查询复杂度界限。

Result: 对于d维问题：1）若函数是L-光滑且μ-强凸的，算法达到$\widetilde{\mathcal O}\!\left(\frac{dL}μ\log\!\frac{dL}{μδ}\log\!\frac{1}{\varepsilon}\right)$的查询复杂度以找到ε-次优解；2）对于光滑非凸目标，达到$\mathcal O\!\left(\frac{dL}{\varepsilon}\log\!\frac{1}{\varepsilon}\right)$的查询复杂度。这些结果以至少1-δ的概率成立。

Conclusion: 该工作首次为基于排序的零阶优化算法提供了显式的非渐进收敛率分析，为理解为什么基于排序的启发式方法能实现高效的零阶优化提供了新的理论见解。

Abstract: Rank-based zeroth-order (ZO) optimization -- which relies only on the ordering of function evaluations -- offers strong robustness to noise and monotone transformations, and underlies many successful algorithms such as CMA-ES, natural evolution strategies, and rank-based genetic algorithms. Despite its widespread use, the theoretical understanding of rank-based ZO methods remains limited: existing analyses provide only asymptotic insights and do not yield explicit convergence rates for algorithms selecting the top-$k$ directions.
  This work closes this gap by analyzing a simple rank-based ZO algorithm and establishing the first \emph{explicit}, and \emph{non-asymptotic} query complexities. For a $d$-dimension problem, if the function is $L$-smooth and $μ$-strongly convex, the algorithm achieves $\widetilde{\mathcal O}\!\left(\frac{dL}μ\log\!\frac{dL}{μδ}\log\!\frac{1}{\varepsilon}\right)$ to find an $\varepsilon$-suboptimal solution, and for smooth nonconvex objectives it reaches $\mathcal O\!\left(\frac{dL}{\varepsilon}\log\!\frac{1}{\varepsilon}\right)$. Notation $\cO(\cdot)$ hides constant terms and $\widetilde{\mathcal O}(\cdot)$ hides extra $\log\log\frac{1}{\varepsilon}$ term. These query complexities hold with a probability at least $1-δ$ with $0<δ<1$. The analysis in this paper is novel and avoids classical drift and information-geometric techniques. Our analysis offers new insight into why rank-based heuristics lead to efficient ZO optimization.

</details>


### [35] [Neural emulation of gravity-driven geohazard runout](https://arxiv.org/abs/2512.16221)
*Lorenzo Nava,Ye Chen,Maximillian Van Wyk de Vries*

Main category: cs.LG

TL;DR: 该研究开发了一个机器学习模型来预测地质灾害（如滑坡、雪崩）的运移范围，相比传统数值模拟方法，计算速度提高了100-10,000倍，同时保持了物理真实性。


<details>
  <summary>Details</summary>
Motivation: 地质灾害（滑坡、雪崩等）的运移范围预测对保护生命、基础设施和生态系统至关重要。这些灾害往往从源头传播数公里，造成大量伤亡。现有预测方法面临计算速度与物理真实性之间的根本性权衡，难以实现大规模准确预测。

Method: 研究训练了一个机器学习模型来预测地质灾害的运移范围。模型基于超过100,000次数值模拟进行训练，这些模拟覆盖了10,000多个真实世界数字高程模型区域。模型能够预测流动范围和沉积厚度，并重现关键物理行为。

Result: 该模型能够以比数值求解器快100到10,000倍的速度，高精度地预测流动范围和沉积厚度。模型能够重现分叉和沉积模式等关键物理行为，并能泛化到不同类型的流动、规模和地形。

Conclusion: 神经仿真技术能够实现跨多样真实世界地形的快速、空间分辨的运移范围预测，为灾害风险减少和基于影响的预报开辟了新机遇。这为将物理真实的地质灾害建模扩展到大规模早期预警系统所需的空间和时间尺度提供了一条有前景的途径。

Abstract: Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.

</details>


### [36] [Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models](https://arxiv.org/abs/2512.16244)
*Xueqi Ma,Xingjun Ma,Sarah Monazam Erfani,Danilo Mandic,James Bailey*

Main category: cs.LG

TL;DR: 提出CFC框架，利用大语言模型进行图数据集的开放集分类，实现从OOD检测到OOD分类的扩展，无需真实标签信息


<details>
  <summary>Details</summary>
Motivation: 现有开放集分类方法通常将所有OOD样本视为单一类别，但在欺诈检测和医疗诊断等高风险应用中，需要对OOD样本有更深入的理解，包括其可能的标签。这引出了一个关键问题：能否在没有真实标签信息的情况下将OOD检测扩展到OOD分类？

Method: 提出粗到细开放集分类（CFC）框架，包含三个关键组件：1）使用LLM提示进行OOD检测和异常标签生成的粗分类器；2）基于GNN的细分类器，使用粗分类器识别的OOD样本进行训练，增强OOD检测和ID分类；3）通过LLM提示和后处理的OOD标签实现精细化的OOD分类

Result: 实验结果表明，CFC在图和文本领域的OOD检测性能比最先进方法提高了10%，在图数据集上的OOD分类准确率达到70%

Conclusion: CFC框架成功地将OOD检测扩展到OOD分类，利用LLM生成语义OOD实例，提高了可解释性和实际应用价值，为高风险场景下的开放集分类提供了有效解决方案

Abstract: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.

</details>


### [37] [Sharpness-aware Federated Graph Learning](https://arxiv.org/abs/2512.16247)
*Ruiyu Li,Peige Zhao,Guangxia Li,Pengcheng Wu,Xingyu Gao,Zhiqiang Xu*

Main category: cs.LG

TL;DR: SEAL算法通过同时最小化损失函数及其锐度，并引入基于局部表示相关矩阵的正则化器，解决联邦图学习中数据异构性和维度坍塌问题，提升GNN模型的分类准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习面临数据异构性问题：1）基于经验风险最小化的优化器导致局部模型陷入尖锐谷，削弱对分布外图数据的泛化能力；2）局部图数据学习表示中的维度坍塌对GNN模型分类能力产生负面影响。

Method: 提出SEAL算法：1）制定考虑局部GNN模型锐度的优化目标，同时最小化损失函数及其锐度，寻找平坦区域参数；2）引入基于局部表示相关矩阵的正则化器，松弛单个局部图样本生成表示的相关性，缓解维度坍塌。

Result: 在多个图分类基准测试中，SEAL始终优于最先进的联邦图学习基线方法，并为更多参与者提供性能增益，显著提升了局部GNN模型的分类准确性和泛化能力。

Conclusion: SEAL算法通过锐度感知优化和维度坍塌缓解，有效解决了联邦图学习中的数据异构性问题，提高了GNN模型在联邦学习环境下的性能和泛化能力。

Abstract: One of many impediments to applying graph neural networks (GNNs) to large-scale real-world graph data is the challenge of centralized training, which requires aggregating data from different organizations, raising privacy concerns. Federated graph learning (FGL) addresses this by enabling collaborative GNN model training without sharing private data. However, a core challenge in FGL systems is the variation in local training data distributions among clients, known as the data heterogeneity problem. Most existing solutions suffer from two problems: (1) The typical optimizer based on empirical risk minimization tends to cause local models to fall into sharp valleys and weakens their generalization to out-of-distribution graph data. (2) The prevalent dimensional collapse in the learned representations of local graph data has an adverse impact on the classification capacity of the GNN model. To this end, we formulate a novel optimization objective that is aware of the sharpness (i.e., the curvature of the loss surface) of local GNN models. By minimizing the loss function and its sharpness simultaneously, we seek out model parameters in a flat region with uniformly low loss values, thus improving the generalization over heterogeneous data. By introducing a regularizer based on the correlation matrix of local representations, we relax the correlations of representations generated by individual local graph samples, so as to alleviate the dimensional collapse of the learned model. The proposed \textbf{S}harpness-aware f\textbf{E}derated gr\textbf{A}ph \textbf{L}earning (SEAL) algorithm can enhance the classification accuracy and generalization ability of local GNN models in federated graph learning. Experimental studies on several graph classification benchmarks show that SEAL consistently outperforms SOTA FGL baselines and provides gains for more participants.

</details>


### [38] [Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data](https://arxiv.org/abs/2512.16277)
*Jialiang Wang,Xueyan Bao,Hao Wu*

Main category: cs.LG

TL;DR: 提出Sharpness-aware SLF模型，通过Hessian-vector products获取二阶信息并在曲率中注入锐度项，解决二阶潜在因子模型优化困难问题


<details>
  <summary>Details</summary>
Motivation: 二阶潜在因子模型在处理高维不完整数据时有效，但由于其双线性和非凸性质，优化非常困难。Sharpness-aware Minimization方法通过寻找平坦局部最小值来改善表示学习模型的泛化能力

Method: 提出Sharpness-aware SLF模型，包含两个关键思想：(1)通过Hessian-vector products获取二阶信息；(2)通过设计的Hessian-vector products在曲率中注入锐度项

Result: 在多个工业数据集上的实验表明，所提出的模型持续优于最先进的基线方法

Conclusion: SSLF模型通过结合二阶信息和锐度感知优化，有效解决了SLF模型的优化困难问题，提升了表示学习性能

Abstract: Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.

</details>


### [39] [Feature-Selective Representation Misdirection for Machine Unlearning](https://arxiv.org/abs/2512.16297)
*Taozhao Chen,Linghan Huang,Kim-Kwang Raymond Choo,Huaming Chen*

Main category: cs.LG

TL;DR: SRMU是一种针对大语言模型的选择性表示误导遗忘方法，通过特征感知和方向控制的激活编辑，在高度纠缠的数据分布中实现安全知识移除，同时保持模型整体效用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在安全关键和受监管领域的应用增加，模型中保留的敏感或禁止知识带来了隐私泄露、监管不合规和潜在滥用等风险。现有遗忘技术假设遗忘集和保留集完全分离，但在实际运营环境中数据分布高度纠缠，传统方法会损害模型效用或无法确保安全。

Method: 提出选择性表示误导遗忘（SRMU）框架，采用激活编辑方法，使用结构化误导向量和激活重要性映射，对模型激活进行特征感知和方向控制的扰动，选择性抑制有害表示同时保留良性表示的效用。

Result: 在WMDP基准测试中，SRMU在低纠缠和高纠缠配置下均实现了最先进的遗忘性能，且效用损失最小。在20-30%数据重叠的情况下仍保持有效，而现有基线方法会失效。

Conclusion: SRMU为安全驱动的模型治理、隐私合规和受控知识移除提供了稳健基础，适用于新兴的LLM应用场景，能够处理实际运营环境中的高度纠缠数据分布问题。

Abstract: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.

</details>


### [40] [Multivariate Uncertainty Quantification with Tomographic Quantile Forests](https://arxiv.org/abs/2512.16383)
*Takuya Kanazawa*

Main category: cs.LG

TL;DR: Tomographic Quantile Forests (TQF) 是一种用于多变量目标回归的非参数、不确定性感知的树模型，通过方向投影学习条件分位数，并通过切片Wasserstein距离重建多变量条件分布。


<details>
  <summary>Details</summary>
Motivation: 量化预测不确定性对于AI在真实世界中的安全可信部署至关重要，但目前多变量目标的完全非参数条件分布估计仍然具有挑战性。

Method: TQF学习方向投影n⊤y的条件分位数作为输入x和单位方向n的函数。在推理时，它聚合多个方向的分位数，并通过高效的交替方案（具有凸子问题）最小化切片Wasserstein距离来重建多变量条件分布。

Result: TQF在合成和真实世界数据集上进行了评估，能够覆盖所有方向而无需凸性限制，且只需单一模型。

Conclusion: TQF提供了一种有效的非参数方法来估计多变量条件分布，解决了传统方向分位数方法只能产生凸分位数区域且需要为不同方向训练单独模型的问题。

Abstract: Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\mathbf{n}^{\top}\mathbf{y}$ as functions of the input $\mathbf{x}$ and the unit direction $\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.

</details>


### [41] [Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference](https://arxiv.org/abs/2512.16391)
*Dhruv Deshmukh,Saurabh Goyal,Nipun Kwatra,Ramachandran Ramjee*

Main category: cs.LG

TL;DR: Kascade是一种无需训练的稀疏注意力方法，通过选择性地在锚定层计算精确Top-k索引并在中间层重用这些索引，显著降低长上下文LLM推理的注意力延迟。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是长上下文LLM推理中延迟的主要来源，随着推理模型和RAG的普及，这一问题日益突出。现有的注意力计算存在冗余，需要更高效的解决方案。

Method: Kascade利用两个关键观察：1) softmax后的注意力本质上是稀疏的；2) 高权重键的身份在相邻层间是稳定的。方法包括：在锚定层计算精确Top-k索引，在中间重用层复用这些索引；通过动态规划算法选择锚定层以最大化跨层相似性；支持头感知的Top-k选择和重用；针对预填充和解码注意力进行高效实现优化。

Result: 在H100 GPU上，Kascade相比FlashAttention-3基线实现了：解码注意力最高4.1倍加速，预填充注意力最高2.2倍加速。在LongBench和AIME-24等长上下文基准测试中，准确率与密集注意力非常接近。

Conclusion: Kascade是一种有效的训练免费稀疏注意力方法，能够显著加速长上下文LLM推理，同时保持高准确率，为实际部署提供了实用的解决方案。

Abstract: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.

</details>


### [42] [NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning](https://arxiv.org/abs/2512.16408)
*Ruifeng Xu,Liang He*

Main category: cs.LG

TL;DR: 本文提出了一种嵌套双智能体强化学习（NDRL）方法，用于优化棉花灌溉和氮肥管理，通过父智能体进行宏观决策、子智能体进行动态微调，显著提高了产量和资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究面临两个主要限制：1）作物生长过程中水氮组合优化复杂度高，产量优化效果差；2）轻度胁迫信号难以量化且反馈延迟，导致水氮动态调控不精确，资源利用效率低。需要一种更智能的方法来解决农业资源管理的复杂性和精确性问题。

Method: 提出嵌套双智能体强化学习（NDRL）方法：父智能体基于预期累积产量收益识别有前景的宏观灌溉和施肥动作，减少无效探索；子智能体的奖励函数包含量化的水分胁迫因子（WSF）和氮胁迫因子（NSF），使用混合概率分布动态优化每日策略。使用2023和2024年田间试验数据校准和验证DSSAT模型来模拟真实条件并与NDRL交互。

Result: 与最佳基线相比，模拟产量在2023和2024年均提高了4.7%；灌溉水生产力分别提高了5.6%和5.1%；氮偏生产力分别提高了6.3%和1.0%。

Conclusion: NDRL方法推动了棉花灌溉和氮肥管理的发展，为解决农业资源管理的复杂性和精确性问题以及可持续农业发展提供了新思路。

Abstract: Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.

</details>


### [43] [Emergent Bias and Fairness in Multi-Agent Decision Systems](https://arxiv.org/abs/2512.16433)
*Maeve Madigan,Parameswaran Kamalaruban,Glenn Moynihan,Tom Kempton,David Sutton,Stuart Burrell*

Main category: cs.LG

TL;DR: 多智能体系统在金融预测任务中可能产生无法追溯到单个智能体的集体偏见，需要整体评估而非组件分析


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在预测任务中表现出色，但缺乏有效的公平性评估方法，在金融等高风险领域部署存在偏见风险，可能导致监管违规和财务损失

Method: 开发多智能体预测系统的公平性评估方法，在金融表格数据领域测量系统公平性特征，通过大规模模拟不同多智能体配置（包括不同通信和协作机制）来检查公平性指标

Result: 揭示了金融决策中出现的偏见模式，这些偏见无法追溯到单个智能体组件，表明多智能体系统可能表现出真正的集体行为，公平性风险是模型风险的重要组成部分，对信用评分和收入估计等任务有实际影响

Conclusion: 多智能体决策系统必须作为整体实体进行评估，而不是通过对其组成组件的还原主义分析，金融多智能体系统的公平性风险需要系统性评估方法

Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.

</details>


### [44] [Topic Modelling Black Box Optimization](https://arxiv.org/abs/2512.16445)
*Roman Akramov,Artem Khamatullin,Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 该研究将LDA主题模型的主题数量选择问题形式化为离散黑盒优化问题，比较了四种优化方法在验证困惑度评估下的性能差异。


<details>
  <summary>Details</summary>
Motivation: LDA主题模型中主题数量T的选择是一个关键设计决策，它显著影响模型的统计拟合度和可解释性。传统方法通常需要大量试错，研究者希望通过优化方法更高效地确定最佳主题数量。

Method: 将主题数量选择问题形式化为离散黑盒优化问题，其中每个函数评估对应训练一个LDA模型并测量其验证困惑度。在固定评估预算下，比较了四种优化器：两种手工设计的进化方法（遗传算法GA和进化策略ES），以及两种学习的摊销方法（优先摊销黑盒优化PABBO和锐度感知黑盒优化SABBO）。

Result: 实验表明，虽然GA、ES、PABBO和SABBO最终都能达到相似的困惑度范围，但摊销优化器在样本效率和时间效率上显著更优。SABBO通常只需一次评估就能识别出接近最优的主题数量，PABBO在几次评估内就能找到有竞争力的配置，而GA和ES需要几乎全部预算才能达到相同区域。

Conclusion: 摊销黑盒优化方法在LDA主题数量选择问题上比传统进化方法更高效，特别是SABBO和PABBO能够以极少的评估次数找到接近最优的配置，这为实际应用中的超参数优化提供了更实用的解决方案。

Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.

</details>


### [45] [Persistent Multiscale Density-based Clustering](https://arxiv.org/abs/2512.16558)
*Daniël Bot,Leland McInnes,Jan Aerts*

Main category: cs.LG

TL;DR: PLSCAN是一种新型基于密度的聚类算法，能自动识别HDBSCAN*中所有最小聚类尺寸参数下的稳定聚类，无需手动调参，在多个真实数据集上表现优于HDBSCAN*。


<details>
  <summary>Details</summary>
Motivation: 密度聚类算法在探索性数据分析中很有用，但实际应用时需要选择超参数（如DBSCAN的密度阈值、HDBSCAN*的最小聚类尺寸），这需要数据分布的先验知识，而实践中往往缺乏这种知识。

Method: PLSCAN算法基于尺度空间聚类原理，等价于在新型度量空间上的持续同调，能高效识别HDBSCAN*产生稳定（叶）聚类的所有最小聚类尺寸。

Result: 在多个真实数据集上，PLSCAN比HDBSCAN*获得更高的平均ARI（调整兰德指数），对互达邻居数量的变化更不敏感。在低维数据集上计算成本与k-Means相当，高维时与HDBSCAN*类似。

Conclusion: PLSCAN提供了一种无需手动调参的密度聚类方法，在保持计算效率的同时提高了聚类性能，特别适用于缺乏先验知识的探索性数据分析场景。

Abstract: Clustering is a cornerstone of modern data analysis. Detecting clusters in exploratory data analyses (EDA) requires algorithms that make few assumptions about the data. Density-based clustering algorithms are particularly well-suited for EDA because they describe high-density regions, assuming only that a density exists. Applying density-based clustering algorithms in practice, however, requires selecting appropriate hyperparameters, which is difficult without prior knowledge of the data distribution. For example, DBSCAN requires selecting a density threshold, and HDBSCAN* relies on a minimum cluster size parameter. In this work, we propose Persistent Leaves Spatial Clustering for Applications with Noise (PLSCAN). This novel density-based clustering algorithm efficiently identifies all minimum cluster sizes for which HDBSCAN* produces stable (leaf) clusters. PLSCAN applies scale-space clustering principles and is equivalent to persistent homology on a novel metric space. We compare its performance to HDBSCAN* on several real-world datasets, demonstrating that it achieves a higher average ARI and is less sensitive to changes in the number of mutual reachability neighbours. Additionally, we compare PLSCAN's computational costs to k-Means, demonstrating competitive run-times on low-dimensional datasets. At higher dimensions, run times scale more similarly to HDBSCAN*.

</details>


### [46] [Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling](https://arxiv.org/abs/2512.16581)
*Sullivan Castro,Artem Betlei,Thomas Di Martino,Nadir El Manouzi*

Main category: cs.LG

TL;DR: Abacus：一种用于展示广告的自监督预训练方法，通过预测用户事件的频率分布来增强深度序列模型，解决用户购买行为建模中的稀疏性和随机性问题。


<details>
  <summary>Details</summary>
Motivation: 展示广告系统中用户购买行为建模面临两大挑战：正样本事件稀疏性和用户行为随机性，导致严重的类别不平衡和不规则事件时序。现有方法依赖手工制作的"计数器"特征，忽略了用户意图的细粒度时序演化，而当前序列模型又缺少有用的事件计数统计信息。

Method: 提出Abacus方法，通过预测用户事件的频率分布进行自监督预训练。进一步提出混合目标函数，将Abacus与序列学习目标相结合，既保留了聚合统计的稳定性，又具备序列建模的敏感性。

Result: 在两个真实世界数据集上的实验表明，Abacus预训练优于现有方法，能加速下游任务收敛。混合方法相比基线在AUC指标上提升高达+6.1%。

Conclusion: Abacus方法有效解决了展示广告中用户行为建模的挑战，通过结合频率分布预测和序列建模，在保持统计稳定性的同时捕捉时序敏感性，显著提升了预测性能。

Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.

</details>


### [47] [Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game](https://arxiv.org/abs/2512.16626)
*Barna Pásztor,Thomas Kleine Buening,Andreas Krause*

Main category: cs.LG

TL;DR: SLHF是一种新的偏好优化框架，将对齐问题建模为领导者-跟随者的顺序博弈，相比RLHF和NLHF能捕捉更丰富的偏好结构，支持推理时精炼，并在实验中表现出良好的对齐效果和跨模型迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法如RLHF（基于标量奖励）和NLHF（基于同时博弈均衡）存在局限性，无法充分捕捉复杂的偏好结构。需要一种能够处理顺序决策、支持推理时精炼、对数据更敏感且对不可传递偏好更鲁棒的新框架。

Method: 将对齐问题建模为顺序博弈：领导者策略承诺一个动作，跟随者策略根据领导者的动作条件性地响应。将偏好优化分解为跟随者的精炼问题和领导者对抗对手的优化问题。支持推理时迭代采样精炼。

Result: 实验表明SLHF在大型语言模型上实现了强大的对齐效果，在多样化偏好数据集上表现良好，参数规模从0.5B到8B均可扩展，推理时精炼能力可在不同模型家族间迁移而无需进一步微调。

Conclusion: SLHF通过顺序博弈框架提供了比RLHF和NLHF更丰富的偏好结构建模能力，在一致性、数据敏感性和不可传递偏好鲁棒性方面具有优势，为对齐问题提供了新的有效解决方案。

Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.

</details>


### [48] [Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario](https://arxiv.org/abs/2512.16648)
*Liu Yang,Qiang Li,Luxiong Wen,Jian Yang*

Main category: cs.LG

TL;DR: 提出MS-SHOT方法解决无线射频指纹识别中的源数据自由跨接收器适应问题，通过动量中心引导的软伪标签和全局结构约束提升目标域性能。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中基于深度学习的RFFI模型在实际部署时面临关键挑战：当应用于具有不同硬件特性的接收器时，由于接收器变化引入的分布偏移，模型性能会显著下降。需要解决源数据自由的跨接收器适应问题。

Method: 提出MS-SHOT方法：1) 基于约束伪标签的SCRFFI适应框架；2) 动量中心引导的软伪标签技术；3) 全局结构约束以鼓励自信和多样化的预测；4) 理论分析框架的泛化性能。

Result: 在真实世界数据集上的大量实验表明，MS-SHOT在准确性和鲁棒性方面始终优于现有方法，特别是在处理目标域中的标签偏移或未知非均匀类别分布场景时表现优异。

Conclusion: MS-SHOT为RFFI中的源数据自由跨接收器适应提供了一个实用且可扩展的解决方案，通过理论分析和实验验证了其有效性，解决了先前方法在处理标签偏移方面的显著局限性。

Abstract: With the rapid proliferation of edge computing, Radio Frequency Fingerprint Identification (RFFI) has become increasingly important for secure device authentication. However, practical deployment of deep learning-based RFFI models is hindered by a critical challenge: their performance often degrades significantly when applied across receivers with different hardware characteristics due to distribution shifts introduced by receiver variation. To address this, we investigate the source-data-free cross-receiver RFFI (SCRFFI) problem, where a model pretrained on labeled signals from a source receiver must adapt to unlabeled signals from a target receiver, without access to any source-domain data during adaptation. We first formulate a novel constrained pseudo-labeling-based SCRFFI adaptation framework, and provide a theoretical analysis of its generalization performance. Our analysis highlights a key insight: the target-domain performance is highly sensitive to the quality of the pseudo-labels generated during adaptation. Motivated by this, we propose Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), a new method for SCRFFI that incorporates momentum-center-guided soft pseudo-labeling and enforces global structural constraints to encourage confident and diverse predictions. Notably, MS-SHOT effectively addresses scenarios involving label shift or unknown, non-uniform class distributions in the target domain -- a significant limitation of prior methods. Extensive experiments on real-world datasets demonstrate that MS-SHOT consistently outperforms existing approaches in both accuracy and robustness, offering a practical and scalable solution for source-data-free cross-receiver adaptation in RFFI.

</details>


### [49] [Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification](https://arxiv.org/abs/2512.16687)
*Natnael Tilahun Sinshaw,Mengmei He,Tadesse K. Bahiru,Sudhir Kumar Mohapatra*

Main category: cs.LG

TL;DR: 该研究对文本分类任务中的多种机器学习算法与神经符号AI方法进行了比较分析，发现神经符号AI方法在有限数据集下能达到与MLP相当的性能。


<details>
  <summary>Details</summary>
Motivation: 文本分类（如博客性别分类）是机器学习中成熟的研究领域，在市场营销、客户推荐等应用中有重要价值。本研究旨在比较传统机器学习算法与新兴的神经符号AI方法在文本分类任务中的表现。

Method: 研究比较了SVM、朴素贝叶斯、逻辑回归、AdaBoost、XGBoost、SVM变体（SVM_R）以及神经符号AI（NeSy）等多种算法。同时探索了TF-IDF、通用句子编码器（USE）、RoBERTa等文本表示方法，以及卡方检验、互信息、主成分分析等特征提取技术。

Result: 实验结果表明，神经符号AI方法在有限数据集下能够达到与多层感知机（MLP）相当的强性能表现。

Conclusion: 神经符号AI在文本分类任务中展现出潜力，未来研究将扩展知识库、嵌入类型范围和超参数配置，以进一步探索NeSy方法的有效性。

Abstract: Text classification problems, such as gender classification from a blog, have been a well-matured research area that has been well studied using machine learning algorithms. It has several application domains in market analysis, customer recommendation, and recommendation systems. This study presents a comparative analysis of the widely used machine learning algorithms, namely Support Vector Machines (SVM), Naive Bayes (NB), Logistic Regression (LR), AdaBoost, XGBoost, and an SVM variant (SVM_R) with neuro-symbolic AI (NeSy). The paper also explores the effect of text representations such as TF-IDF, the Universal Sentence Encoder (USE), and RoBERTa. Additionally, various feature extraction techniques, including Chi-Square, Mutual Information, and Principal Component Analysis, are explored. Building on these, we introduce a comparative analysis of the machine learning and deep learning approaches in comparison to the NeSy. The experimental results show that the use of the NeSy approach matched strong MLP results despite a limited dataset. Future work on this research will expand the knowledge base, the scope of embedding types, and the hyperparameter configuration to further study the effectiveness of the NeSy approach.

</details>


### [50] [CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies](https://arxiv.org/abs/2512.16700)
*John M. Statheros,Hairong Wang,Richard Klein*

Main category: cs.LG

TL;DR: CLARiTy是一种基于视觉Transformer的模型，用于胸部X光的多标签病理分类和弱监督定位，通过类特定token生成注意力图，结合SegmentCAM模块进行前景分割，在NIH ChestX-ray14数据集上实现了竞争性的分类性能和最先进的弱监督定位性能。


<details>
  <summary>Details</summary>
Motivation: 胸部X光解释面临多标签病理分类和空间定位的挑战，特别是在区域级（密集）标注稀缺的情况下。现有方法通常需要不同粒度的标注，但实际应用中缺乏密集标注数据，因此需要开发能够在弱监督条件下同时完成分类和定位的模型。

Method: CLARiTy采用视觉Transformer架构，使用多个类特定token生成区分性注意力图，引入SegmentCAM模块利用解剖先验进行前景分割和背景抑制。模型使用图像级标签在NIH ChestX-ray14数据集上训练，通过ConvNeXtV2教师模型进行蒸馏以提高效率。还包括DINO预训练、正交类token损失和注意力池化等技术。

Result: 在NIH ChestX-ray14数据集上，CLARiTy-S-16-512配置在14种病理分类上取得竞争性性能，在8种病理的弱监督定位上达到最先进水平，比先前方法提升50.7%。特别是对小病理（如结节和肿块）有显著提升。低分辨率变体CLARiTy-S-16-224在保持高效率的同时明显超越基线模型。

Conclusion: CLARiTy超越了CNN-ViT混合模型，通过利用ViT自注意力获取全局上下文和类特定定位，结合卷积背景抑制生成精确、降噪的热力图，为低资源环境下的胸部X光分析提供了高效解决方案。

Abstract: The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.

</details>


### [51] [Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library](https://arxiv.org/abs/2512.16715)
*Oliver Stritzel,Nick Hühnerbein,Simon Rauch,Itzel Zarate,Lukas Fleischmann,Moike Buck,Attila Lischka,Christian Frey*

Main category: cs.LG

TL;DR: SPICE是一个Python框架，重新实现了三种基于深度学习的预测性流程挖掘方法，提供可配置的基础框架，支持可重复和稳健的模型比较。


<details>
  <summary>Details</summary>
Motivation: 当前预测性流程挖掘技术缺乏可重复性、决策透明度、对新数据集的适应性以及基准测试能力，导致不同实现之间的比较非常困难。

Method: 提出SPICE框架，在PyTorch中重新实现三种流行的深度学习基线方法，设计具有严格可配置性的通用基础框架，支持可重复和稳健的模型比较。

Result: 在11个数据集上比较SPICE与原始报告指标以及公平指标，验证框架的有效性。

Conclusion: SPICE框架能够促进预测性流程挖掘领域的可重复研究、透明决策和公平比较，为未来建模方法提供基准测试平台。

Abstract: In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.

</details>


### [52] [Polyharmonic Spline Packages: Composition, Efficient Procedures for Computation and Differentiation](https://arxiv.org/abs/2512.16718)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 提出级联架构解决高维空间中的机器学习回归问题，使用多谐样条包构建，同时解决可扩展性问题并适应未知内在低维度的理论需求


<details>
  <summary>Details</summary>
Motivation: 先前研究显示机器学习回归问题可在随机函数理论框架下解决，最优核函数与多谐样条一致，但直接应用受限于O(N^3)计算成本和输入空间维度过高时原始理论假设失效的问题

Method: 提出基于多谐样条包的级联架构，设计高效矩阵算法进行前向计算和端到端级联微分，同时为具有未知内在低维度的问题提供理论依据

Result: 级联架构同时解决了可扩展性问题，并为具有未知内在低维度的机器学习回归问题提供了理论支持

Conclusion: 多谐样条包的级联架构为高维机器学习回归问题提供了可扩展且理论合理的解决方案，克服了先前方法的计算复杂度和理论局限性

Abstract: In a previous paper it was shown that a machine learning regression problem can be solved within the framework of random function theory, with the optimal kernel analytically derived from symmetry and indifference principles and coinciding with a polyharmonic spline. However, a direct application of that solution is limited by O(N^3) computational cost and by a breakdown of the original theoretical assumptions when the input space has excessive dimensionality. This paper proposes a cascade architecture built from packages of polyharmonic splines that simultaneously addresses scalability and is theoretically justified for problems with unknown intrinsic low dimensionality. Efficient matrix procedures are presented for forward computation and end-to-end differentiation through the cascade.

</details>


### [53] [Machine Learning Algorithms: Detection Official Hajj and Umrah Travel Agency Based on Text and Metadata Analysis](https://arxiv.org/abs/2512.16742)
*Wisnu Uriawan,Muhamad Veva Ramadhan,Firman Adi Nugraha,Hasbi Nur Wahid,M Dantha Arianvasya,Muhammad Zaki Alghifari*

Main category: cs.LG

TL;DR: 本研究针对印尼朝觐服务数字化中的虚假应用欺诈问题，通过机器学习算法自动验证应用真实性，SVM算法在混合特征提取方法下达到92.3%准确率。


<details>
  <summary>Details</summary>
Motivation: 印尼朝觐和副朝服务的快速数字化虽然便利了朝觐者，但也为通过假冒移动应用进行数字欺诈开辟了途径。这些欺诈应用不仅造成经济损失，还通过收集敏感个人数据带来严重的隐私风险。

Method: 使用包含官方应用和非官方应用的综合数据集，比较三种分类器性能：支持向量机(SVM)、随机森林(RF)和朴素贝叶斯(NB)。采用混合特征提取方法，结合应用描述的文本分析(TF-IDF)和敏感访问权限的元数据分析。

Result: 实验结果表明，SVM算法表现最佳，准确率达到92.3%，精确度为91.5%，F1分数为92.0%。特征分析显示，与合法性相关的特定关键词和高风险权限(如READ PHONE STATE)是最显著的区分特征。

Conclusion: 该系统被提议作为一种主动、可扩展的解决方案，以增强宗教旅游领域的数字信任，可能作为国家验证系统的原型。

Abstract: The rapid digitalization of Hajj and Umrah services in Indonesia has significantly facilitated pilgrims but has concurrently opened avenues for digital fraud through counterfeit mobile applications. These fraudulent applications not only inflict financial losses but also pose severe privacy risks by harvesting sensitive personal data. This research aims to address this critical issue by implementing and evaluating machine learning algorithms to verify application authenticity automatically. Using a comprehensive dataset comprising both official applications registered with the Ministry of Religious Affairs and unofficial applications circulating on app stores, we compare the performance of three robust classifiers: Support Vector Machine (SVM), Random Forest (RF), and Na"ive Bayes (NB). The study utilizes a hybrid feature extraction methodology that combines Textual Analysis (TF-IDF) of application descriptions with Metadata Analysis of sensitive access permissions. The experimental results indicate that the SVM algorithm achieves the highest performance with an accuracy of 92.3%, a precision of 91.5%, and an F1-score of 92.0%. Detailed feature analysis reveals that specific keywords related to legality and high-risk permissions (e.g., READ PHONE STATE) are the most significant discriminators. This system is proposed as a proactive, scalable solution to enhance digital trust in the religious tourism sector, potentially serving as a prototype for a national verification system.

</details>


### [54] [NRGPT: An Energy-based Alternative for GPT](https://arxiv.org/abs/2512.16762)
*Nima Dehmamy,Benjamin Hoover,Bishwajit Saha,Leo Kozachkov,Jean-Jacques Slotine,Dmitry Krotov*

Main category: cs.LG

TL;DR: NRGPT模型将GPT架构与能量基模型框架结合，通过能量景观探索进行推理，在多种语言任务上表现良好且具有抗过拟合特性。


<details>
  <summary>Details</summary>
Motivation: GPT架构是语言建模的主流设计，而能量基模型将推理视为能量景观上的动态过程。研究者希望将这两种范式统一起来，探索结合两者的优势。

Method: 提出eNeRgy-GPT（NRGPT）模型，对GPT设置进行最小修改以统一能量基模型框架。将推理步骤概念化为在能量景观上对token进行探索，在某些情况下这种探索会变成梯度下降。

Result: 模型在简单语言（莎士比亚数据集）、代数ListOPS任务以及更丰富的OpenWebText语言建模场景中都表现良好。模型还显示出更强的抗过拟合能力，只在非常长的训练中才会出现过拟合。

Conclusion: NRGPT成功地将GPT架构与能量基模型框架统一，提供了一种新的语言建模方法，在保持性能的同时增强了模型的鲁棒性和抗过拟合能力。

Abstract: Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.

</details>


### [55] [Meta-RL Induces Exploration in Language Agents](https://arxiv.org/abs/2512.16848)
*Yulun Jiang,Liangze Jiang,Damien Teney,Michael Moor,Maria Brbic*

Main category: cs.LG

TL;DR: LaMer是一个元强化学习框架，让大语言模型智能体在测试时能够主动探索环境并从反馈中学习，显著提升了在需要探索的长期任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习训练的大语言模型智能体在需要主动探索的多轮长期任务中表现不佳，难以从试错经验中高效适应环境。需要一种方法让智能体能够在测试时主动探索并学习环境反馈。

Method: LaMer包含两个关键组件：1) 跨回合训练框架，鼓励探索和长期奖励优化；2) 通过反思进行上下文策略适应，使智能体能够从任务反馈信号中适应策略而无需梯度更新。

Result: 在多种环境中，LaMer显著优于强化学习基线，在Sokoban、MineSweeper和Webshop任务上分别获得11%、14%和19%的性能提升。此外，LaMer在更具挑战性或未见任务上表现出更好的泛化能力。

Conclusion: 元强化学习为语言智能体提供了一种原则性方法来引导探索，通过学习探索策略实现对新颖环境的更鲁棒适应。

Abstract: Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.

</details>


### [56] [Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models](https://arxiv.org/abs/2512.16866)
*Jiabin Xue*

Main category: cs.LG

TL;DR: 本文提出知识转换(KT)方法，结合知识蒸馏、主动学习和因果推理，为在线边缘机器学习中未见数据生成伪标签，解决边缘设备持续学习中的标注难题。


<details>
  <summary>Details</summary>
Motivation: 在线边缘机器学习需要在边缘设备上持续训练和更新模型，但面临未来未见数据如何标注的关键挑战。传统静态模型无法适应动态变化的数据分布，需要解决未见数据的标签获取问题。

Method: 提出知识转换(KT)方法，结合知识蒸馏、主动学习和因果推理。KT作为主动学习中的"预言机"，通过将教师模型的知识转换为学生模型的伪标签，实现未见数据的自动标注。

Result: 仿真实验表明：当使用相对稳定的教师模型时，学生模型最终能达到预期最大性能。KT在教师任务通用且已有预训练模型，或学生任务标签获取困难/昂贵时特别有益。

Conclusion: KT方法为解决在线边缘机器学习中未见数据标注问题提供了有效解决方案，特别适用于教师任务通用或学生任务标注困难的场景，能显著降低标注成本并提升模型适应性。

Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.

</details>


### [57] [Sequencing to Mitigate Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2512.16871)
*Hesham G. Moussa,Aroosa Hameed,Arashmid Akhavain*

Main category: cs.LG

TL;DR: 本文提出通过任务排序优化来缓解持续学习中的灾难性遗忘问题，利用零样本评分算法确定最优任务序列，实验表明智能任务排序能显著减少遗忘


<details>
  <summary>Details</summary>
Motivation: 持续学习系统需要在生命周期中增量获取、更新和利用知识，但灾难性遗忘是主要挑战。现有方法主要分为五类，本文从不同角度考虑任务呈现顺序对缓解遗忘的影响

Method: 提出通过任务排序优化来缓解灾难性遗忘，利用受神经架构搜索启发的零样本评分算法确定最优任务顺序，将任务排序与传统持续学习策略结合

Result: 智能任务排序能显著减少灾难性遗忘，与传统持续学习策略结合时能提供增强的性能和抗遗忘鲁棒性

Conclusion: 任务排序是缓解持续学习中灾难性遗忘的有效方法，所提方法也可应用于课程学习等其他领域

Abstract: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.

</details>


### [58] [Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies](https://arxiv.org/abs/2512.16876)
*Astrid Brull,Sara Aguti,Véronique Bolduc,Ying Hu,Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del-Rio,Oleksii Sliusarenko,Haiyan Zhou,Francesco Muntoni,Carsten G. Bönnemann,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 利用联邦学习平台在分布式数据集上训练机器学习模型，用于胶原VI相关肌营养不良症的诊断，相比单机构模型显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断面临数据稀缺和碎片化问题，跨机构数据共享存在隐私、监管和物流障碍，需要一种能在保护隐私的前提下实现多机构协作的解决方案。

Method: 采用联邦学习技术，通过Sherpa.ai FL平台在两个国际组织的分布式数据集上进行协作训练，使用患者来源成纤维细胞培养物的胶原VI免疫荧光显微镜图像。

Result: 开发的机器学习模型能够将胶原VI患者图像分类为COL6-RD的三种主要致病机制组：外显子跳跃、甘氨酸替代和假外显子插入，F1分数达到0.82，优于单机构模型（0.57-0.75）。

Conclusion: 联邦学习相比孤立机构模型显著提高了诊断效用和泛化能力，不仅支持更准确的诊断，还能帮助解释意义未明的变异，并指导测序策略的优先级排序以识别新的致病性变异。

Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.

</details>


### [59] [Impacts of Racial Bias in Historical Training Data for News AI](https://arxiv.org/abs/2512.16901)
*Rahul Bhargava,Malene Hornstrup Jespersen,Emily Boardman Ndulue,Vivica Dsouza*

Main category: cs.LG

TL;DR: 该论文研究了基于《纽约时报》语料库训练的AI分类器中存在的"blacks"主题标签偏见问题，发现该标签部分充当了"种族主义检测器"，但在现代案例中表现不佳，揭示了新闻编辑室采用AI工具时面临的历史偏见再现风险。


<details>
  <summary>Details</summary>
Motivation: AI技术在新闻编辑室和研究中的应用日益广泛，但这些基于历史数据训练的模型可能编码了数十年前的刻板印象和偏见。本研究旨在调查一个基于《纽约时报》语料库训练的多标签分类器中"blacks"主题标签的偏见问题，揭示AI工具在新闻工作流程中可能带来的意外输出和历史偏见再现风险。

Method: 通过定量和定性方法分析训练语料库中"blacks"标签的使用情况，应用可解释AI技术探究该标签在训练分类器中编码的概念，并测试其在现代案例（如COVID-19时期反亚裔仇恨报道和"黑人的命也是命"运动报道）中的表现。

Result: 研究发现"blacks"标签部分充当了针对某些少数群体的"种族主义检测器"，但在现代案例中表现不佳：无法有效识别COVID-19时期的反亚裔仇恨报道，对"黑人的命也是命"运动的报道处理也存在问题。这揭示了模型可能产生的意外输出。

Conclusion: 新闻编辑室在采用AI工作流程工具时面临根本性矛盾：如何利用AI技术提高效率的同时，降低历史偏见在新闻报道中再现的风险。该案例研究表明，任何大型语言模型在故事发现、受众定位、摘要生成等应用中都可能受到类似偏见影响。

Abstract: AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning "blacks" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the "blacks" label operates partially as a general "racism detector" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.

</details>


### [60] [Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning](https://arxiv.org/abs/2512.16911)
*Andrew Wagenmaker,Perry Dong,Raymond Tsao,Chelsea Finn,Sergey Levine*

Main category: cs.LG

TL;DR: 该论文提出后验行为克隆（PostBC）方法，通过建模演示者的后验分布而非直接匹配动作，为强化学习微调提供更好的初始化策略，相比标准行为克隆显著提升微调性能。


<details>
  <summary>Details</summary>
Motivation: 当前实践中，通常先在大规模演示数据集上预训练策略，然后通过强化学习微调以提升性能。然而，虽然微调算法研究较多，但如何确保预训练策略是强化学习微调的有效初始化却很少被关注。标准行为克隆可能无法覆盖演示者的所有动作，影响后续微调效果。

Method: 提出后验行为克隆（PostBC）方法：不直接拟合观察到的演示动作，而是训练策略来建模给定演示数据集下演示者行为的后验分布。这种方法仅依赖标准监督学习，可基于现代生成模型在机器人控制领域实现，确保覆盖演示者的动作同时保持与标准行为克隆相当的预训练性能。

Result: 理论分析表明标准行为克隆可能无法覆盖演示者的动作（这是有效强化学习微调的必要条件），而后验行为克隆能确保这种覆盖。在实际机器人控制基准测试和真实世界机器人操作任务中，PostBC相比标准行为克隆显著提升了强化学习微调性能。

Conclusion: 后验行为克隆（PostBC）为强化学习微调提供了更有效的初始化策略，通过建模演示者的后验分布而非直接匹配动作，既保持了预训练性能，又显著提升了后续微调效果，在机器人控制领域具有实际应用价值。

Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

</details>


### [61] [Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward](https://arxiv.org/abs/2512.16912)
*Peter Chen,Xiaopeng Li,Ziniu Li,Wotao Yin,Xi Chen,Tianyi Lin*

Main category: cs.LG

TL;DR: 本文研究了强化学习可验证奖励（RLVR）框架中的探索-利用权衡问题，揭示了虚假奖励和熵最小化如何通过看似矛盾的方式提升大语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR框架可以通过两种看似矛盾的机制提升LLMs的数学推理能力：虚假奖励（抑制利用）和熵最小化（抑制探索）。这两种机制都能改善推理性能，但其背后的原理尚不清楚，需要深入理解这些机制如何相互作用。

Method: 研究聚焦两个基本问题：策略熵与性能的关系，以及虚假奖励是否通过裁剪偏差和模型污染的相互作用产生收益。通过分析虚假奖励如何减少策略熵，以及熵最小化本身是否足够，提出了奖励错配模型来解释虚假奖励在污染设置之外的性能提升机制。

Result: 研究发现，虚假奖励下的裁剪偏差会降低策略熵，导致更自信和确定性的输出，而仅靠熵最小化不足以带来改进。提出的奖励错配模型解释了为什么虚假奖励能够在污染设置之外提升性能。

Conclusion: 研究阐明了虚假奖励效益背后的机制，为更有效的RLVR训练提供了原则性指导，揭示了探索-利用权衡在提升LLMs推理能力中的复杂动态。

Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [62] [Relational Emergent Time for Quantum System: A Multi-Observer, Gravitational, and Cosmological Framework](https://arxiv.org/abs/2512.15789)
*Amir Hossein Ghasemi*

Main category: quant-ph

TL;DR: 该论文提出了一个关系性框架，其中时间结构不是基本的，而是从全局稳态量子态的内部关联中涌现出来的。每个子系统包含一个内部时钟，条件态相对于这些内部读数有效演化。该框架自然地扩展到相对论运动、引力红移和宇宙膨胀，导出一个适用于不同物理体系的统一涌现时间函数。


<details>
  <summary>Details</summary>
Motivation: 传统物理理论将时间视为基本量，但该研究旨在探索时间是否可以从更基本的量子关联中涌现出来。作者希望建立一个统一框架，能够同时处理量子系统、相对论效应和宇宙学背景下的时间概念，从而深化对时间本质的理解。

Method: 构建一个关系性框架，其中全局量子态是稳态的，时间结构从子系统之间的关联中涌现。每个子系统配备内部时钟，通过条件态相对于这些时钟读数的演化来描述时间。该方法扩展到相对论运动、引力场和宇宙膨胀场景，推导出统一的涌现时间函数。

Result: 该理论成功再现了经典的时间膨胀效应，并预测了关联依赖的偏离标准演化的现象。特别发现非相互作用或质量为零的粒子表现出可忽略的内部时间。框架预测高度纠缠系统会出现可测量的偏离标准量子演化，并为质量为零粒子内部时间可忽略提供了理论解释。

Conclusion: 该关系性框架为理解时间的涌现本质提供了统一的理论基础，将量子力学、相对论和宇宙学中的时间概念统一起来。它开辟了从多时钟量子系统到精密计量学和宇宙学背景下的概念性和实验性研究新方向，特别是对高度纠缠系统的量子演化偏离和质量为零粒子内部时间特性的预测具有重要科学意义。

Abstract: We present a relational framework in which temporal structure is not fundamental but emerges from correlations within a globally stationary quantum state. Each subsystem includes an internal clock, and conditional states evolve effectively with respect to these internal readings. The construction naturally extends to relativistic motion, gravitational redshift, and cosmological expansion, leading to a unified emergent-time functional valid across diverse physical regimes. The theory reproduces classical time dilation, predicts correlation-dependent deviations from standard evolution, and suggests that non-interacting or massless particles exhibit negligible internal time. These consequences open directions for conceptual and experimental investigations in the foundations of temporal physics, from multi-clock quantum systems to precision metrology and cosmological settings. In particular, the framework suggests measurable deviation from standard quantum evolution for highly entangled systems and predicts negligible internal time for massless particles.

</details>


### [63] [Optimization Techniques in Quantum Information](https://arxiv.org/abs/2512.15831)
*Benjamin Desef*

Main category: quant-ph

TL;DR: 该论文开发了PolynomialOptimization.jl软件包，用于解决量子信息中的多项式优化问题，通过混合非凸和凸方法、自动转换为半定规划、资源高效中间层等技术，显著提升了大规模多项式优化问题的求解能力。


<details>
  <summary>Details</summary>
Motivation: 量子信息中的许多问题自然涉及多项式目标和约束，但现有的多项式优化软件框架已成为瓶颈，无法充分利用硬件和算法的最新进展，限制了大规模多项式优化问题的求解能力。

Method: 1. 开发PolynomialOptimization.jl开源软件包；2. 采用混合非凸和凸方法；3. 自动将多项式优化问题转换为半定规划层次结构；4. 提供资源高效的中间层和多种算法减少问题规模；5. 支持复数和对量子信息至关重要的半定约束；6. 开发近时间最优的平方和矩阵锥内点障碍计算方法。

Result: 1. 成功应用于纠缠分布问题，证明即使半定矩阵规模达到三位数和四位数也能方便求解；2. 实现了高效的算法实现，避免了界面瓶颈；3. 显著提升了多项式优化问题的求解规模和效率。

Conclusion: 该论文通过开发先进的软件工具和算法，成功解决了量子信息中多项式优化问题的计算瓶颈，为复杂量子优化问题提供了有效的解决方案，并为进一步减少资源消耗奠定了基础。

Abstract: This thesis focuses on the intersection of mathematical and computational optimization and quantum information. Main contributions are open-source software code: A hybrid approach mixing "traditional" nonconvex and convex methods can make difficult problems more accessible. A demonstration of how to efficiently implement such an algorithm, avoiding interfacial bottlenecks, is provided, finding optimal protocols to establish entanglement through a lossy channel. The central software package developed addresses polynomial optimization problems. Many problems naturally involve only a polynomial objective and constraint polynomials. Such problems can automatically be cast into semidefinite programs that provide a hierarchy of outer approximations. The resulting problems are often so large and scale so unfavorably with respect to the variable number and degree involved that the boundary of the doable is reached quickly. However, technical progress both in hardware and algorithms has pushed this boundary - but software frameworks for polynomial optimization have not followed in the same manner, often now making them the bottleneck that before was the solver. The package PolynomialOptimization.jl developed during this thesis aims to fill the gap and provide a very resource-efficient intermediate layer together with a wide number of algorithms to reduce the problem size, and naturally supporting complex numbers and semidefinite constraints ubiquitous in quantum information problems. Its application on an entanglement distribution problem is demonstrated, showing that even relaxations with semidefinite matrices of three- and four-digit size can be solved conveniently. Finally, a new way to calculate interior-point barriers for the cone of sums-of-squares matrices in a nearly time-optimal way is developed, whose efficient implementation has the potential of further reducing resource consumption.

</details>


### [64] [Efficient Simulation of Sparse, Non-Local Fermion Models](https://arxiv.org/abs/2512.15843)
*Reinis Irmejs,J. Ignacio Cirac*

Main category: quant-ph

TL;DR: 该论文提出了一种通过引入辅助费米子来消除Jordan-Wigner弦的编码方法，使得稀疏费米子模型的量子模拟在电路深度上达到渐近最优，将原有的O(log N)乘法开销降低为加法开销。


<details>
  <summary>Details</summary>
Motivation: 费米子系统的量子模拟是近期量子计算机的关键应用，但将费米子算符编码到量子比特硬件时存在Jordan-Wigner弦带来的开销问题，这限制了模拟效率。

Method: 提出了一种新的编码方案：为每个物理费米子模式添加少量辅助费米子，从而消除Jordan-Wigner弦。虽然准备辅助费米子状态需要初始开销，但该状态在时间演化中保持不变。

Result: 实现了稀疏费米子模型模拟的渐近最优电路深度，将原有的乘法O(log N)开销降低为加法开销，仅需O(dN)个辅助量子比特。

Conclusion: 该编码方案使得在量子比特硬件上模拟稀疏费米子模型的性能与理想费米子硬件相当，仅相差常数因子和O(dN)个辅助量子比特的开销。

Abstract: Efficient simulation of interacting fermionic systems is a key application of near-term quantum computers, but is hindered by the overhead required to encode fermionic operators on qubit hardware. Here, we consider models with $N$ fermionic modes in which each participates in at most a constant number $d$ of interactions and study the circuit depth required to implement the Trotterized time evolution on qubit hardware with all-to-all connectivity. We introduce an encoding that augments each physical fermionic mode with a small number of auxiliary fermions, enabling the removal of Jordan--Wigner strings. Although the preparation of the auxiliary fermion state incurs an initial overhead, this state remains invariant under time evolution. As a result, long-time evolution can be implemented with asymptotically optimal circuit depth, reducing a previously multiplicative $O(\log N)$ overhead to an additive overhead. Our results thus establish that the simulation of sparse fermionic models on qubit hardware matches the performance achievable on ideal fermionic hardware up to constant factors and $O(dN)$ ancillary qubits.

</details>


### [65] [Solvable Quantum Circuits from Spacetime Lattices](https://arxiv.org/abs/2512.15871)
*Michael A. Rampp,Suhail A. Rather,Pieter W. Claeys*

Main category: quant-ph

TL;DR: 该论文提出了"完全可约电路"框架，统一了双幺正电路及其多幺正推广，为量子多体混沌提供了新的可精确求解模型，将纠缠动力学与信息流模式、结理论联系起来。


<details>
  <summary>Details</summary>
Motivation: 虽然双幺正电路及其多幺正推广作为精确可解且混沌的量子多体动力学模型已经出现，但缺乏对多幺正动力学可解性的系统性理解。需要建立一个统一框架来涵盖更广泛的非可积但精确可解的模型。

Method: 提出了"完全可约电路"框架，将时空对称性扩展到更一般的晶格几何结构。这些电路全局上打破双幺正性但局部保持，允许超越双幺正性的丰富现象学。通过分析信息流模式来表征决定算子增长和纠缠动力学的纠缠膜。

Result: 构建了支持四向和五向信息流的电路示例。推导了纠缠线张力与时空信息流模式的一般表达式。证明了可解性与信息流结的缺失相关，将纠缠动力学与作为结不变量的Kauffman括号联系起来。提出在一般非可积动力学中，纠缠线张力的曲率可解释为信息传输密度。

Conclusion: 该研究为量子多体混沌的精确可解模型提供了一个新的统一框架，涵盖并扩展了已知构造。完全可约电路通过将纠缠动力学与信息流模式和结理论联系起来，为理解非可积量子系统的动力学提供了新视角。

Abstract: In recent years dual-unitary circuits and their multi-unitary generalizations have emerged as exactly solvable yet chaotic models of quantum many-body dynamics. However, a systematic picture for the solvability of multi-unitary dynamics remains missing. We present a framework encompassing a large class of such non-integrable models with exactly solvable dynamics, which we term \emph{completely reducible} circuits. In these circuits, the entanglement membrane determining operator growth and entanglement dynamics can be characterized analytically. Completely reducible circuits extend the notion of space-time symmetry to more general lattice geometries, breaking dual-unitarity globally but not locally, and allow for a rich phenomenology going beyond dual-unitarity. As example, we introduce circuits that support four and five directions of information flow. We derive a general expression for the entanglement line tension in terms of the pattern of information flow in spacetime. The solvability is shown to be related to the absence of knots of this information flow, connecting entanglement dynamics to the Kauffman bracket as knot invariant. Building on these results, we propose that in general non-integrable dynamics the curvature of the entanglement line tension can be interpreted as a density of information transport. Our results provide a new and unified framework for exactly solvable models of many-body quantum chaos, encompassing and extending known constructions.

</details>


### [66] [Anticoncentration and State Design of Doped Real Clifford Circuits and Tensor Networks](https://arxiv.org/abs/2512.15880)
*Beatrice Magni,Markus Heinrich,Lorenzo Leone,Xhek Turkeshi*

Main category: quant-ph

TL;DR: 该论文研究了掺杂魔法态和虚数资源的正交（实数）Clifford电路的统计特性，发现了新的普适性类别——正交Clifford Porter-Thomas分布，并揭示了资源需求的层次结构。


<details>
  <summary>Details</summary>
Motivation: 研究正交Clifford电路中掺杂魔法态和虚数资源时的统计特性，理解这些资源如何影响量子电路的全局统计行为，以及不同资源需求之间的层次关系。

Method: 开发了实数Clifford群的Weingarten微积分，推导了实数稳定子态的精确重叠分布，分析了局部实数架构在对数深度下恢复全局统计的能力。

Result: 识别了新的普适性类别——正交Clifford Porter-Thomas分布；证明了局部实数架构在对数深度内恢复全局统计；发现了资源需求的尖锐层次：恢复Haar统计需要多项式对数数量的魔法态，而恢复完整幺正Clifford统计仅需单个相位门。

Conclusion: 正交Clifford电路掺杂资源时展现出独特的统计特性，形成了新的普适性类别，并且不同统计行为的资源需求存在显著差异，为理解量子电路中的资源层次提供了新视角。

Abstract: We investigate the statistical properties of orthogonal, or real, Clifford circuits doped with magic and imaginary resources. By developing the Weingarten calculus for the real Clifford group, we derive the exact overlap distribution of real stabilizer states, identifying a new universality class: the orthogonal Clifford Porter-Thomas distribution. We prove that local real architectures recover this global statistic in logarithmic depth. Furthermore, we uncover a sharp hierarchy in resource requirements: while retrieving Haar statistics necessitates a polylogarithmic amount of magic states, recovering the full unitary Clifford statistics requires only a single phase gate.

</details>


### [67] [Deflating quantum error-correcting codes](https://arxiv.org/abs/2512.15887)
*Jaron Skovsted Gundersen,Rene Bødker Christensen,Petar Popovski,Rafał Wisniewski*

Main category: quant-ph

TL;DR: 提出了一种称为"紧缩"的新技术，用于缩短量子稳定子码的长度，这是对经典穿孔和缩短技术的推广，可以控制紧缩后量子码的参数，在某些情况下能获得比连续应用穿孔和缩短更好的参数。


<details>
  <summary>Details</summary>
Motivation: 现有量子码长度调整技术（如穿孔和缩短）存在局限性，特别是在需要移除多个量子比特时。作者希望开发一种更通用的技术，能够更好地控制量子码参数，并在某些情况下获得比传统方法更优的结果。

Method: 引入"紧缩"技术作为量子稳定子码长度减少的方法，这是对经典穿孔和缩短技术的推广，特别适用于移除多个量子比特的情况。该方法通过特定方式控制紧缩后量子码的参数，并展示了与连续应用穿孔和缩短相比的额外自由度。

Result: 紧缩技术能够有效控制量子码的参数，在某些情况下可以获得比连续应用穿孔和缩短更好的参数。同时证明了类似方法在经典线性码中并不那么有益，突显了量子码的特殊性。

Conclusion: 紧缩技术为量子稳定子码的长度调整提供了一种新的有效方法，相比传统的穿孔和缩短技术具有更多优势，特别是在需要移除多个量子比特时能获得更好的参数控制。

Abstract: In this work, we introduce a technique for reducing the length of a quantum stabilizer code, and we call this deflation of the code. Deflation can be seen as a generalization of the well-known puncturing and shortening techniques in cases where more than a single qudit is removed. We show that the parameters of the deflated quantum code can be controlled, and argue that a similar approach is not as beneficial when applied to classical linear codes. Furthermore, it is shown that deflation introduces additional freedom compared to applying just puncturing and shortening consecutively. We exemplify that it is possible to obtain better parameters by deflating a code rather than consecutively using puncturing and shortening.

</details>


### [68] [Discrete time crystals enhanced by Stark potentials in Rydberg atom arrays](https://arxiv.org/abs/2512.16097)
*Jian-Jia Wang,Ling-Zhi Tang,Yan-Xiong Du,Dan-Wei Zhang*

Main category: quant-ph

TL;DR: 本文提出在周期性驱动的里德堡原子阵列中实现无无序离散时间晶体的实验方案，利用线性势增强时间晶体序，无需无序诱导的多体局域化。


<details>
  <summary>Details</summary>
Motivation: 大多数离散时间晶体相通过无序诱导的多体局域化实现稳定，本文旨在探索无需无序的平均和特殊态制备，在里德堡原子阵列中实现无无序离散时间晶体。

Method: 利用原子失谐中的线性势（斯塔克势）增强离散时间晶体序，通过数值模拟验证该方案在周期性驱动的里德堡原子阵列中的可行性。

Result: 斯塔克势增强了离散时间晶体对翻转缺陷的鲁棒性并延长了其寿命，这些效果与初始态无关，无需无序平均和特殊态制备。

Conclusion: 该方案为在里德堡原子阵列中探索无无序离散时间晶体提供了有前景的途径，无需依赖无序诱导的多体局域化。

Abstract: Discrete time crystals (DTCs) are non-equilibrium phases in periodically driven systems that exhibit spontaneous breaking of discrete time-translation symmetry. The stabilization of most DTC phases is achieved via the disorder-induced many-body localization. In this work, we propose an experimental scheme to realize disorder-free DTCs in a periodically driven Rydberg atom array. Our scheme utilizes a linear potential in the atomic detuning to enhance the DTC order, without being tired to (Stark) many-body localization. We numerically demonstrate that the Stark potential enhances the robustness of the DTC against the flip imperfections and extends its lifetime, which are independent of initial states. Thus, our scheme provides a promising way to explore DTCs in Rydberg atom arrays without disorder averaging and special state preparation.

</details>


### [69] [Universal and Maximal Entanglement Swapping in General Fermionic Gaussian States](https://arxiv.org/abs/2512.15890)
*Jiyuan Fang,Qicheng Tang,Xueda Wen*

Main category: quant-ph

TL;DR: 在费米子高斯态中通过贝尔测量实现最大纠缠交换的普适机制


<details>
  <summary>Details</summary>
Motivation: 探索多体系统中的普适纠缠结构，特别是在系统经历非幺正操作时，这是一个基础且具有挑战性的问题。研究费米子系统中的测量诱导纠缠机制。

Method: 考虑两个初始解耦的半填充自由费米子系统，在任意维度下，对两个副本中对应位置的一半位点进行后选择贝尔测量。推导一般粒子数守恒的费米子高斯态的后测量态精确表达式。

Result: 后测量态可分解为贝尔对的乘积，建立了完全独立于初始高斯态的最大层间纠缠。通过数值模拟验证了该机制的普适性和有效性。

Conclusion: 这种现象源于费米子统计和高斯性之间的稳健相互作用，揭示了测量诱导最大纠缠的独特费米子路径，为理解多体系统中的普适纠缠结构提供了新视角。

Abstract: Exploring universal entanglement structure in many-body systems is both fundamental and challenging, particularly when the system undergoes non-unitary operations. In this work, we uncover a universal mechanism for realizing maximal entanglement swapping in fermionic Gaussian states subjected to projective Bell measurements. We consider two initially decoupled, half-filled copies of a free-fermion system in arbitrary dimensions and perform post-selective Bell measurements on half of the corresponding sites across the two copies. Remarkably, the post-measurement state factorizes into a product of Bell pairs, establishing maximal interlayer entanglement entirely independent of the initial Gaussian state. We derive this post-measurement state exactly for general particle-number-conserving fermionic Gaussian states, establishing both the validity and universality of the mechanism, with numerical simulations serving as consistency checks. This phenomenon arises from a robust interplay between fermionic statistics and Gaussianity, revealing a distinct fermionic route to measurement-induced maximal entanglement.

</details>


### [70] [Resource-resolved quantum fluctuation theorems in end-point measurement scheme](https://arxiv.org/abs/2512.15928)
*Sukrut Mondkar,Sayan Mondal,Ujjwal Sen*

Main category: quant-ph

TL;DR: 该论文提出了一个统一框架，将量子资源（非热性、相干性和纠缠）纳入涨落定理，推导了包含资源分辨贡献的量子涨落定理家族，并引入了相干性和纠缠涨落距离来量化量子资源的热力学相关性。


<details>
  <summary>Details</summary>
Motivation: 涨落定理为非平衡能量和熵涨落提供了普适约束，使其成为评估量子资源如何以及在何种程度上变得热力学相关的自然框架。然而，现有框架缺乏将通用量子资源系统性地纳入涨落定理的方法。

Method: 采用端点测量方案，避免初始能量测量，允许初始状态中的量子资源影响非平衡能量统计。推导了量子涨落定理家族，包括广义Jarzynski等式和Crooks型涨落关系，其中修正分解为资源分辨贡献。对于单系统，引入了非热性权重和相干性权重的概念；对于二分系统，使用附加相关算子和最佳可分离近似分别获得两个纠缠分辨涨落定理家族。

Result: 建立了统一的量子资源涨落定理框架，能够分离不同量子资源（非热性、相干性、纠缠）的热力学效应。引入了相干性和纠缠涨落距离作为Kullback-Leibler散度，以过程依赖和操作性的方式量化量子资源的热力学相关性。

Conclusion: 该研究为理解量子资源在非平衡热力学中的作用提供了系统框架，通过资源分辨的涨落定理和量化指标，能够精确评估量子相干性和纠缠等资源如何影响热力学过程，为量子热力学和量子资源理论建立了重要联系。

Abstract: Fluctuation theorems provide universal constraints on nonequilibrium energy and entropy fluctuations, making them a natural framework to assess how and to what extent quantum resources become thermodynamically relevant. We develop a unified framework for incorporating a generic quantum resource, including athermality, quantum coherence, and entanglement, into fluctuation theorems. We work within the end point measurement scheme, which avoids an initial energy measurement and allows quantum resources in the initial state to affect nonequilibrium energy statistics. We derive a family of quantum fluctuation theorems, including generalized Jarzynski equalities and Crooks type fluctuation relations, in which corrections decompose into resource resolved contributions. For single systems, we introduce the concept of weight of athermality, and combine it with the weight of coherence to isolate distinct thermodynamic effects of these quantum resources. For bipartite systems, we furthermore obtain two families of entanglement-resolved fluctuation theorems using an appended correlation operator and the best separable approximation, respectively. Finally, we introduce the concepts of coherence and entanglement fluctuation distances, as Kullback Leibler divergences, which quantify the thermodynamic relevance of quantum resources in a process-dependent and operational manner.

</details>


### [71] [Closed-Form Optimal Quantum Circuits for Single-Query Identification of Boolean Functions](https://arxiv.org/abs/2512.15901)
*Leonardo Bohac*

Main category: quant-ph

TL;DR: 该论文研究了在单次查询限制下，对未知单比特布尔函数进行最小误差识别的构造性解决方案，提出了一个显式的量子电路实现最优识别概率3/4。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于超越抽象的最优测量理论，为量子黑盒函数识别问题提供完全构造性的解决方案，将最优POVM转化为可在电路层面实现的门级原语，促进理论到硬件的转化。

Method: 针对单比特布尔函数的四种可能性，设计了一个低深度、固定门集的量子电路，包括显式的状态制备和测量酉操作，通过计算基读取实现Helstrom最优成功概率3/4。

Result: 成功构造了一个不需要纠缠输入状态的量子电路，在单次查询限制下实现了区分四种可能函数的最优成功概率3/4，并将最优POVM转化为可编译、验证和组合的门级原语。

Conclusion: 该研究展示了在特定参数设置下，最优黑盒识别问题不仅理论定义明确，而且可实现为构造性的电路级实现，为更大规模的最优查询识别问题提供了研究方向，强调了"oracle访问"物理意义的重要性。

Abstract: We study minimum-error identification of an unknown single-bit Boolean function given black-box (oracle) access with one allowed query. Rather than stopping at an abstract optimal measurement, we give a fully constructive solution: an explicit state preparation and an explicit measurement unitary whose computational-basis readout achieves the Helstrom-optimal success probability 3/4 for distinguishing the four possible functions. The resulting circuit is low depth, uses a fixed gate set, and (in this smallest setting) requires no entanglement in the input state. Beyond the specific example, the main message is operational. It highlights a regime in which optimal oracle discrimination is not only well-defined but implementably explicit: the optimal POVM collapses to a compact gate-level primitive that can be compiled, verified, and composed inside larger routines. Motivated by this, we discuss a "what if" question that is open in spirit: for fixed (n,m,k), could optimal k-query identification (possibly for large hypothesis classes) admit deterministic, closed-form descriptions of the inter-query unitaries and the final measurement unitary acting on the natural n+m-qubit input--output registers (and, if needed, small work registers)? Even when such descriptions are not compact and do not evade known circuit-complexity barriers for generic Boolean functions, making the optimum constructive at the circuit level would be valuable for theory-to-hardware translation and for clarifying which forms of "oracle access" are physically meaningful.

</details>


### [72] [Replica Keldysh field theory of quantum-jump processes: General formalism and application to imbalanced and inefficient fermion counting](https://arxiv.org/abs/2512.16520)
*Felix Kloiber-Tollinger,Lukas M. Sieberer*

Main category: quant-ph

TL;DR: 该论文开发了一种用于一般量子跃迁过程的综合复本Keldysh场论，统一描述了高效检测下的纯态量子轨迹和低效监测下的混合态动力学，建立了非平衡稳态相变与测量诱导动力学之间的直接联系。


<details>
  <summary>Details</summary>
Motivation: 测量诱导相变通常只研究厄米可观测量在完美检测下的情况，但这类相变也出现在更一般的场景中，包括非厄米跃迁算符的量子跃迁过程和低效检测。目前缺乏处理这些更广泛情况的理论框架。

Method: 开发了适用于玻色子和费米子系统的一般量子跃迁过程的综合复本Keldysh场论。该形式主义统一描述了高效检测下的纯态量子轨迹和低效监测下的混合态动力学，确定性Lindbladian演化作为极限情况出现。

Result: 应用该理论研究了不平衡和低效的费米子计数。对于不平衡但高效的计数，恢复了平衡情况的定性图像：任何非零跃迁率下纠缠都服从面积律，在两个参数分离的长度尺度之间出现扩展的量子临界区域。低效检测引入了有限关联长度，超过该长度后费米子对数负性度量的纠缠服从面积律，而子系统熵显示体积律标度。

Conclusion: 该工作为研究广泛类别的监测和开放量子系统中的测量诱导现象提供了通用且多功能的理沦基础，建立了非平衡稳态相变与测量诱导动力学之间的直接联系。

Abstract: Measurement-induced phase transitions have largely been explored for projective or continuous measurements of Hermitian observables, assuming perfect detection without information loss. Yet such transitions also arise in more general settings, including quantum-jump processes with non-Hermitian jump operators, and under inefficient detection. A theoretical framework for treating these broader scenarios has been missing. Here we develop a comprehensive replica Keldysh field theory for general quantum-jump processes in both bosonic and fermionic systems. Our formalism provides a unified description of pure-state quantum trajectories under efficient detection and mixed-state dynamics emerging from inefficient monitoring, with deterministic Lindbladian evolution appearing as a limiting case. It thus establishes a direct connection between phase transitions in nonequilibrium steady states of driven open quantum matter and in measurement-induced dynamics. As an application, we study imbalanced and inefficient fermion counting in a one-dimensional lattice system: monitored gain and loss of fermions occurring at different rates, with a fraction of gain and loss jumps undetected. For imbalanced but efficient counting, we recover the qualitative picture of the balanced case: entanglement obeys an area law for any nonzero jump rate, with an extended quantum-critical regime emerging between two parametrically separated length scales. Inefficient detection introduces a finite correlation length beyond which entanglement, as quantified by the fermionic logarithmic negativity, obeys an area law, while the subsystem entropy shows volume-law scaling. Numerical simulations support our analytical findings. Our results offer a general and versatile theoretical foundation for studying measurement-induced phenomena across a wide class of monitored and open quantum systems.

</details>


### [73] [Scalable tests of quantum contextuality from stabilizer-testing nonlocal games](https://arxiv.org/abs/2512.16654)
*Wanbing Zhao,H. W. Shawn Liew,Wen Wei Ho,Chunxiao Liu,Vir B. Bulchandani*

Main category: quant-ph

TL;DR: 该论文研究稳定子测试非局域游戏的经典值上界，证明如果经典值小于1，则必然≤7/8，并对GHZ、环面码和循环簇态等具体例子给出了更紧的上界。


<details>
  <summary>Details</summary>
Motivation: 稳定子码字可以通过上下文性提供量子优越性的简单证明，但稳定子测试游戏的经典值在GHZ态之外的可扩展例子中大多未知，需要建立系统方法来界定这些经典值。

Method: 提出多种新方法来上界稳定子测试游戏的经典值：1) 证明通用编码理论界；2) 对GHZ、环面码和循环簇态等常见可扩展例子收紧界限；3) 使用转移矩阵方法为循环簇态建立渐近紧的上界。

Result: 证明如果经典值p_cl*<1，则p_cl*≤7/8；对循环簇态建立了渐近紧的上界；发现测量循环簇态的指数小保真度就足以见证其上下文性。

Conclusion: 该工作为稳定子测试游戏的经典值提供了系统分析框架，揭示了稳定子态的上下文性特征，特别是循环簇态仅需指数小保真度测量即可证明量子优越性。

Abstract: Soon after the dawn of quantum error correction, DiVincenzo and Peres observed that stabilizer codewords could give rise to simple proofs of quantumness via contextuality. This discovery can be recast in the language of nonlocal games: every $n$-qubit stabilizer state defines a specific "stabilizer-testing" $n$-player nonlocal game, which quantum players can win with probability one. If quantum players can moreover outperform all possible classical players, then the state is contextual. However, the classical values of stabilizer-testing games are largely unknown for scalable examples beyond the $n$-qubit GHZ state. We introduce several new methods for upper-bounding the classical values of these games. We first prove a general coding-theory bound for all stabilizer-testing games: if the classical value $p_{\mathrm{cl}}^* < 1$, then $p_{\mathrm{cl}}^* \leq 7/8$, i.e., there is no classical strategy that can perform as well as the optimal quantum strategy even in an asymptotic sense. We then show how to tighten this bound for the most common scalable examples, namely GHZ, toric-code and cyclic cluster states. In particular, we establish an asymptotically tight upper bound for cyclic cluster states using transfer-matrix methods. This leads to the striking conclusion that measuring an exponentially small fidelity to the cyclic cluster state will suffice to witness its contextuality.

</details>


### [74] [AC Stark effect or time-dependent Aharonov-Bohm effect for particle on a ring](https://arxiv.org/abs/2512.15935)
*Patrick Hinrichs,Douglas Singleton,Nader Inan*

Main category: quant-ph

TL;DR: 研究环形约束量子粒子在时变螺线管矢势下的效应，类似时变版Aharonov-Bohm效应但存在场作用，与交流斯塔克效应比较，产生可观测的准能边带


<details>
  <summary>Details</summary>
Motivation: 研究量子粒子在环形约束下受时变螺线管矢势影响的物理效应，探索这种类似时变Aharonov-Bohm效应但与标准交流斯塔克效应不同的新现象

Method: 分析环形约束量子粒子在时变螺线管矢势下的动力学行为，通过理论模型研究其与标准交流斯塔克效应的异同

Result: 发现该设置会产生准能边带，这些边带可通过光谱学观测到，效应与交流斯塔克效应相似但电场来源不同（来自矢势而非标势）

Conclusion: 环形约束量子粒子在时变螺线管矢势下表现出独特效应，虽类似Aharonov-Bohm效应但存在场作用，与交流斯塔克效应有重要区别，产生可观测的准能边带特征

Abstract: We study the effect of a time-varying solenoidal vector potential for a quantum particle confined to a ring. The setup appears to be a time-varying version of the Aharonov-Bohm effect, but since the particle moves in the presence of fields, it is not strictly an Aharonov-Bohm effect. The results are similar to the ac Stark effect, but with a time-varying electric field coming from the vector potential, rather than the scalar potential. We compare and contrast the present effect with the standard ac Stark effect. The signature of this setup is the generation of quasi-energy sidebands which are observable via spectroscopy.

</details>


### [75] [Numerically exact open quantum system work statistics with process tensors](https://arxiv.org/abs/2512.16823)
*Mike Shubrook,Moritz Cygorek,Erik Gauger,Jake Iles-Smith,Ahsan Nazir*

Main category: quant-ph

TL;DR: 该论文提出了一种过程张量框架，用于精确计算驱动开放量子系统的完整量子功统计，超越了弱耦合、马尔可夫和慢驱动等传统近似限制。


<details>
  <summary>Details</summary>
Motivation: 准确量化量子操作的热力学功成本对于量子技术的发展至关重要，但在快速控制和复杂非平衡环境（许多当代量子设备的操作条件）下，传统近似方法失效，需要新的精确计算方法。

Method: 引入过程张量框架，能够计算驱动开放量子系统的完整数值精确量子功统计，该方法具有非微扰精度，适用于超越弱耦合、马尔可夫和慢驱动极限的情况。

Result: 将方法应用于Landauer擦除协议，发现功概率分布显示出传统低阶矩方法忽略的独特量子特征，这些特征显著影响协议的擦除保真度。

Conclusion: 该框架为表征驱动开放量子系统中的能量交换波动提供了强大而通用的工具，为探索近期和未来量子设备的操作区域中的热力学和控制问题奠定了基础。

Abstract: Accurately quantifying the thermodynamic work costs of quantum operations is essential for the continued development and optimisation of emerging quantum technologies. This present a significant challenge in regimes of rapid control within complex, non-equilibrium environments - conditions under which many contemporary quantum devices operate and conventional approximations break down. Here, we introduce a process tensor framework that enables the computation of the full numerically exact quantum work statistics of driven open quantum systems. We demonstrate the utility of our approach by applying it to a Landauer erasure protocol operating beyond the weak-coupling, Markovian, and slow-driving limits. The resulting work probability distributions reveal distinct quantum signatures that are missed by low-order moments yet significantly impact the erasure fidelity of the protocol. Our framework delivers non-perturbative accuracy and detail in characterising energy-exchange fluctuations in driven open quantum systems, establishing a powerful and versatile tool for exploring thermodynamics and control in the operating regimes of both near-term and future quantum devices.

</details>


### [76] [Random coding for long-range continuous-variable QKD](https://arxiv.org/abs/2512.15990)
*Arpan Akash Ray,Boris Skoric*

Main category: quant-ph

TL;DR: 本文提出了一种适用于长距离高斯调制连续变量量子密钥分发的随机码本纠错方法，使用似然比评分和基于阈值的块拒绝机制，预测实时密钥率可达Devetak-Winter值的8%以上。


<details>
  <summary>Details</summary>
Motivation: 连续变量量子密钥分发（CVQKD）虽然与标准电信设备兼容，但在长距离传输中面临极低的信噪比，需要劳动密集型的纠错过程，且实时实现纠错解码具有挑战性。

Method: 提出随机码本纠错方法，采用似然比评分和基于阈值的块拒绝机制。为证明技术原因，接受/拒绝决策以加密形式通信，避免在泄漏分析中处理非高斯态。该方法高度可并行化。

Result: 在计算资源的保守假设下，预测实时密钥率至少达到Devetak-Winter值的8%，优于现有的协调方案。

Conclusion: 所提出的随机码本纠错方法适用于长距离高斯调制CVQKD，具有高度可并行化特性，有利于实时实现，并在性能上超越了现有协调方案。

Abstract: Quantum Key Distribution (QKD) schemes are key exchange protocols based on the physical properties of quantum channels. They avoid the computational-hardness assumptions that underlie the security of classical key exchange. Continuous-Variable QKD (CVQKD), in contrast to qubit-based discrete-variable (DV) schemes, makes use of quadrature measurements of the electromagnetic field. CVQKD has the advantage of being compatible with standard telecom equipment, but at long distances has to deal with very low signal to noise ratios, which necessitates labour-intensive error correction. It is challenging to implement the error correction decoding in realtime.
  In this paper we introduce a random-codebook error correction method that is suitable for long range Gaussian-modulated CVQKD. We use likelihood ratio scoring with block rejection based on thresholding. For proof-technical reasons, the accept/reject decisions are communicated in encrypted form; in this way we avoid having to deal with non-Gaussian states in the analysis of the leakage. The error correction method is highly parallelisable, which is advantageous for realtime implementation. Under conservative assumptions on the computational resources, we predict a realtime key ratio of at least 8% of the Devetak-Winter value, which outperforms existing reconciliation schemes.

</details>


### [77] [Many-body contextuality and self-testing quantum matter via nonlocal games](https://arxiv.org/abs/2512.16886)
*Oliver Hart,David T. Stephen,Evan Wickenden,Rahul Nandkishore*

Main category: quant-ph

TL;DR: 该论文研究了量子上下文性与CSS纠错码的关系，通过多人非局域量子游戏量化量子态的非经典性，并建立了与经典统计力学模型和拓扑序的联系。


<details>
  <summary>Details</summary>
Motivation: 量子上下文性是量子力学区别于经典物理的基本特性，对量子计算加速至关重要。论文旨在通过CSS纠错码的多人量子游戏来量化量子态的上下文性，并建立与经典统计力学和拓扑序的联系。

Method: 引入基于CSS纠错码的多人非局域量子游戏家族，通过单点泡利测量获胜。使用辅助超图态的对称性分析方法，计算经典获胜概率。建立与经典统计力学模型和对称保护拓扑态的奇异关联子的联系。

Result: 计算了多个典型CSS码的经典获胜概率，展示了与经典统计力学模型的对应关系。提出了CSS子测量游戏，可用于自测试（如2D环面码），并实现了多体态中广泛的上下文性概念。

Conclusion: CSS纠错码的量子游戏为量化多体量子态的上下文性提供了系统框架，建立了量子上下文性与经典统计力学、拓扑序之间的深刻联系，为量子计算和量子信息处理提供了新视角。

Abstract: Contextuality is arguably the fundamental property that makes quantum mechanics different from classical physics. It is responsible for quantum computational speedups in both magic-state-injection-based and measurement-based models of computation, and can be directly probed in a many-body setting by multiplayer nonlocal quantum games. Here, we discuss a family of games that can be won with certainty when performing single-site Pauli measurements on a state that is a codeword of a Calderbank-Shor-Steane (CSS) error-correcting quantum code. We show that these games require deterministic computation of a code-dependent Boolean function, and that the classical probability of success is upper bounded by a generalized notion of nonlinearity/nonquadraticity. This success probability quantifies the state's contextuality, and is computed via the function's (generalized) Walsh-Hadamard spectrum. To calculate this, we introduce an efficient, many-body-physics-inspired method that involves identifying the symmetries of an auxiliary hypergraph state. We compute the classical probability of success for several paradigmatic CSS codes and relate it to both classical statistical mechanics models and to strange correlators of symmetry-protected topological states. We also consider CSS submeasurement games, which can only be won with certainty by sharing the appropriate codeword up to local isometries. These games therefore enable self-testing, which we illustrate explicitly for the 2D toric code. We also discuss how submeasurement games enable an extensive notion of contextuality in many-body states.

</details>


### [78] [Stationary two-qubit entanglement mediated by one-dimensional plasmonic nanoarrays](https://arxiv.org/abs/2512.16016)
*Luke C. Ugwuoke,Tjaart P. J. Krüger,Mark S. Tame*

Main category: quant-ph

TL;DR: 研究金属纳米粒子阵列对量子点量子比特间纠缠的远程等离子体介导作用，发现奇数阵列能维持更稳定的纠缠


<details>
  <summary>Details</summary>
Motivation: 纠缠是纳米光子系统中量子关联的关键度量，在量子光学等领域有重要应用。先前研究表明，使用金属纳米粒子作为介质可以保持两个量子点量子比特之间的纠缠程度。本研究旨在探索通过多个金属纳米粒子进行远程等离子体介导对量子比特间纠缠的影响。

Method: 采用满足弱耦合近似的共线且周期性排列的金属纳米粒子阵列。在腔量子电动力学框架内建立有效模型，推导介质相互作用。在单粒子共振频率下进行弱驱动，分析不同阵列配置对稳态纠缠的影响。

Result: 模型显示奇数阵列对纠缠衰减更具鲁棒性。这归因于奇数阵列中杂化偶极子等离子体与驱动频率共振产生的强量子比特间耗散耦合。这些阵列能够在超过1微米的量子比特间距下维持非零稳态纠缠。

Conclusion: 奇数金属纳米粒子阵列能够有效介导远程量子比特纠缠，为实现每个量子点的独立空间光学探测提供了可能性。

Abstract: Entanglement is one of the key measures of quantum correlations present in nanophotonic systems, with promising applications in quantum optics and beyond. Previous studies have shown that the degree of entanglement between two quantum dot qubits is preserved when a metal nanoparticle is used to mediate the interactions between the qubits. In this work, we investigate long-range plasmonic mediation of qubit--qubit entanglement by studying the impact of the number of mediating metal nanoparticles on stationary concurrence. Collinear and periodically spaced metal nanoparticles that satisfy the weak-coupling approximation are considered. An effective model that enables the derivation of the mediated interactions within the framework of cavity quantum electrodynamics is employed. Under weak driving at the single particle resonance frequency, the model shows that odd-number arrays are more robust to entanglement decay. We attribute this to strong inter-qubit dissipative coupling as a result of a hybridized dipole plasmon resonating with the driving frequency in odd-number arrays. These arrays can sustain non-vanishing stationary entanglement beyond an inter-qubit spacing of one micron, opening up the possibility of independent spatial optical probing of each quantum dot.

</details>


### [79] [Antisymmetrization of composite fermionic states for quantum simulations of nuclear reactions in first-quantization mapping](https://arxiv.org/abs/2512.16138)
*Ionel Stetcu*

Main category: quant-ph

TL;DR: 提出了一种用于反称化空间分离的靶-弹系统的第一量子化确定性算法，通过Dicke态辅助寄存器编码单粒子交换通道，仅需O(N_T N_p)次单粒子交换操作。


<details>
  <summary>Details</summary>
Motivation: 在反应和散射模拟中，需要处理包含N_T个靶粒子和N_p个弹粒子的空间分离系统，这些系统包含全同费米子，需要构造完全反称化的波函数。现有方法在处理这类系统时存在可扩展性限制。

Method: 算法从两个独立反称化的多体态（可以是Slater行列式的叠加）的乘积出发，使用Dicke态辅助寄存器相干编码两个子系统间的所有单粒子交换通道。关键是通过单粒子交换生成完全反称结构，需要O(N_T N_p)次单粒子交换，其中最多N_p个可以并行实现。费米子相位通过应用Z门到N_T个辅助量子位来纳入，然后通过紧凑的受控操作序列高效地解除辅助寄存器的计算。

Result: 该算法为第一量子化量子算法提供了一种非平凡且可扩展的协议，用于在反应和散射模拟中准备完全反称化态，显著扩展了可处理系统的范围。

Conclusion: 该工作提出了一种高效的反称化算法，通过巧妙的辅助寄存器设计和并行化交换操作，为第一量子化量子模拟中的多费米子系统处理提供了重要工具。

Abstract: I present a first-quantization deterministic algorithm for antisymmetrizing a spatially separated target-projectile system containing $N_T$ and $N_p$ identical fermions, respectively. The method constructs a fully antisymmetric wavefunction from the product of two independently antisymmetrized many-body states, each of which may be a superposition of Slater determinants. The algorithm uses a Dicke-state ancilla register that coherently encodes all one-particle exchange channels between the two subsystems, and, crucially, requires only single-particle swaps to generate the full antisymmetric structure. A total of $O(N_T N_p)$ single-particle exchanges are needed, with up to $N_p$ of them implemented in parallel, if an additional $N_p$ ancillae are used. The correct fermionic phase is incorporated through application of $Z$ gates on $N_T$ ancillae, after which the ancilla register is efficiently uncomputed using a compact sequence of controlled operations. This construction provides a nontrivial and scalable protocol for preparing fully antisymmetric states in reaction and scattering simulations, significantly expanding the range of systems that can be addressed with first-quantized quantum algorithms.

</details>


### [80] [Tunneling in double-well potentials within stochastic quantization: Application to ammonia inversion](https://arxiv.org/abs/2512.16168)
*Danilo F. Schafaschek,Giovani L. Vasconcelos,Antônio M. S. Macêdo*

Main category: quant-ph

TL;DR: 该研究使用随机量子化方法分析双势阱中的隧穿时间统计，推导了平均隧穿时间与量子力学隧穿时间的关系，并以氨分子为例验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随机量子化为量子现象提供了基于扩散过程的替代框架，能够研究标准量子力学中不易定义的动力学量，特别是隧穿时间统计问题。

Method: 采用随机量子化框架，结合首达时间理论，计算双势阱中隧穿时间的平均值和概率分布，并通过数值模拟验证理论预测。

Result: 对于方形势阱推导出解析表达式，在高势垒极限下建立了随机力学与量子力学隧穿时间的关系τ_QM = (π/2)τ̄，氨分子反演频率计算为约24GHz，与实验观测一致。

Conclusion: 随机量子化是分析量子系统隧穿现象的强大且具有物理洞察力的框架，为研究量子动力学提供了新的视角和工具。

Abstract: Stochastic quantization - introduced by Nelson in 1966 - describes quantum behavior as a conservative diffusion process in which a particle undergoes Brownian-like motion with a fluctuation amplitude set by Planck's constant. While it fully reproduces conventional quantum mechanics, this approach provides an alternative framework that enables the study of dynamical quantities not easily defined within the standard formulation. In the present work, stochastic quantization is employed to investigate tunneling-time statistics for bound states in double-well potentials. Using first-passage time theory within the stochastic quantization framework, both the mean tunneling time, $\barτ$, and the full probability distribution, $p(τ)$, are computed, and the theoretical predictions are validated through extensive numerical simulations of stochastic trajectories for the two potentials considered as representative cases. For the square double-well potential, analytical expressions for $\barτ$ are derived and show excellent agreement with simulations. In the high-barrier limit, the results reveal a direct relation between the stochastic-mechanical and quantum-mechanical tunneling times, expressed as $τ_{\mathrm{QM}} = (π/2)\barτ$, where $τ_{\mathrm{QM}}$ corresponds to half the oscillation period of the probability of finding the particle in either well. This relation is further confirmed for generic double-well systems through a WKB analysis. As a concrete application, the inversion dynamics of the ammonia molecule is analyzed, yielding an inversion frequency of approximately $24$ GHz, in close agreement with experimental observations. These results highlight the potential of stochastic quantization as a powerful and physically insightful framework for analyzing tunneling phenomena in quantum systems.

</details>


### [81] [Entropy Stability and Spectral Concentration under Convex Block Constraints](https://arxiv.org/abs/2512.16192)
*Hassan Nasreddine*

Main category: quant-ph

TL;DR: 论文研究了量子态在凸块对角约束下的熵最小化问题，证明了定量稳定性定理：如果某态在固定块约束下的熵与最小值相差ε，则该态在迹范数下距离熵最小化流形为O(ε^{1/2})，且该速率最优。


<details>
  <summary>Details</summary>
Motivation: 研究量子态在凸块对角约束下的熵最小化问题的稳定性，为量子信息理论中的熵优化问题提供定量分析框架。

Method: 采用完全有限维的分析方法，基于熵的经典和内部分量的精确分解，结合尖锐的相对熵不等式进行证明。

Result: 证明了定量稳定性定理：熵与最小值的偏差ε对应迹范数距离为O(ε^{1/2})，且该速率是最优的。应用于有限加性算子的谱分解，得到诱导谱测度的定量非集中界。

Conclusion: 该框架抽象且独立于算术输入，为线性谱约束下的熵最小化器提供了通用的稳定性原理，在量子信息理论中具有重要应用价值。

Abstract: We investigate entropy minimization problems for quantum states subject to convex block-diagonal constraints. Our principal result is a quantitative stability theorem: if a state has entropy within epsilon of the minimum possible value under a fixed block constraint, then it must lie within O(epsilon^{1/2}) in trace norm of the manifold of entropy minimizers. We show that this rate is optimal. The analysis is entirely finite-dimensional and relies on a precise decomposition of entropy into classical and internal components, together with sharp relative entropy inequalities. As an application, we study finite additive operators whose spectral decomposition induces natural block constraints. In this setting, the stability theorem yields quantitative non-concentration bounds for induced spectral measures. The framework is abstract and independent of arithmetic input. It provides a general stability principle for entropy minimizers under linear spectral constraints.

</details>


### [82] [Amplifying Decoherence-Free Many-Body Interactions with Giant Atoms Coupled to Parametric Waveguide](https://arxiv.org/abs/2512.16232)
*Xin Wang,Zhao-Min Gao*

Main category: quant-ph

TL;DR: 将巨原子与行波参量波导结合，实现增强且抗噪声的量子相互作用，为量子模拟提供可扩展平台


<details>
  <summary>Details</summary>
Motivation: 传统参量放大通过场压缩增强量子相互作用，但会引入额外噪声加速量子退相干，限制了量子信息处理的可扩展性。巨原子通过多点耦合波导可设计干涉抑制耗散，但尚未与压缩场结合。

Method: 将压缩放大相互作用扩展到新型量子平台：结合巨原子与基于χ²非线性的行波参量波导。利用不同耦合点间的相消干涉，使巨原子间相互作用不仅显著增强，而且对压缩噪声免疫。

Result: 巨原子表现出交换和配对两种相互作用，适合模拟多体量子物理。这些相互作用的强度可通过调节压缩和耦合参数平滑调谐。与传统无压缩泵的波导量子电动力学不同，该平台具有独特优势。

Conclusion: 该架构为强关联物理的量子模拟提供了多功能且可扩展的平台，为多体体系中的鲁棒量子控制开辟了新途径。

Abstract: Parametric amplification offers a powerful means to enhance quantum interactions through field squeezing, yet it typically introduces additional noise which accelerates quantum decoherence, a major obstacle for scalable quantum information processing. The squeezing field is implemented in cavities rather than continuous waveguides, thereby limiting its scalability for applications in quantum simulation. Giant atoms, which couple to waveguides at multiple points, provide a promising route to mitigate dissipation via engineered interference, enabling decoherence-free interactions. We extend the squeezing-amplified interaction to a novel quantum platform combining giant atoms with traveling-wave parametric waveguides based on $χ^{(2)}$ nonlinearity. By exploiting destructive interference between different coupling points, the interaction between giant atoms is not only significantly enhanced but also becomes immune to squeezed noise. Unlike conventional waveguide quantum electrodynamics without a squeezing pump, the giant emitters exhibit both exchange and pairing interactions, making this platform particularly suitable for simulating many-body quantum physics. More intriguingly, the strengths of these interactions can be smoothly tuned by adjusting the squeezing and coupling parameters. Our architecture thus provides a versatile and scalable platform for quantum simulation of strongly correlated physics and paves the way toward robust quantum control in many-body regimes.

</details>


### [83] [Self-testing GHZ state via a Hardy-type paradox](https://arxiv.org/abs/2512.16242)
*Smritikana Patra,Soumyajit Pal,Ranendu Adhikary*

Main category: quant-ph

TL;DR: 本文提出了一种基于Hardy非定域性论证推广的GHZ态自测试协议，证明了达到最大Hardy成功概率的关联是量子关联集的暴露点，并建立了Hardy型悖论与Mermin不等式之间的统一视角。


<details>
  <summary>Details</summary>
Motivation: 自测试是一种基于关联的框架，能够在不对设备内部结构做任何假设的情况下验证量子态和测量。本文旨在为GHZ态开发一种基于Hardy非定域性论证推广的自测试协议，并探索其在实验不完美情况下的鲁棒性。

Method: 引入基于Hardy非定域性论证自然推广的GHZ态自测试协议，证明达到最大Hardy成功概率的关联是量子关联集的暴露点。针对实验不完美情况，开发了专门针对Hardy构造的鲁棒自测试分析。同时展示了在该场景下，达到Hardy型悖论最大违反的量子关联与达到Mermin不等式最大违反的关联相同。

Result: 证明了达到最大Hardy成功概率的关联构成量子关联集的极值点，并且该点是暴露的。建立了Hardy型悖论与Mermin不等式之间的统一视角，表明同一多体关联既具有逻辑悖论解释，又具有Bell不等式特征。为研究N方Hardy悖论最大违反的关联在更高方数情况下是否保持暴露性奠定了基础。

Conclusion: 本文提出的自测试协议为GHZ态提供了基于Hardy论证的验证框架，证明了相关关联的暴露性，并建立了逻辑悖论与Bell不等式之间的统一视角，为探索更高方数情况下的暴露性问题铺平了道路。

Abstract: Self-testing is a correlation-based framework that enables the certification of both the underlying quantum state and the implemented measurements without imposing any assumptions on the internal structure of the devices. In this work, we introduce a self-testing protocol for the Greenberger-Horne-Zeilinger (GHZ) state based on a natural generalization of Hardy's nonlocality argument. Within this framework, we prove that the correlation achieving the maximal Hardy success probability constitutes an extremal point of the quantum correlation set and, moreover, that this point is \emph{exposed}. To address experimentally relevant imperfections, we further develop a robust self-testing analysis tailored to the Hardy construction. Additionally, we show that, in this scenario, the quantum correlation that attains the maximal violation of the Hardy-type paradox coincides with the correlation that yields the maximal violation of the Mermin inequality. This establishes a unified perspective in which the same multipartite correlation admits both a logical-paradox interpretation and a Bell-inequality-based characterization. Collectively, our results pave the way for investigating whether the correlations that maximally violate the generalized $N$-party Hardy paradox remain exposed in higher-party regimes.

</details>


### [84] [Prefix Sums via Kronecker Products](https://arxiv.org/abs/2512.16309)
*Aleksandros Sobczyk,Anastasios Zouzias*

Main category: quant-ph

TL;DR: 该论文通过线性代数视角重新审视前缀和问题，提出了一种新的电路设计方法，同时实现了零缺陷、每层常数扇出和小于2log(n)的深度，并应用于量子加法器设计。


<details>
  <summary>Details</summary>
Motivation: 重新审视前缀和问题，通过线性代数方法寻找更优的电路设计，特别是要同时满足零缺陷、常数扇出和更小的深度这三个关键属性。

Method: 利用线性代数中的恒等式，将三角全1矩阵分解为两个Kronecker积的和，基于此设计递归的前缀和算法和电路。

Result: 提出的电路家族首次同时实现了零缺陷、每层常数扇出和深度严格小于2log(n)（对于输入长度n）。应用于量子加法器设计，实现了1.893log(n)+O(1)的Toffoli深度、O(n)个Toffoli门和O(n)个额外量子比特。

Conclusion: 通过线性代数方法重新审视前缀和问题，可以设计出同时满足多个理想特性的电路，这些电路在量子加法器等应用中能够显著改进现有构造的深度和规模。

Abstract: In this work, we revisit prefix sums through the lens of linear algebra. We describe an identity that decomposes triangular all-ones matrices as a sum of two Kronecker products, and apply it to design recursive prefix sum algorithms and circuits. Notably, the proposed family of circuits is the first one that achieves the following three properties simultaneously: (i) zero-deficiency, (ii) constant fan-out per-level, and (iii) depth that is asymptotically strictly smaller than $2\log(n)$ for input length n. As an application, we show how to use these circuits to design quantum adders with $1.893\log(n) + O(1)$ Toffoli depth, $O(n)$ Toffoli gates, and $O(n)$ additional qubits, improving the Toffoli depth and/or Toffoli size of existing constructions.

</details>


### [85] [Feedback Cooling and Thermometry of a Single Trapped Ion Using a Knife Edge](https://arxiv.org/abs/2512.16368)
*Hans Dang,Sebastian Luff,Martin Fischer,Markus Sondermann,Gerd Leuchs*

Main category: quant-ph

TL;DR: 首次实现单个囚禁离子的反馈冷却，温度低于多普勒极限9倍


<details>
  <summary>Details</summary>
Motivation: 传统多普勒冷却存在温度极限（ħΓ/2kB），需要突破这一限制以实现更低的离子温度

Method: 使用刀口成像实时监测离子运动，通过荧光强度调制生成反馈信号，利用抛物面镜提高光子收集效率

Result: 成功将单个囚禁离子冷却到多普勒极限温度的1/9以下

Conclusion: 首次实现了单个囚禁离子的反馈冷却，突破了多普勒冷却极限，为量子信息处理和精密测量提供了新方法

Abstract: We report on the first feedback cooling of a single trapped ion below the Doppler limit of $\hbarΓ/2 k_\mathrm{B}$. The motion of a single ion is monitored in real-time and cooled up to 9-times below the Doppler cooling temperature by applying electronic feedback. Real-time motion detection is implemented by imaging the fluorescence photons emitted by the ion onto a knife edge and detecting the transmitted light, a method used so far to cool trapped nanoparticles. The intensity modulation of the fluorescence resulting from the ion motion is used to generate and apply the feedback signal and also to determine the ion temperature. The method benefits from a high rate of detected scattered photons, which can be a challenge, and which we address by using a parabolic mirror for collecting the fluorescence.

</details>


### [86] [Instantaneous velocity during quantum tunnelling](https://arxiv.org/abs/2512.16385)
*Xiao-Wen Shang,Jian-Peng Dou,Feng Lu,Sen Lin,Hao Tang,Xian-Min Jin*

Main category: quant-ph

TL;DR: 该论文通过分析量子隧穿过程的时间演化，揭示了粒子在势垒内的速度会从较大的初始值逐渐减小，在渐逝态中甚至趋近于零，同时概率密度在势垒内逐渐积累形成稳定分布，解决了稳态速度为零却存在有限概率密度的表观悖论。


<details>
  <summary>Details</summary>
Motivation: 量子隧穿是量子力学的基本现象，支撑着从核聚变、光合作用到超导量子比特操作等关键过程。然而，粒子在隧穿过程中的动力学行为仍然微妙且存在争议，特别是关于粒子在势垒内的运动速度问题。

Method: 通过分析隧穿过程的时间演化，研究粒子速度在势垒内的连续弛豫行为；从稳态方程出发，推导出粒子速度与势垒宽度之间的显式关系；比较基于概率密度和概率电流定义的有效速度。

Result: 发现粒子在势垒内的速度从较大的初始值逐渐减小，在渐逝态中甚至趋近于零；概率密度在势垒内逐渐积累形成稳定分布；推导出速度与势垒宽度的关系，表明在足够宽的势垒中渐逝态速度趋近于零；指出基于概率密度定义的有效速度可能导致虚假的非零"稳态速度"。

Conclusion: 该研究建立了隧穿流形成的清晰动力学图像，解决了稳态速度为零与有限概率密度共存的表观悖论，为测试时间分辨隧穿现象提供了理论基础，并纠正了先前研究中基于概率密度定义有效速度可能导致的误解。

Abstract: Quantum tunnelling, a hallmark phenomenon of quantum mechanics, allows particles to pass through the classically forbidden region. It underpins fundamental processes ranging from nuclear fusion and photosynthesis to the operation of superconducting qubits. Yet the underlying dynamics of particle motion during tunnelling remain subtle and are still the subject of active debate. Here, by analyzing the temporal evolution of the tunnelling process, we show that the particle velocity inside the barrier continuously relaxes from a large initial value toward a smaller one, and may even approach zero in the evanescent regime. Meanwhile, the probability density within the barrier gradually builds up before reaching its stationary profile, in contrast to existing inherently. In addition, starting from the steady-state equations, we derive an explicit relation between the particle velocity and the barrier width, and show that the velocity in evanescent states approaches zero when the barrier is sufficiently wide. These findings resolve the apparent paradox of a vanishing steady-state velocity coexisting with a finite particle density. We point out that defining an effective speed from the probability density, rather than from the probability current, can lead to spuriously nonzero "stationary speed," as appears to be the case in Ref. [Nature 643, 67 (2025)]. Our work establishes a clear dynamical picture for the formation of tunnelling flow and provides a theoretical foundation for testing time-resolved tunnelling phenomena.

</details>


### [87] [Quantum-Inspired Ising Machines for Quantum Chemistry Calculations](https://arxiv.org/abs/2512.16435)
*Mahmood Hasani,Hadis Salasi,Negar Ashari Astani*

Main category: quant-ph

TL;DR: 量子启发算法（相干伊辛机和模拟分岔算法）能准确模拟H₂和H₂O的电子能量分布，计算速度比门基量子计算快5倍以上


<details>
  <summary>Details</summary>
Motivation: 量子模拟是量子计算最有前景的应用之一，但受限于近期硬件的噪声问题。量子启发算法提供了一种有吸引力的替代方案，避免了易出错的量子设备需求

Method: 使用相干伊辛机和模拟分岔算法这两种量子启发算法来模拟H₂和H₂O分子的电子能量分布

Result: 成功再现了H₂和H₂O的电子能量分布，捕捉了其基本能量特征。计算时间分别为1.2秒和2.4秒，比门基量子计算方法（通常需要至少6秒计算单个分子几何结构）快5倍以上

Conclusion: 量子启发算法在扩展到更大分子系统以及未来在化学和材料科学应用中具有巨大潜力

Abstract: Four decades after Richard Feynman's famous remark, we have reached a stage at which nature can be simulated quantum mechanically. Quantum simulation is among the most promising applications of quantum computing. However, like many quantum algorithms, it is severely constrained by noise in near-term hardware. Quantum-inspired algorithms provide an attractive alternative by avoiding the need for error-prone quantum devices. In this study, we demonstrate that the coherent Ising machine and simulated bifurcation algorithms can accurately reproduce the electronic energy profiles of H_2 and H_2O, capturing their essential energetic features. Notably, we obtain computational times of 1.2 s and 2.4 s for the H_2 and H_2O profiles, respectively, representing a substantial speed-up compared to gate-based quantum computing approaches, which typically require at least 6 s to compute a single molecular geometry with comparable accuracy. These results highlight the potential of quantum-inspired approaches for scaling to larger molecular systems and for future applications in chemistry and materials science.

</details>


### [88] [Classical and quantum electromagnetic momentum in anisotropic optical waveguides](https://arxiv.org/abs/2512.16495)
*Denis Kopylov,Manfred Hammer*

Main category: quant-ph

TL;DR: 该论文证明了介质通道波导中的导模满足与光电磁场动量相关的正交条件，为集成光子电路中宽带导引电磁场的量化提供了严格的自洽方法。


<details>
  <summary>Details</summary>
Motivation: 当前在集成光子电路中，经典麦克斯韦方程组的导引场解与波导中光子的直观理解之间存在理论鸿沟，需要建立严格的量化方法。

Method: 通过证明介质波导中的导模满足与动量相关的正交条件，建立了与功率（能量）正交性的联系，为宽带导引电磁场的量化提供了自洽程序。

Result: 建立了适用于直线、无损、各向异性、任意形状介质波导的严格量化方法，填补了经典场解与波导光子理解之间的理论空白，并以铌酸锂薄膜条波导的混合模为例进行了讨论。

Conclusion: 该工作为集成光子电路中导引电磁场的量子化提供了理论基础，消除了经典场论与量子光学描述之间的不一致性，适用于包括材料色散在内的线性体系。

Abstract: The guided modes supported by dielectric channel waveguides act as individual carriers of momentum. We show this by proving that the modes satisfy an orthogonality condition which relates to the momentum of the optical electromagnetic field, with a link to the more familiar power (energy) orthogonality. This result forms the basis for a rigorous, self-consistent procedure for the quantization of broadband guided electromagnetic fields in the typical channels used in integrated photonic circuits. Our work removes the existing theoretical gap between the classical solution of the Maxwell equations for guided fields and the intuitive understanding of photons in waveguides. The presented approach is valid for straight, lossless, and potentially anisotropic, dielectric waveguides of general shape, in the linear regime, and including material dispersion. Examples for the hybrid modes of a thin film lithium niobate strip waveguide are briefly discussed.

</details>


### [89] [Wichmann-Kroll vacuum polarization density in a finite Gaussian basis set](https://arxiv.org/abs/2512.16569)
*Ryan Benazzouk,Maen Salman,Trond Saue*

Main category: quant-ph

TL;DR: 该研究进一步发展了有限高斯基下QED效应的计算，重点关注真空极化密度中非线性α(Zα)^{n≥3}项对氢类离子1s_{1/2}态能量位移的贡献，旨在达到与文献中格林函数方法相当的精度。


<details>
  <summary>Details</summary>
Motivation: 改进数值计算精度，使其达到与现有格林函数方法相当的水平，为氢类离子1s_{1/2}态提供更精确的真空极化能量位移计算。

Method: 使用Riesz投影器推导真空极化密度线性贡献的解析表达式；采用有限高斯基方案并研究其收敛性；使用偶调谐基组进行完全基组极限外推；进行误差分析评估方法对数值噪声的鲁棒性。

Result: 开发了计算能量位移的策略，能够实现足够精度以进行部分波展开的合理外推；建立了偶调谐基组外推方法；对数值困难进行了表征并评估了方法的鲁棒性。

Conclusion: 该研究成功发展了有限高斯基下计算真空极化效应的数值方法，通过解析推导、收敛性分析和误差控制，为高精度计算氢类离子QED效应提供了有效工具。

Abstract: This work further develops the calculation of QED effects in a finite Gaussian basis. We focus on the non-linear $α(Zα)^{n\ge 3}$ contribution to the vacuum polarization density, computing the energy shift of 1s$_{1/2}$ states of hydrogen-like ions. Our goal is to improve the numerical computations to achieve a precision comparable to that of Green's function methods reported in the literature. To do so, an analytic expression for the linear contribution to the vacuum polarization density is derived using Riesz projectors. Alternative formulations of the vacuum polarization density and their relation is discussed. The convergence of the finite Gaussian basis scheme is investigated, and the numerical difficulties that arise are characterized. In particular, an error analysis is performed to assess the method's robustness to numerical noise. We then report a strategy for computing the energy shift with sufficient precision to enable a sensible extrapolation of the partial-wave expansion. A key feature of the procedure is the use of even-tempered basis sets, allowing for an extrapolation towards the complete basis set limit.

</details>


### [90] [The measured speed in the evanescent regime reflects the spatial decay of the wavefunction, not particle motion](https://arxiv.org/abs/2512.16580)
*Weixiang Ye*

Main category: quant-ph

TL;DR: 论文回应了Sharoglazova等人的研究，澄清了能量相关参数ν在玻姆力学中的真正含义，指出该实验实际上支持而非挑战玻姆力学的本体论框架。


<details>
  <summary>Details</summary>
Motivation: 针对Sharoglazova等人对玻姆力学的误解进行澄清，他们错误地将实验中提取的参数ν解释为量子粒子的速度，并声称其有限值与玻姆力学预测的零粒子速度相矛盾。

Method: 通过分析ν在玻姆力学本体论框架中的操作意义，论证ν实际上量化的是波函数振幅的空间梯度，这是引导场的几何性质，而非点状粒子的运动学速度。

Result: 研究表明，实验测量的ν参数与玻姆力学中粒子的实际速度无关，因此实验并不挑战玻姆力学，反而清晰地展示了波和粒子方面的本体论分离。

Conclusion: Sharoglazova等人的实验误解了ν参数的意义，该实验实际上验证了玻姆力学中波函数引导场与粒子运动之间的本体论区分，而非反驳玻姆力学。

Abstract: The recent paper by Sharoglazova et al. reports an energy-dependent parameter $ν$ extracted from the spatial distribution of photons in a coupled-waveguide experiment. The authors interpret $ν$ as the speed of quantum particles, even in the classically forbidden regime, and claim that its finite value contradicts the Bohmian mechanics prediction of zero particle velocity. This challenge arises from a fundamental misunderstanding of the operational meaning of v within the Bohmian ontological framework. We demonstrate that v quantifies the spatial gradient of the wavefunctions amplitude, a geometric property of the guiding field, not the kinematical velocity of point-like particles. The experiment therefore does not challenge but rather illustrates the clean ontological separation between the wave and particle aspects inherent to Bohmian mechanics.

</details>


### [91] [Giant-atom quantum acoustodynamics in hybrid superconducting-phononic integrated circuits](https://arxiv.org/abs/2512.16582)
*Lintao Xiao,Bo Zhang,Yu Zeng,Xiaoxuan Pan,Jia-Qi Wang,Ziyue Hua,Hongwei Huang,Yifang Xu,Guangming Xue,Haifeng Yu,Xin-Biao Xu,Weiting Wang,Chang-Ling Zou,Luyan Sun*

Main category: quant-ph

TL;DR: 研究人员通过将超导transmon量子比特耦合到铌酸锂声子波导的两个相距约600个声波波长的点上，创建了一个"巨原子"系统，展示了非马尔可夫弛豫动力学和频率相关的有效衰变率。


<details>
  <summary>Details</summary>
Motivation: 探索声子集成电路作为实现巨原子物理学的多功能平台，为高级量子信息处理提供高度可调的量子器件。

Method: 将超导transmon量子比特耦合到铌酸锂声子波导的两个点上，两点间距约600个声波波长（传播延迟125纳秒），形成巨原子系统。

Result: 巨原子表现出非马尔可夫弛豫动力学，具有声子回流特性；有效衰变率在仅4 MHz范围内变化四倍，对应Purcell因子超过40；利用频率相关耗散制备了高纯度的量子叠加态。

Conclusion: 铌酸锂声子集成电路是研究巨原子物理学的理想平台，能够实现高度可调的量子器件，为高级量子信息处理提供了新的可能性。

Abstract: We demonstrate a giant atom by coupling a superconducting transmon qubit to a lithium niobate phononic waveguide at two points separated by about 600 acoustic wavelengths, with a propagation delay of 125 ns. The giant atom yields non-Markovian relaxation dynamics characterized by phonon backflow and a frequency-dependent effective decay rate varying four-fold over merely 4 MHz, corresponding to a Purcell factor exceeding 40. Exploiting this frequency-dependent dissipation, we prepare quantum superposition states with high purity. Our results establish phononic integrated circuits as a versatile platform for giant-atom physics, providing highly tunable quantum devices for advanced quantum information processing.

</details>


### [92] [Indistinguishable photons from a two-photon cascade](https://arxiv.org/abs/2512.16617)
*Timon L. Baltisberger,Francesco Salusti,Mark R. Hogg,Malwina A. Marczak,Nils Heinisch,Sascha R. Valentin,Stefan Schumacher,Arne Ludwig,Klaus D. Jöns,Richard J. Warburton*

Main category: quant-ph

TL;DR: 通过Purcell增强双激子跃迁，在低噪声量子点器件中实现了高相干性的纠缠光子对，XX和X光子的双光子干涉可见度分别达到94%和82%。


<details>
  <summary>Details</summary>
Motivation: 虽然量子点中的双激子级联可以产生纠缠光子对，但通常XX和X光子之间存在时间相关性，导致光子相干性较差。研究旨在提高纠缠光子对的相干性。

Method: 在低噪声器件中通过Purcell效应增强双激子跃迁，调节XX:X寿命比超过两个数量级，并测量双光子干涉可见度来评估光子相干性。

Result: 实现了高双光子干涉可见度：XX光子为94±2%，X光子为82±6%。光子相干性随XX:X寿命比的变化符合已知的量子光学理论结果。

Conclusion: 通过Purcell增强双激子跃迁，可以在量子点中产生高相干性的纠缠光子对，为量子信息处理提供了高质量的纠缠光子源。

Abstract: Decay of a four-level diamond scheme via a cascade is a potential source of entangled photon pairs. A solid-state implementation is the biexciton cascade in a semiconductor quantum dot. While high entanglement fidelities have been demonstrated, the two photons, XX and X, are temporally correlated, typically resulting in poor photon coherence. Here, we demonstrate a high two-photon interference visibility (a measure of the photon coherence) for both XX (V=94$\pm$2%) and X (V=82$\pm$6%) photons. This is achieved by Purcell-enhancing the biexciton transition in a low-noise device. We find that the photon coherence follows the well-known quantum optics result upon tuning the XX:X lifetime ratio over two orders of magnitude.

</details>


### [93] [Fast Native Three-Qubit Gates and Fault-Tolerant Quantum Error Correction with Trapped Rydberg Ions](https://arxiv.org/abs/2512.16641)
*Katrin Bolsmann,Thiago L. M. Guedes,Weibin Li,Joseph W. P. Wilkinson,Igor Lesanovsky,Markus Müller*

Main category: quant-ph

TL;DR: 该论文提出了一种基于里德伯离子的三量子比特受控-受控-Z门方案，利用微波修饰的里德伯态实现快速多量子比特操作，并展示了其在容错量子纠错中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统离子阱量子计算平台中，基于集体运动的纠缠门速度慢且难以扩展。里德伯离子通过强长程偶极-偶极相互作用，为实现快速多量子比特操作提供了有前景的途径。

Method: 提出首个基于微波修饰里德伯离子的原生受控-受控-Z门方案，通过优化单脉冲协议考虑里德伯态有限寿命，并开发了九量子比特Bacon-Shor码的容错、无测量量子纠错方案。

Result: 在现实条件下，该门保真度超过97%，执行时间约2微秒（低温环境）。模拟证实，在线性里德伯离子链上可以实现完全容错的量子纠错，尽管其量子比特连接性有限。

Conclusion: 原生多量子比特里德伯离子门是快速、高保真量子计算的有价值资源，在容错量子纠错方面具有显著潜力，为可扩展量子信息处理提供了新途径。

Abstract: Trapped ions as one of the most promising quantum-information-processing platforms, yet conventional entangling gates mediated by collective motion remain slow and difficult to scale. Exciting trapped ions to high-lying electronic Rydberg states provides a promising route to overcome these limitations by enabling strong, long-range dipole-dipole interactions that support much faster multi-qubit operations. Here, we introduce the first scheme for implementing a native controlled-controlled-Z gate with microwave-dressed Rydberg ions by optimizing a single-pulse protocol that accounts for the finite Rydberg-state lifetime. The resulting gate outperforms standard decompositions into one- and two-qubit gates by achieving fidelities above 97% under realistic conditions, with execution times of about 2 microseconds at cryogenic temperatures. To explore the potential of trapped Rydberg ions for fault-tolerant quantum error correction, and to illustrate the utility of three-qubit Rydberg-ion gates in this context, we develop and analyze a proposal for fault-tolerant, measurement-free quantum error correction using the nine-qubit Bacon-Shor code. Our simulations confirm that quantum error correction can be performed in a fully fault-tolerant manner on a linear Rydberg-ion chain despite its limited qubit connectivity. These results establish native multiqubit Rydberg-ion gates as a valuable resource for fast, high-fidelity quantum computing and highlight their potential for fault-tolerant quantum error correction.

</details>


### [94] [Shaping Dynamics Through Memory: A Study of Reservoir Profiles in Open Quantum Systems](https://arxiv.org/abs/2512.16657)
*J. R. Silva,C. Antunis B. S. Santos*

Main category: quant-ph

TL;DR: 研究不同储层记忆核（洛伦兹型、高斯型、均匀型）对波导系统动力学的影响，分析其对传输特性、透明度和非马尔可夫效应的影响。


<details>
  <summary>Details</summary>
Motivation: 探究不同储层记忆核如何影响波导系统的动力学演化，特别是不同空间关联特性对系统行为的影响，以及如何量化与马尔可夫动力学的偏离。

Method: 比较三种代表性记忆核（洛伦兹型、高斯型、均匀型），计算传输振幅、透明度特性和长时间行为，使用基于信息回流的非马尔可夫性度量来量化与马尔可夫极限的偏离。

Result: 结果揭示了无记忆诱导的传输谱修改的清晰特征，并展示了特定储层轮廓如何增强或抑制非马尔可夫效应。

Conclusion: 不同储层记忆核对波导系统动力学有显著影响，特定的储层轮廓可以调控非马尔可夫效应，为理解结构化储层中的量子输运提供了重要见解。

Abstract: In this work, we investigate how different reservoir memory profiles influence the dynamical evolution of a single waveguide coupled to an external environment. We compare three representative memory kernels: Lorentzian, Gaussian and Uniform, highlighting their distinct spatial correlations and their impact on system behavior. We compute the transmission amplitude, transparency properties, as well as long-time behavior of the system under each memory model. To quantify deviations from Markovian dynamics, we employ a non-Markovianity measure based on information backflow, allowing a direct comparison between the structured reservoirs and the Markovian limit. Our results reveal clear signatures of memoryless-induced modifications in the transmission spectrum and demonstrate how specific reservoir profiles enhance or suppress non-Markovian effects.

</details>


### [95] [Topological magic response in quantum spin chains](https://arxiv.org/abs/2512.16673)
*Ritu Nehra,Poetri Sonya Tarabunga,Martina Frau,Mario Collura,Emanuele Tirrito,Marcello Dalmonte*

Main category: quant-ph

TL;DR: 论文提出"拓扑魔法响应"概念，用于探测拓扑相在非Clifford扰动下的响应，揭示非局域量子关联。在Ising型自旋链中，对称破缺相和顺磁相缺乏这种响应，而对称保护拓扑相总是表现出该响应。


<details>
  <summary>Details</summary>
Motivation: 虽然纠缠与拓扑的关系已确立，但非稳定化（魔法）在拓扑相中的作用尚不清楚。非稳定化是容错量子计算的关键概念，需要研究其在拓扑物质中的作用。

Method: 引入拓扑魔法响应概念，结合稳定子Rényi熵（类似拓扑纠缠熵）来隔离非局域存储信息。使用精确解析计算和基于新算法技术的矩阵乘积态模拟，研究SPT相掺杂T门的情况。

Result: 对称破缺相和顺磁相缺乏拓扑魔法响应，而对称保护拓扑相总是表现出该响应。掺杂T门的SPT相支持鲁棒的拓扑魔法响应，而平凡相保持无特征。

Conclusion: 拓扑魔法响应是探测拓扑相对非Clifford扰动响应的有效工具，揭示了非局域量子关联，为理解拓扑相中的非稳定化提供了新视角。

Abstract: Topological matter provides natural platforms for robust, non-local information storage, central to quantum error correction. Yet, while the relation between entanglement and topology is well established, little is known about the role of nonstabilizerness (or magic), a pivotal concept in fault-tolerant quantum computation, in topological phases. We introduce the concept of topological magic response, the ability of a state to spread over stabilizer space when perturbed by finite-depth non-Clifford circuits. Unlike a topological invariant or order parameter, this response function probes how a phase reacts to non-Clifford perturbations, revealing the presence of non-local quantum correlations. In Ising-type spin chains, we show that symmetry-broken and paramagnetic phases lack such a response, whereas symmetry-protected topological (SPT) phases always display it. To capture this, we utilize a combination of stabilizer Rényi entropies that, in analogy with topological entanglement entropy, isolates non-locally stored information. Using exact analytic computations and matrix product states simulations based on an algorithmic technique we introduce, we show that SPT phases doped with $T$ gates support robust topological magic response, while trivial phases remain featureless.

</details>


### [96] [Symbolic Pauli Propagation for Gradient-Enabled Pre-Training of Quantum Circuits](https://arxiv.org/abs/2512.16674)
*Saverio Monaco,Jamal Slim,Florian Rehm,Dirk Krücker,Kerstin Borras*

Main category: quant-ph

TL;DR: 提出了一种基于泡利传播的量子机器学习方法，通过符号表示和可控截断实现可扩展的梯度估计，避免了昂贵的片上训练。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习模型通常需要昂贵的片上训练程序，并且缺乏高效的梯度估计方法。现有方法在电路深度增加时计算复杂度急剧增长，限制了可扩展性。

Method: 采用泡利传播技术，推导可观测量作为电路参数的解析函数的符号表示。通过合适的ansatz设计和可控截断（泡利权重和频率分量截断），获得准确且可处理的目标观测量估计器。

Result: 该方法在变分量子本征求解器中得到验证，能够准确获得自旋模型的基态。通过合适的ansatz设计，该方法可以扩展到经典模拟无法处理的系统规模，实现可扩展训练。

Conclusion: 提出的方法提供了一种可扩展且计算高效的量子机器学习训练方法，支持在量子硬件部署前进行基于梯度的经典预训练，为大规模量子系统的高效训练提供了新途径。

Abstract: Quantum Machine Learning models typically require expensive on-chip training procedures and often lack efficient gradient estimation methods. By employing Pauli propagation, it is possible to derive a symbolic representation of observables as analytic functions of a circuit's parameters. Although the number of terms in such functional representations grows rapidly with circuit depth, suitable choices of ansatz and controlled truncations on Pauli weights and frequency components yield accurate yet tractable estimators of the target observables. With the right ansatz design, this approach can be extended to system sizes beyond the reach of classical simulation, enabling scalable training for larger quantum systems. This also enables a form of classical pre-training through gradient-based optimization prior to deployment on quantum hardware. The proposed approach is demonstrated on the Variational Quantum Eigensolver for obtaining the ground state of a spin model, showing that accurate results can be achieved with a scalable and computationally efficient procedure.

</details>


### [97] [On the Dynamics of Local Hidden-Variable Models](https://arxiv.org/abs/2512.16682)
*Nick von Selzam,Florian Marquardt*

Main category: quant-ph

TL;DR: 该论文探讨了贝尔非定域性的动态扩展，研究在静态相关保持局域的情况下，相关性的时间演化是否可以通过隐变量的演化来描述。作者提出了动态局域隐变量模型的概念，并通过反例和严格的不可行定理证明这种动态模型并不总是存在，从而揭示了一种新的非定域性类型。


<details>
  <summary>Details</summary>
Motivation: 贝尔非定域性是量子力学的重要特性，但传统定义仅关注静态相关性。作者希望探索在动态情境下，即使每个时刻的静态相关性都是局域的，相关性的时间演化是否仍然可以通过局域隐变量模型来描述，从而扩展对量子非定域性的理解。

Method: 作者首先定义了动态局域隐变量模型，并讨论了可能的物理和数学假设。然后通过构造一个简单的反例来质疑这种动态模型的存在性，最后通过严格的不可行定理从理论上证明动态局域隐变量模型并不总是存在。

Result: 研究结果表明，即使每个时刻的静态相关性都可以用局域隐变量模型描述，相关性的时间演化也可能无法通过演化隐变量来捕捉。作者通过反例和不可行定理证明了动态局域隐变量模型并不总是存在，从而揭示了一种新的非定域性类型。

Conclusion: 该工作提出了一种基于测量统计时间演化的新型非定域性，这种非定域性在相互作用的量子系统中普遍存在。这扩展了传统贝尔非定域性的概念，为理解量子系统的动态特性提供了新的视角。

Abstract: Bell nonlocality is an intriguing property of quantum mechanics with far reaching consequences for information processing, philosophy and our fundamental understanding of nature. However, nonlocality is a statement about static correlations only. It does not take into account dynamics, i.e. time evolution of those correlations. Consider a dynamic situation where the correlations remain local for all times. Then at each moment in time there exists a local hidden-variable (LHV) model reproducing the momentary correlations. Can the time evolution of the correlations then be captured by evolving the hidden variables? In this light, we define dynamical LHV models and motivate and discuss potential additional physical and mathematical assumptions. Based on a simple counter example we conjecture that such LHV dynamics does not always exist. This is further substantiated by a rigorous no-go theorem. Our results suggest a new type of nonlocality that can be deduced from the observed time evolution of measurement statistics and which generically occurs in interacting quantum systems.

</details>


### [98] [Propagators of singular anharmonic oscillators with quasi-equidistant spectra](https://arxiv.org/abs/2512.16695)
*Andrey M. Pupasov-Maksimov,Marcelo Silva Oliveira*

Main category: quant-ph

TL;DR: 该论文研究了奇异谐振子的达布变换，利用镜像法获得传播子的解析表达式，提出了双势阱和三势阱势能族及其传播子，并识别了对应的轴对称磁场构型。


<details>
  <summary>Details</summary>
Motivation: 研究奇异谐振子的达布变换，旨在获得相关传播子的解析表达式，并探索由此产生的多势阱势能系统及其在轴对称磁场构型中的应用。

Method: 采用达布变换方法处理奇异谐振子，运用镜像法处理形式奇异传播子以获得解析表达式，构建双势阱和三势阱势能族。

Result: 获得了奇异谐振子传播子的解析表达式，提出了双势阱和三势阱势能族及其对应的传播子，并识别了与这些势能对应的轴对称磁场构型。

Conclusion: 达布变换方法成功应用于奇异谐振子，获得了传播子的解析解，构建了多势阱势能系统，并建立了与轴对称磁场构型的对应关系，为相关物理系统研究提供了新工具。

Abstract: Darboux transformations of the singular harmonic oscillator are considered. Analytical expressions for the propagators are obtained, using the image method applied to formal singular propagators. Two-well and three-well families of potentials and the corresponding propagators are presented. Axially symmetric magnetic field configurations corresponding to these potentials have been identified.

</details>


### [99] [Reconstruction of Quantum Fields](https://arxiv.org/abs/2512.16775)
*Nicolás Medina Sánchez,Borivoje Dakić*

Main category: quant-ph

TL;DR: 论文提出了一种通过可区分粒子状态空间的商空间构造不可区分粒子状态空间的新方法，并基于三个操作原理推导出新的产生-湮灭代数，这些代数再现了超越传统玻色子和费米子的统计最大推广的配分函数。


<details>
  <summary>Details</summary>
Motivation: 传统上通过产生和湮灭代数引入玻色子和费米子的方法基于第二量子化语言。本文旨在通过可区分粒子状态空间的商空间来形式化从第一量子化到第二量子化的过渡，推广通常的对称化程序，并基于操作原理推导新的产生-湮灭代数。

Method: 通过取可区分粒子状态空间的商空间，使得等价类识别不包含能够区分粒子信息的态，从而构造不可区分粒子空间。假设该空间满足三个操作原理：(i) 存在与观测者可标记可访问模式兼容的有序基，(ii) 在这些模式的幺正变换下不变，(iii) 支持粒子计数作为模式上的局域操作。基于这些原理推导新的产生-湮灭代数。

Result: 推导出了一类新的产生-湮灭代数，这些代数能够再现超越传统玻色子和费米子的统计最大推广（transtatistics-maximal generalisations）的配分函数，这些推广与所提出的操作原理一致。

Conclusion: 通过可区分粒子状态空间的商空间方法，结合三个操作原理，可以系统地推导出超越传统玻色子和费米子的新的产生-湮灭代数，为量子统计的推广提供了新的理论基础。

Abstract: One of the traditional ways of introducing bosons and fermions is through creation and annihilation algebras. Historically, these have been associated with emission and absorption processes at the quantum level and are characteristic of the language of second quantization. In this work, we formulate the transition from first to second quantization by taking quotients of the state spaces of distinguishable particles, so that the resulting equivalence classes identify states that contain no information capable of distinguishing between particles, thereby generalising the usual symmetrisation procedure. Assuming that the resulting indistinguishable-particle space (i) admits an ordered basis compatible with how an observer may label the accessible modes, (ii) is invariant under unitary transformations of those modes, and (iii) supports particle counting as a mode-wise local operation, we derive a new class of creation-annihilation algebras. These algebras reproduce the partition functions of transtatistics-maximal generalisations of bosons and fermions consistent with these operational principles.

</details>


### [100] [A magic criterion (almost) as nice as PPT, with applications in distillation and detection](https://arxiv.org/abs/2512.16777)
*Zhenhuan Liu,Tobias Haug,Qi Ye,Zi-Wen Liu,Ingo Roth*

Main category: quant-ph

TL;DR: 提出了一种混合态魔术判据——三角判据，它在魔术检测中扮演类似纠缠中PPT判据的角色，具有强检测能力、几何解释和与魔术蒸馏的操作联系。


<details>
  <summary>Details</summary>
Motivation: 为了开发类似于纠缠检测中PPT判据的混合态魔术检测工具，需要建立能够有效检测多量子比特魔术态、揭示魔术蒸馏协议能力差异的判据。

Method: 引入三角判据作为混合态魔术检测工具，分析其几何特性，证明该判据在张量积下的不稳定性，推导魔术态最小纯度的上界。

Result: 发现多量子比特魔术蒸馏协议严格强于单量子比特方案，证明低秩多量子比特魔术态几乎不可能被单量子比特协议蒸馏，预测存在无法被保真度基魔术见证检测的"不忠实魔术态"。

Conclusion: 三角判据为魔术检测提供了强有力的工具，揭示了多量子比特魔术蒸馏的优势，并指出了单拷贝魔术检测方案的基本局限性。

Abstract: We introduce a mixed-state magic criterion, the Triangle Criterion, which plays a role for magic analogous to the Positive Partial Transposition (PPT) criterion for entanglement: it combines strong detection capability, a clear geometric interpretation, and an operational link to magic distillation. Using this criterion, we uncover several new features of multi-qubit magic distillation and detection. We prove that genuinely multi-qubit magic distillation protocols are strictly more powerful than all single-qubit schemes by showing that the Triangle Criterion is not stable under tensor products, in sharp contrast to the PPT criterion. Moreover, we show that, with overwhelming probability, multi-qubit magic states with relatively low rank cannot be distilled by any single-qubit distillation protocol. We derive an upper bound on the minimal purity of magic states, which is conjectured to be tight with both numerical and constructive evidences. Using this minimal-purity result, we predict the existence of unfaithful magic states, namely states that cannot be detected by any fidelity-based magic witness, and reveal fundamental limitations of mixed-state magic detection in any single-copy scheme.

</details>


### [101] [Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences](https://arxiv.org/abs/2512.16778)
*Theshani Nuradha,Ian George,Christoph Hirche*

Main category: quant-ph

TL;DR: 该论文针对量子曲棍球散度建立了非线性强数据处理不等式，改进了现有线性SDPI，并应用于量子隐私通道的隐私保证和有限混合时间分析。


<details>
  <summary>Details</summary>
Motivation: 现有的线性强数据处理不等式在量子信息处理中往往不够紧致，无法提供最优的区分度衰减界限，需要建立更精确的非线性SDPI来改进量子状态区分度的分析。

Method: 针对满足特定噪声准则的量子信道，建立量子曲棍球散度的非线性SDPI；定义推广Dobrushin曲线的F_γ曲线来表征异构信道序列组合的SDPI；推导具有曲棍球散度约束的f-散度的反向Pinsker型不等式。

Result: 提出的非线性SDPI改进了现有线性SDPI和经典曲棍球散度的非线性SDPI；能够建立线性SDPI无法实现的更紧致的有限混合时间界限；在量子局部差分隐私框架下为序列私有量子信道的组合提供更强的隐私保证。

Conclusion: 非线性强数据处理不等式为量子信息处理提供了更精确的分析工具，在量子状态区分、信道组合分析和量子隐私保护等方面具有重要应用价值，显著改进了现有线性方法的局限性。

Abstract: Data-processing is a desired property of classical and quantum divergences and information measures. In information theory, the contraction coefficient measures how much the distinguishability of quantum states decreases when they are transmitted through a quantum channel, establishing linear strong data-processing inequalities (SDPI). However, these linear SDPI are not always tight and can be improved in most of the cases. In this work, we establish non-linear SDPI for quantum hockey-stick divergence for noisy channels that satisfy a certain noise criterion. We also note that our results improve upon existing linear SDPI for quantum hockey-stick divergences and also non-linear SDPI for classical hockey-stick divergence. We define $F_γ$ curves generalizing Dobrushin curves for the quantum setting while characterizing SDPI for the sequential composition of heterogeneous channels. In addition, we derive reverse-Pinsker type inequalities for $f$-divergences with additional constraints on hockey-stick divergences. We show that these non-linear SDPI can establish tighter finite mixing times that cannot be achieved through linear SDPI. Furthermore, we find applications of these in establishing stronger privacy guarantees for the composition of sequential private quantum channels when privacy is quantified by quantum local differential privacy.

</details>


### [102] [Nonstabilizerness in Stark many-body localization](https://arxiv.org/abs/2512.16859)
*Han-Ze Li,Yi-Rui Zhang,Yu-Jun Zhao,Xuyang Huang,Jian-Xin Zhong*

Main category: quant-ph

TL;DR: 该研究通过横向场伊辛链中的无无序Stark多体局域化，发现稳定子Rényi熵在强Stark场区域保持非零并缓慢增长至有限平台，表明非稳定子性（"魔法"）可作为无无序遍历性破坏和约束局域化的实用复杂性探针。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索无无序多体局域化中的非稳定子性（魔法）行为，填补对无无序多体局域化中非稳定子性理解的空缺，为近期量子模拟器的基准测试和设计提供实用工具。

Method: 使用实现无无序Stark多体局域化的横向场伊辛链，分析稳定子Rényi熵的行为，研究强Stark场区域下魔法和纠缠的长时间演化，考察初始态选择性对结果的影响。

Result: 稳定子Rényi熵在强Stark场区域保持非零并缓慢增长至有限平台，表现出强烈的初始态选择性。随着Stark场强度增加，长时间魔法和纠缠一致显示从遍历动力学到约束局域化动力学的交叉转变。

Conclusion: 非稳定子性（魔法）可作为无无序遍历性破坏和约束局域化的实用复杂性探针，对近期量子模拟器的基准测试和设计具有直接相关性，填补了无无序多体局域化中非稳定子性理解的空缺。

Abstract: Quantum many-body disorder-free localization can suppress transport while still allowing the buildup of computationally costly non-Clifford resources. In a transverse-field Ising chain realizing disorder-free Stark many-body localization, we show that the stabilizer Rényi entropy remains nonzero and grows slowly to a finite plateau deep in the strong Stark-field regime, with strong initial-state selectivity. As the Stark field strength increases, long-time magic and entanglement consistently signal a crossover from ergodic to constrained localized dynamics. These results establish nonstabilizerness (``magic'') as a practical complexity probe for disorder-free ergodicity breaking and constrained localization, with direct relevance to benchmarking and designing near-term quantum simulators, and fill a gap in the understanding of nonstabilizerness in disorder-free many-body localization.

</details>


### [103] [Random purification channel for passive Gaussian bosons](https://arxiv.org/abs/2512.16878)
*Francesco Anna Mele,Filippo Girardi,Senrui Chen,Marco Fanizza,Ludovico Lami*

Main category: quant-ph

TL;DR: 本文构建了一个高斯版本的随机纯化信道，该信道给定n个未知的玻色被动高斯态副本，能够制备n个随机选择的高斯纯化副本，且每个纯化的平均光子数恰好是初始态的两倍。


<details>
  <summary>Details</summary>
Motivation: 随机纯化信道在量子信息理论中已被证明是极其有价值的工具，但需要构建其高斯版本以处理玻色被动高斯态。

Method: 利用对偶约化酉群对的表示理论，通过表征被动高斯酉算子的交换子来构造高斯随机纯化信道。

Result: 成功构建了高斯随机纯化信道，该信道能够为被动高斯态生成随机高斯纯化，且每个纯化的平均光子数恰好是初始态的两倍。

Conclusion: 该工作扩展了随机纯化信道的应用范围到高斯态领域，为量子信息理论提供了新的工具，并揭示了被动高斯态纯化的数学结构。

Abstract: The random purification channel, which, given $n$ copies of an unknown mixed state $ρ$, prepares $n$ copies of an associated random purification, has proved to be an extremely valuable tool in quantum information theory. In this work, we construct a Gaussian version of this channel that, given $n$ copies of a bosonic passive Gaussian state, prepares $n$ copies of one of its randomly chosen Gaussian purifications. The construction has the additional advantage that each purification has a mean photon number which is exactly twice that of the initial state. Our construction relies on the characterisation of the commutant of passive Gaussian unitaries via the representation theory of dual reductive pairs of unitary groups.

</details>


### [104] [Advantage of Warm Starts for Electron-Phonon Systems on Quantum Computers](https://arxiv.org/abs/2512.16879)
*Arnab Adhikary,S. E. Skelton,Alberto Nocera,Mona Berciu*

Main category: quant-ph

TL;DR: 该研究针对量子计算机模拟电子-声子相互作用提出了一种新的初始态构建方法，显著提高了强耦合区域的基态重叠度，从而减少了量子相位估计所需的迭代次数。


<details>
  <summary>Details</summary>
Motivation: 量子计算机模拟电子-声子相互作用仍然具有挑战性，现有研究主要集中在哈密顿量模拟和电路优化上。需要寻找更有效的方法来降低量子相位估计的计算成本。

Method: 研究单电子Holstein模型，提出了一种初始态构建方法，该方法在强耦合区域能显著提高基态重叠度。该构建方法可以高效实现，相对于传统初始猜测能指数级降低整体电路成本。

Result: 提出的初始态构建方法在强耦合区域显著提高了基态重叠度，减少了量子相位估计所需的迭代次数，实现了相对于传统方法的指数级电路成本降低。

Conclusion: 研究结果表明，将物理直觉融入电子-声子耦合系统的初始态准备具有重要的实用价值，为量子计算机模拟复杂物理系统提供了更高效的策略。

Abstract: Simulating electron-phonon interactions on quantum computers remains challenging, with most algorithmic effort focused on Hamiltonian simulation and circuit optimization. In this work, we study the single-electron Holstein model and propose an initial-state ansatz that substantially enhances ground state overlap in the strong coupling regime, thereby reducing the number of iterations required in standard quantum phase estimation. We further show that this ansatz can be implemented efficiently and yields an exponential reduction in overall circuit costs relative to conventional initial guesses. Our results highlight the practical value of incorporating physical intuition into initial state preparation for electron-phonon coupled systems.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [105] [Minority Takeover in Majority Dynamics: Searching for Rare Initializations via the History Passing Algorithm](https://arxiv.org/abs/2512.16021)
*Marek Jankola,Freya Behrens,Cédric Koller,Lenka Zdeborová*

Main category: cond-mat.dis-nn

TL;DR: 研究随机d-正则图上多数动力学达成全局共识所需的最小初始偏差，发现d≥4时初始少数+1节点即可快速达成+1共识，并提出了HPR算法寻找此类初始配置。


<details>
  <summary>Details</summary>
Motivation: 研究在同步确定性多数动力学中，需要多大的初始配置偏差才能在大规模随机d-正则图上驱动全局共识。具体探讨初始少数+1节点能否最终主导系统达成+1共识。

Method: 使用回溯动力学空腔方法(BDCM)估计达成共识所需的最小初始+1节点比例。提出历史传递强化(HPR)算法来寻找能够使少数+1节点接管多数的初始配置。

Result: BDCM预测对于d≥4，初始全局少数+1节点足以快速引导整个系统达成+1共识。HPR算法能够找到使少数接管多数的初始配置，但未达到BDCM预测的最低密度，而是接近动态一步副本对称破缺相变点。

Conclusion: 在d≥4的随机正则图上，初始少数+1节点可以驱动全局+1共识。HPR算法能有效找到此类初始配置，其性能受限于动态一步副本对称破缺相变。该方法可扩展到其他动力学规则和稀疏图类。

Abstract: We investigate how much bias in the initial configuration is required to drive global agreement in synchronous, deterministic majority dynamics on large random $d$-regular graphs. Nodes take values $\pm 1$ and update their states at each discrete time step to align with the majority of their neighbors. Using the backtracking dynamical cavity method (BDCM), we estimate the minimal fraction of initial $+1$ nodes required to achieve a $+1$ consensus in $p$ time steps. Our analysis predicts that for $d\geq4$ an initial global minority of $+1$ nodes is sufficient to quickly steer the entire system toward consensus on $+1$.
  We then investigate whether such initial conditions can be determined explicitly for a given large random regular graph. To this end, we introduce a new algorithm, which we name history-passing reinforcement (HPR), designed to find such initial configurations with a minority of $+1$ nodes. We find, as a main result, that the HPR algorithm finds initial configurations where the minority takes over the majority for $d$-regular random graphs with $d\geq4$.
  The HPR algorithm outperforms standard simulated annealing-based methods, but does not reach the lowest densities predicted by the BDCM. Rather, the lowest density achievable by the algorithm is near the onset of a dynamical one-step replica symmetry breaking (d1RSB) phase, which we estimate using a one-step replica symmetry breaking (1RSB) formulation of the BDCM. While we focus on the majority dynamics and random $d$-regular graphs, the algorithm can be extended to other dynamical rules and classes of sparse graphs.

</details>


### [106] [Reduction of interaction order in hard combinatorial optimization via conditionally independent degrees of freedom](https://arxiv.org/abs/2512.16726)
*Alexandru Ciobanu,David Dahmen,John Paul Strachan,Moritz Helias*

Main category: cond-mat.dis-nn

TL;DR: 该研究提出了一种基于重整化群的方法，将三阶相互作用的3-SAT问题转化为保持自由能的双体相互作用系统，避免了传统二次化方法带来的计算复杂度爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 3-SAT问题作为NP完全问题，在物理上对应具有三阶相互作用的无序伊辛自旋哈密顿量基态寻找。传统二次化方法虽然降低了相互作用阶数，但引入了额外自由度，导致大规模问题计算不可行。

Method: 采用重整化群方法，通过引入具有独立动力学的额外自由度，在保持原始自由度固定的情况下，逐步追踪额外变量的迹，得到状态依赖的有效相互作用，从而将三阶系统重构为双体相互作用系统。

Result: 该方法能够重构原始三阶能量谱，保持与原始张量公式相同的基态计算缩放比例。在零温极限下，叠加态收敛到单一状态，为无序伊辛系统的基态寻找提供了新途径。

Conclusion: 基于重整化群的谱工程技术为无序伊辛系统的基态寻找开辟了新路径，通过马尔可夫链方法实现了高效的技术实现，解决了传统二次化方法在大规模问题中的计算瓶颈。

Abstract: Combinatorial optimization problems have a broad range of applications and map to physical systems with complex dynamics. Among them, the 3-SAT problem is prominent due to its NP-complete nature. In physics terms, its solution corresponds to finding the ground state of a disordered Ising spin Hamiltonian with third-order, or tensor, interactions. The large growth of the number of third-order interactions with number of variables poses technical difficulties for the physical implementation of minimizers. Therefore, researchers have proposed quadratization techniques which reduce the order of the system, however, at the cost of including additional degrees of freedom. Their inclusion induces a drastic slow down in the minimization, which makes such procedures technically infeasible for large problems. In this work, we take a physics approach by employing the renormalization group to create a pairwise interacting system from the original third-order system while preserving the free energy. Our procedure utilizes additional degrees of freedom that exhibit an independent dynamics provided the original degrees of freedom are fixed. A step-wise trace of the extra variables while running the minimization is therefore theoretically manageable, yielding a state-dependent effective interaction. We use the effective interaction to reconstruct the original third-order energy spectrum, as this yields equal scaling of computations-to-ground-state compared to the original tensor formulation. Here, the original degrees of freedom interact with a subsystem that appears to be in a superposition of an exponentially large number of states. In the zero-temperature limit, the superposition concentrates on one state. Our spectrum-engineering techniques reveal new routes toward the ground state of disordered Ising systems, through Markov chains, and allow for efficient technological implementations.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [107] [Entropy-production fluctuation theorem for a generalized Langevin particle in crossed electric and magnetic fields](https://arxiv.org/abs/2512.16026)
*L. C. González-Morales,I. Pérez Castillo,J. I. Jiménez-Aquino*

Main category: cond-mat.stat-mech

TL;DR: 研究带电布朗粒子在交叉电磁场中熵产生的涨落，证明了熵产生满足详细涨落定理


<details>
  <summary>Details</summary>
Motivation: 研究非平衡态系统中熵产生的统计特性，特别是带电粒子在电磁场驱动下的非马尔可夫动力学

Method: 使用具有记忆效应和高斯噪声的广义朗之万方程建模，通过线性动力学的精确解获得时变高斯相空间概率密度，计算轨迹相关的总熵产生

Result: 对于两种可解驱动协议，从解析上证明了熵产生满足详细涨落定理

Conclusion: 即使在非马尔可夫动力学和电磁场驱动的复杂系统中，熵产生的统计特性仍然遵循详细涨落定理

Abstract: We study fluctuations of entropy production for a charged Brownian particle confined in a harmonic trap and driven out of equilibrium by crossed electric and magnetic fields. The magnetic field is constant and perpendicular to the plane of motion, while the electric field is time dependent and provides the driving. The non-Markovian dynamics is modeled by a generalized Langevin equation with memory and Gaussian noise. Using the exact solution of this linear dynamics, we obtain the time-dependent Gaussian phase-space probability density and from it compute the trajectory-dependent total entropy production. For two solvable driving protocols, we prove analytically that the entropy production obeys a detailed fluctuation theorem.

</details>


### [108] [Complete Decomposition of Anomalous Diffusion in Variable Speed Generalized Lévy Walks](https://arxiv.org/abs/2512.16073)
*Abhijit Bera,Kevin E. Bassler*

Main category: cond-mat.stat-mech

TL;DR: VGLWs统一了多种时空耦合随机过程，其异常扩散行为由Joseph、Noah和Moses三种效应共同作用，其中Noah指数无上限，揭示了比以往Lévy行走模型更丰富的异常扩散景观。


<details>
  <summary>Details</summary>
Motivation: 研究VGLWs的异常扩散行为，通过分解为三种违反中心极限定理的基本效应，揭示异常扩散的复杂机制，发现VGLWs框架下Noah指数无上限，扩展了对异常扩散的理解。

Method: 将VGLWs的异常扩散行为分解为三种基本效应：Joseph效应（反映长程增量相关性）、Noah效应（来自无限方差的厚尾步长分布）和Moses效应（与统计老化和非平稳性相关）。

Result: VGLWs中的异常扩散通常由三种效应的非平凡组合产生，而非单一机制；特别发现VGLW框架中量化Noah效应强度的Noah指数L无上限，揭示了比以往Lévy行走模型更极端丰富的异常扩散景观。

Conclusion: VGLWs提供了一个统一框架来理解异常扩散的复杂机制，异常扩散通常是多种效应共同作用的结果，其中Noah效应的无上限特性扩展了异常扩散的理论边界。

Abstract: Variable Speed Generalized Lévy Walks (VGLWs) are a class of spatio-temporally coupled stochastic processes that unify a broad range of previously studied models within a single parametrized framework. Their dynamics consist of discrete random steps, or flights, during which the walker's speed varies deterministically with both the elapsed time and the total duration of the flight. We investigate the anomalous diffusive behavior of VGLWs and analyze it through decomposition into the three fundamental constitutive effects that capture violations of the Central Limit Theorem (CLT): the Joseph effect, reflecting long-range increment correlations, the Noah effect, arising from heavy-tailed step-size distributions with infinite variance, and the Moses effect, associated with statistical aging and non-stationarity. Our results show that anomalous diffusion in VGLWs is typically generated by a nontrivial combination of all three effects, rather than being attributable to a single mechanism. Strikingly, we find that within the VGLW framework the Noah exponent $L$, which quantifies the strength of the Noah effect, is unbounded from above, revealing a richer and more extreme landscape of anomalous diffusion than in previously studied Lévy-walk-type models.

</details>


### [109] [Efficient Monte-Carlo sampling of metastable systems using non-local collective variable updates](https://arxiv.org/abs/2512.16812)
*Christoph Schönle,Davide Carbone,Marylou Gabrié,Tony Lelièvre,Gabriel Stoltz*

Main category: cond-mat.stat-mech

TL;DR: 该论文提出了一种基于非线性集体变量和欠阻尼朗之万动力学的蒙特卡洛模拟算法，解决了传统方法中的亚稳态问题，相比过阻尼方法显著提升了采样效率。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛模拟在处理复杂分子系统时存在亚稳态问题，虽然已有研究提出在集体变量空间使用非局部提议更新，但需要进一步推广到非线性集体变量和更真实的动力学模型。

Method: 提出了一种通用算法，将非局部提议更新扩展到非线性集体变量空间，并基于欠阻尼朗之万动力学构建可逆的采样方案，支持中等维度（数十到数百个变量）的集体变量空间。

Result: 证明了所提方案的可逆性，并通过多个数值实验验证了其性能，相比先前基于过阻尼朗之万动力学的方法，观察到显著的性能提升。

Conclusion: 该算法结合生成式机器学习提议采样器，扩展了中等维度集体变量空间采样方法的适用性，使其能够应用于更现实的分子系统模拟。

Abstract: Monte-Carlo simulations are widely used to simulate complex molecular systems, but standard approaches suffer from metastability. Lately, the use of non-local proposal updates in a collective-variable (CV) space has been proposed in several works. Here, we generalize these approaches and explicitly spell out an algorithm for non-linear CVs and underdamped Langevin dynamics. We prove reversibility of the resulting scheme and demonstrate its performance on several numerical examples, observing a substantial performance increase compared to methods based on overdamped Langevin dynamics as considered previously. Advances in generative machine-learning-based proposal samplers now enable efficient sampling in CV spaces of intermediate dimensionality (tens to hundreds of variables), and our results extend their applicability toward more realistic molecular systems.

</details>


### [110] [Random planting with harvest: A statistical-mechanical analysis](https://arxiv.org/abs/2512.16858)
*Julian Talbot*

Main category: cond-mat.stat-mech

TL;DR: 该研究通过统计力学方法分析了一个随机种植模型，其中植物表示为生长的硬圆盘，系统通过简单的生长-收获规则达到非平衡稳态，并建立了与多分散硬圆盘流体的映射关系。


<details>
  <summary>Details</summary>
Motivation: 研究随机种植模型中植物生长和收获的动态过程，理解系统如何通过简单的拒绝重叠规则达到稳态，并建立与统计力学系统的联系。

Method: 1. 建立随机种植模型：植物表示为生长的硬圆盘，随机播种，按固定速率生长，达到成熟直径时收获，生长过程中重叠的种植被拒绝；2. 将稳态映射到非加性多分散硬圆盘流体；3. 使用低密度维里展开和标度粒子理论进行解析预测；4. 进行数值模拟验证；5. 扩展理论到S型生长规律。

Result: 1. 成功将稳态映射到多分散硬圆盘流体；2. 解析理论预测与数值模拟在广泛参数范围内吻合良好；3. 高种植率下密度接近去同步规则种植的最优值，以接近1/3的指数代数趋近；4. 空间组织显示出与最优去同步种植相同的几何约束特征；5. 半径分辨对关联函数g(z,r)揭示了亲代-子代播种事件的强尺寸关联。

Conclusion: 随机种植模型可以通过统计力学方法有效描述，系统稳态与多分散硬圆盘流体存在映射关系。高种植率下系统表现出最优去同步种植的特征，空间组织反映了底层几何约束。理论框架可扩展到更复杂的生长规律。

Abstract: We formulate a statistical-mechanical description of a recently introduced random planting model in which plants are represented by growing hard disks. Seedlings of negligible size are introduced at random positions in a field, grow at a prescribed rate, and are harvested upon reaching a fixed maturity diameter. Planting attempts that would lead to an overlap at any time during growth are rejected. Starting from an empty field, this simple dynamical rule drives the system to a nonequilibrium steady state in which the mean planting and harvesting rates coincide. We show that the steady state can be mapped onto a nonadditive polydisperse hard-disk fluid and exploit this mapping to develop analytical predictions based on a low-density virial expansion and on scaled particle theory. The resulting description yields an effective adsorption isotherm for the steady-state plant density as a function of the planting rate and compares favorably with numerical simulations over a wide range of parameters. At large planting rates, the density approaches the optimal value achieved by desynchronized regular planting, and the data are consistent with an algebraic approach to this limit with an exponent close to 1/3. Beyond density and yield, we show that the spatial organization of the field at high planting rates exhibits clear signatures of the same underlying geometric constraints that characterize optimal desynchronized planting. This connection is revealed through both the conventional radial distribution function and a radius-resolved pair correlation g(z,r) which highlights strong size correlations associated with parent-child seeding events and whose structure can be interpreted as a dynamically broadened precursor of the corresponding ideal mixe--size lattice. Finally, we extend the theory to sigmoidal growth laws and compute the associated virial coefficient.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [111] [Applying Gaussian Mixture Models to Track Reconstruction in Inelastic Scattering Experiments with Active Targets](https://arxiv.org/abs/2512.16794)
*A. Arokiaraj,M. B. Latif,R. Raabe,D. Thisse,M. Vandebrouck*

Main category: physics.data-an

TL;DR: 本文提出了一种基于高斯混合模型（GMM）的径迹重建方法，专门用于主动靶（如ACTAR TPC）中的非弹性散射实验，特别关注小角度低能出射粒子的精确重建。


<details>
  <summary>Details</summary>
Motivation: 在逆运动学条件下，使用主动靶（如ACTAR TPC）研究不稳定核的巨共振时，面临的主要挑战是探测小角度低能出射粒子。准确重建这些径迹对于区分不同共振模式至关重要。

Method: 采用高斯混合模型（GMM）作为径迹重建方法，该模型能有效捕捉窄角度事件中束流-反冲界面特征的复杂协方差结构。方法专门针对主动靶非弹性散射实验设计，特别强调低能径迹的处理。

Result: 该方法在模拟数据上进行了验证，模拟了$^{58}\mathrm{Ni}(α,α')^{58}\mathrm{Ni}$反应，入射能量为$E=49$~MeV/核子，模拟条件与GANIL进行的相同实验条件一致。

Conclusion: 基于GMM的径迹重建方法能够有效处理主动靶非弹性散射实验中的小角度低能出射粒子，为研究不稳定核的巨共振提供了可靠的技术手段。

Abstract: Active targets such as ACTAR TPC are well suited for studying giant resonances in unstable nuclei via inelastic scattering in inverse kinematics. A key challenge in such measurements is the detection of low-energy ejectiles emitted at small angles relative to the beam direction. Accurate reconstruction of these tracks is essential for disentangling different resonance modes. Probabilistic models such as the Gaussian Mixture Model (GMM) are particularly effective in capturing the complex covariance structures characteristic of the beam-recoil interface in narrow-angle events. In this work, we present a track reconstruction approach based on the GMM, specifically designed for inelastic scattering experiments with active targets. Special emphasis is placed on the treatment of low-energy tracks. The proposed method is demonstrated on simulated data of the $^{58}\mathrm{Ni}(α,α')^{58}\mathrm{Ni}$ reaction at an incident energy of $E=49$~MeV/nucleon, generated under conditions representative of the experiment carried out at GANIL for the same reaction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [112] [Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments](https://arxiv.org/abs/2512.15736)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: Anubuddhi是一个多智能体AI系统，能够从自然语言提示设计和模拟量子光学实验，无需专业编程知识，通过语义检索组合光学组件，并通过物理模拟验证设计。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在民主化计算实验设计，使非专业用户能够通过自然语言交互设计和模拟复杂的量子光学实验，降低量子光学研究的门槛，促进研究和教学。

Method: 系统采用多智能体架构，结合意图路由、知识增强生成和双模式验证（QuTiP和FreeSim）。通过语义检索从三层工具箱中组合光学组件，然后通过物理模拟进行验证和收敛细化。

Result: 在13个实验中（包括基础光学、量子信息协议和先进技术），系统获得8-9/10的设计-模拟对齐分数，模拟能够忠实建模预期物理。发现结构正确性与定量准确性不同：高对齐确认正确的物理架构，而数值预测需要专家审查。自由形式模拟在11/13实验中优于约束框架。

Conclusion: Anubuddhi系统成功实现了量子光学实验的民主化设计，产生强大的初始设计，用户可以通过对话迭代细化。量子光学的多样性需要灵活的数学表示，系统为研究和教学提供了有效的工具。

Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.

</details>


### [113] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: 论文提出比例责任原则（PPD），将道德责任建模为随智能体认知状态变化的函数，揭示不确定性不会消除责任而是将其转化为修复责任，并通过数学公式和模拟验证了该框架的稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统伦理框架在处理不确定性决策时存在局限，通常将不确定性简单视为行动约束。本文旨在开发一个能够建模道德责任如何随智能体认知状态（特别是不确定性水平）变化的新框架。

Method: 提出比例责任原则（PPD）框架，使用数学方程 D_total = K[(1-HI) + HI * g(C_signal)] 建模总责任，其中包含知识(K)、谦逊/不确定性(HI)和情境信号强度(C_signal)。通过蒙特卡洛模拟验证框架稳定性，并在临床伦理、受赠人权利法、经济治理和人工智能四个领域进行跨学科应用验证。

Result: 模拟显示保持基准谦逊系数（λ>0）的系统能产生更稳定的责任分配，降低过度自信决策的风险。该框架在四个应用领域均表现出有效性，证明比例责任原则可作为复杂系统中的稳定原则。

Conclusion: 比例责任原则提供了一个数学上可处理的道德责任建模方法，通过将谦逊形式化为系统参数，能够平衡认知信心与情境风险，防止决策中的过度行动和疏忽，为可审计AI决策系统的发展提供理论支持。

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [114] [Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions](https://arxiv.org/abs/2512.15743)
*David Noever*

Main category: cs.AI

TL;DR: 提出一个从自然语言描述生成物理可实现的装配指令的框架，使用LDraw作为中间表示，通过约束词汇表确保几何有效性和可建造性，支持3000+部件的砖块原型装配。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如像素扩散或CAD模型）无法支持复杂装配指令或组件交换，需要一种能连接语义设计意图与可制造输出的新方法。

Method: 使用LDraw作为文本丰富的中间表示，引导大语言模型生成有效的逐步建造序列；提出Python库进行程序化模型生成；采用"砖块袋"方法作为物理API，通过约束词汇表将精确定向的砖块位置与"词袋"连接。

Result: 在复杂卫星、飞机和建筑领域评估可建造输出，支持超过3000个装配部件的砖基原型；展示了四个原创设计，证明该方法具有可扩展性、模块化和保真度。

Conclusion: 该方法作为物理API，通过约束词汇表将任意功能需求编译为物质现实，为制造和工程原型中的自然语言实现开辟了新的设计可能性。

Abstract: We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel "bag of bricks" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a "bag of words" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.

</details>


### [115] [AI Epidemiology: achieving explainable AI through expert oversight patterns](https://arxiv.org/abs/2512.15783)
*Kit Tempest-Walters*

Main category: cs.AI

TL;DR: AI流行病学是一个通过应用群体层面监测方法来治理和解释高级AI系统的框架，它通过标准化捕获AI-专家交互的结构化评估字段来预测输出失败，类似于流行病学中通过统计证据进行公共卫生干预的方法。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释性方法（如SHAP和机制可解释性）在部署模型的规模上面临模型复杂性的问题，需要一种能够绕过模型复杂性、提供有效AI治理和解释的框架。

Method: 通过标准化捕获AI-专家交互为结构化评估字段：风险等级、对齐分数和准确度分数，这些作为暴露变量通过统计关联预测输出失败。框架被动跟踪专家与AI建议的趋同和分歧，分析输出而非内部模型计算。

Result: 该框架为零专家负担，提供自动审计跟踪，在机构更新模型和切换供应商时提供治理连续性，通过可靠性分数和语义评估使专家和机构能够在AI输出造成危害前检测不可靠输出。

Conclusion: AI流行病学通过使领域专家能够在不需要机器学习专业知识的情况下治理AI系统，实现了AI监督的民主化，为高级AI系统的治理和解释提供了实用框架。

Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.

</details>


### [116] [State-Augmented Graphs for Circular Economy Triage](https://arxiv.org/abs/2512.15824)
*Richard Fox,Rui Li,Gustav Jonsson,Farzaneh Goli,Miying Yang,Emel Aktas,Yongjing Wang*

Main category: cs.AI

TL;DR: 本文提出了一种用于循环经济产品分拣的新型决策框架，通过状态增强的拆卸序列规划图实现最优递归评估，并以电动汽车电池为例展示了该框架的灵活性。


<details>
  <summary>Details</summary>
Motivation: 循环经济分拣需要评估产品在达到当前使用终点后的可持续处理路径，这需要平衡保留价值与处理成本、劳动力约束的自适应决策。现有方法缺乏能够统一处理复杂约束和递归价值评估的通用框架。

Method: 提出基于状态增强拆卸序列规划图的确定性求解器框架，将拆卸历史编码到状态中以确保马尔可夫性质，实现仅依赖前一状态的最优递归评估。决策包括继续拆卸或选择循环经济选项，整合基于诊断健康评分的条件感知效用和复杂操作约束。

Result: 以电动汽车电池分层分拣为例，展示了框架如何通过组件递归估值驱动决策，能够适应不同的机械复杂性、安全要求和经济驱动因素，证明了框架的灵活性和通用性。

Conclusion: 该统一形式化为优化循环经济分拣决策提供了可处理且可推广的基础，适用于不同产品和操作环境，为循环经济决策支持系统提供了理论框架。

Abstract: Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.

</details>


### [117] [PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations](https://arxiv.org/abs/2512.15894)
*Vahideh Zolfaghari*

Main category: cs.AI

TL;DR: 研究评估了大型语言模型在儿科咨询中的安全性，发现对抗性查询（如家长焦虑表达）会降低模型安全性，模型规模越大安全性越好，但所有模型都存在临床相关漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着家长越来越多地使用大型语言模型获取儿科指导，但模型在真实世界对抗性压力下的安全性了解不足。焦虑家长使用的紧急语言可能破坏模型安全防护，导致有害建议。

Method: 开发了PediatricAnxietyBench开源基准测试，包含300个高质量查询（150个患者来源，150个对抗性），涵盖10个儿科主题。使用多维安全框架评估两个Llama模型（70B和8B），评估维度包括诊断克制、转诊依从性、措辞谨慎性和紧急情况识别。对抗性查询融入了家长压力模式，如紧迫性、经济障碍和对免责声明的挑战。

Result: 平均安全得分为5.50/15（SD=2.41）。70B模型优于8B模型（6.26 vs 4.95，p<0.001），关键失败率更低（4.8% vs 12.0%，p=0.02）。对抗性查询使安全性降低8%（p=0.03），紧迫性导致最大降幅（-1.40）。在癫痫发作（33.3%不适当诊断）和疫苗接种后查询中表现出脆弱性。措辞谨慎性与安全性强相关（r=0.68，p<0.001），而紧急情况识别完全缺失。

Conclusion: 模型规模影响安全性，但所有模型都对现实家长压力表现出脆弱性。PediatricAnxietyBench提供了一个可重复使用的对抗性评估框架，能够揭示标准基准测试忽略的临床重要失败模式。

Abstract: Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.

</details>


### [118] [Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries](https://arxiv.org/abs/2512.15906)
*Jonathan A. Handler*

Main category: cs.AI

TL;DR: Darth Vecdor (DV) 是一个从大语言模型中提取知识到结构化SQL数据库的工具，旨在解决直接查询LLM时存在的成本、速度、安全性和置信度等问题，特别针对医疗保健领域应用。


<details>
  <summary>Details</summary>
Motivation: 虽然可以直接查询大语言模型获取知识，但在高容量操作中，成本、速度、安全性和置信度等问题可能成为障碍。通过将信息从LLM中预先提取到标准数据库中，可以缓解这些问题，特别是在医疗保健等关键领域。

Method: DV通过构建一个结构化、术语映射的SQL数据库（知识图谱）来提取LLM中的知识。系统包含多种功能来缓解LLM响应中的错误、离题、自由文本、过于笼统和不一致等问题，并支持多元素响应。为便于使用，DV提供了基于浏览器的图形用户界面，允许具有领域专业知识但技术背景有限的人员进行提示工程。

Result: DV已作为免费、开源、可扩展的软件发布，采用"按现状"基础，不提供任何明示或暗示的保证或条件。作者承认软件可能存在严重错误，但希望适当使用当前和未来版本的DV及其输出能够帮助改善医疗保健。

Conclusion: DV为解决直接查询LLM的局限性提供了一种替代方案，通过将知识提取到结构化数据库中，可以在医疗保健等领域实现更可靠、高效和安全的知识查询。用户需要意识到使用DV的潜在风险和益处，并负责确保任何使用都是安全和有效的。

Abstract: Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database ("knowledge base" or "knowledge graph"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an "as is" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.

</details>


### [119] [Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems](https://arxiv.org/abs/2512.15922)
*Jovan Pavlović,Miklós Krész,László Hajdu*

Main category: cs.AI

TL;DR: 提出基于传播激活算法的RAG框架，通过自动构建的知识图谱增强大语言模型在复杂推理任务中的表现，相比传统RAG方法在答案正确性上提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统将所有检索信息视为同等可靠，忽略了文本语料库中信息的可信度和关联性差异。GraphRAG方法虽然通过知识图谱改进，但依赖高质量图表示，构建成本高且自动化构建不可靠。现有系统使用大语言模型指导图遍历，面临与传统RAG类似的挑战。

Method: 提出新型RAG框架，采用传播激活算法从自动构建的知识图谱连接的文档语料库中检索信息。该方法可作为即插即用模块与多种RAG方法集成，特别适合资源受限环境。

Result: 实验表明，该方法在复杂任务（如多跳问答）上达到或优于迭代RAG方法。与思维链迭代检索结合时，相比朴素RAG实现39%的绝对增益，且使用小型开源语言模型即可获得良好效果。

Conclusion: 基于传播激活算法的RAG框架能有效提升大语言模型在复杂推理任务中的性能，易于集成且适合资源受限环境，为RAG系统提供了新的改进方向。

Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.

</details>


### [120] [Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning](https://arxiv.org/abs/2512.15943)
*Polaris Jhandi,Owais Kazi,Shreyas Subramanian,Neel Sendas*

Main category: cs.AI

TL;DR: 该研究探索用小语言模型(SLMs)替代大语言模型(LLMs)的可行性，通过微调OPT-350M模型在特定任务上取得优异性能，显著降低生成式AI的部署成本。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的规模化采用，模型成本优化和运营效率成为决定可持续性和可访问性的关键因素。LLMs虽然能力强大，但计算需求巨大，成本高昂，限制了其在企业常规应用中的使用。这促使研究者探索SLMs，它们能在特定应用中提供可比性能，同时大幅降低基础设施开销。

Method: 研究训练了一个领域适应的SLM来执行传统由LLMs处理的任务，如文档摘要、查询回答和结构化数据解释。具体使用Hugging Face TRL的监督微调(SFT)训练器，对facebook/opt-350m模型进行单轮微调。OPT-350M是Meta AI于2022年发布的OPT模型家族成员。

Result: 实验结果显示，微调后的SLM在ToolBench评估中取得了77.55%的通过率，显著优于所有基线模型，包括ChatGPT-CoT(26.00%)、ToolLLaMA-DFS(30.18%)和ToolLLaMA-CoT(16.27%)。

Conclusion: 精心设计和针对性训练的SLMs能够显著降低生成式AI的采用门槛，实现成本效益高、可大规模集成到生产系统中的AI解决方案，为组织提供了更可持续和可访问的AI部署路径。

Abstract: As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\%), ToolLLaMA-DFS (30.18\%), and ToolLLaMA-CoT (16.27\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.

</details>


### [121] [Subjective functions](https://arxiv.org/abs/2512.15948)
*Samuel J. Gershman*

Main category: cs.AI

TL;DR: 论文探讨了智能体如何内生地合成目标函数，提出了主观函数的概念，并以期望预测误差作为具体示例。


<details>
  <summary>Details</summary>
Motivation: 研究目标函数的来源和选择机制，探索人类智能如何动态合成新目标函数，并试图赋予人工智能系统相同能力。

Method: 提出主观函数的概念（一种内生于智能体的高阶目标函数），并以期望预测误差作为具体示例进行研究。

Result: 建立了主观函数的概念框架，展示了期望预测误差作为主观函数的可行性，并与心理学、神经科学和机器学习中的相关思想建立了联系。

Conclusion: 主观函数为理解智能体如何内生地形成目标提供了新视角，有望推动人工智能系统自主目标合成能力的发展。

Abstract: Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.

</details>


### [122] [Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets](https://arxiv.org/abs/2512.16030)
*Lukas Nel*

Main category: cs.AI

TL;DR: 本文介绍了KalshiBench基准测试，用于评估大语言模型对未来事件的校准能力，发现所有前沿模型都存在系统性过度自信问题，即使推理增强模型也表现不佳。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在各种任务上表现出色，但其认知校准能力仍未被充分理解。传统基准测试主要评估静态知识的准确性，而缺乏对模型在真正未知未来事件上量化不确定性能力的评估。

Method: 研究者创建了KalshiBench基准，包含300个来自CFTC监管交易所Kalshi的预测市场问题，这些问题的真实结果发生在模型训练截止日期之后。评估了Claude Opus 4.5、GPT-5.2、DeepSeek-V3.2、Qwen3-235B和Kimi-K2五个前沿模型，使用预期校准误差（ECE）和Brier技能评分等指标来衡量校准质量。

Result: 所有模型都表现出系统性过度自信。最佳校准模型（Claude Opus 4.5）的ECE为0.120，仍存在显著校准误差。推理增强模型如GPT-5.2-XHigh的校准更差（ECE=0.395），尽管准确性相当。只有一个模型获得了正的Brier技能评分，表明大多数模型的表现甚至不如简单预测基准率。

Conclusion: 模型规模和推理能力的增强并不能自动带来校准能力的提升，认知校准是一种需要针对性开发的独立能力。当前前沿大语言模型在量化未来事件不确定性方面存在系统性缺陷。

Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.

</details>


### [123] [Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education](https://arxiv.org/abs/2512.16036)
*Diane Myung-kyung Woodbridge,Allyson Seba,Freddie Seba,Aydin Schwartz*

Main category: cs.AI

TL;DR: 研究人员开发了一个自动化系统，用于发现和分类课程大纲与机构政策网站中的AI相关政策，结合无监督主题建模和大型语言模型来识别政策主题并分类GenAI使用允许程度。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在教育中的普及，学生使用这些工具进行学习，但也带来了错误信息、幻觉输出以及可能削弱批判性思维的问题。各教育机构制定了不同的AI政策，但这些政策差异大且不断变化，导致学生不确定期望和最佳实践。

Method: 设计并实施了一个自动化系统，结合无监督主题建模技术识别关键政策主题，并使用大型语言模型（LLMs）对政策文本中的GenAI允许程度和其他要求进行分类。

Result: 该系统在主题发现方面获得了0.73的一致性分数。基于GPT-4.0的政策分类在八个已识别主题中实现了0.92-0.97的精确度和0.85-0.97的召回率。

Conclusion: 该工具通过提供结构化和可解释的政策信息，促进了GenAI技术在教育中的安全、公平和教学对齐的使用，并且可以集成到教育技术平台中帮助学生理解和遵守相关指南。

Abstract: As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.

</details>


### [124] [WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning](https://arxiv.org/abs/2512.16108)
*Wendong Bi,Yirong Mao,Xianglong Liu,Kai Tian,Jian Zhang,Hanjie Wang,Wenhui Que*

Main category: cs.AI

TL;DR: 提出WeMusic-Agent训练框架，通过知识内化和智能边界学习，让LLM在对话式音乐推荐中智能决策何时使用内部知识、何时调用外部工具，并构建了首个开源对话音乐推荐基准。


<details>
  <summary>Details</summary>
Motivation: 现有对话式音乐推荐方法难以平衡专业领域知识和灵活工具集成，需要既能理解用户偏好和音乐上下文，又能智能决定何时使用内部知识、何时调用外部工具的解决方案。

Method: 提出WeMusic-Agent训练框架，结合知识内化（在500亿音乐相关语料上持续预训练）和智能边界学习，教导模型智能决策何时使用内部知识、何时调用外部工具（如音乐检索API、推荐系统）。

Result: 在真实世界数据上的实验表明，WeMusic-Agent相比现有模型有显著改进。同时构建了首个基于微信听书真实数据的开源对话音乐推荐基准，支持相关性、个性化、多样性等多维度评估。

Conclusion: WeMusic-Agent框架通过知识内化和智能边界学习的结合，有效解决了对话式音乐推荐中平衡专业知识和工具集成的挑战，为个性化音乐推荐提供了更智能的解决方案。

Abstract: Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.

</details>


### [125] [ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs](https://arxiv.org/abs/2512.16149)
*Hao Chen,Zhexin Hu,Jiajun Chai,Haocheng Yang,Hang He,Xiaohan Wang,Wei Lin,Luhang Wang,Guojun Yin,Zhuofeng zhao*

Main category: cs.AI

TL;DR: ToolForge是一个自动化合成框架，仅需少量虚拟工具即可生成高质量工具调用数据，无需真实API调用，使8B参数模型在多个基准测试中超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有工具调用数据生成方法依赖大量真实API调用，成本高昂且缺乏多跳推理和自反思能力，需要更高效、低成本的数据生成方案。

Method: 基于(问题、黄金上下文、答案)三元组合成大规模工具学习数据，引入多跳推理和自反思机制，采用多层验证框架（规则和模型评估）确保数据质量。

Result: 仅使用8B参数的模型在训练合成数据后，在多个基准测试中表现优于GPT-4o，证明了方法的有效性。

Conclusion: ToolForge框架能够高效生成高质量工具调用数据，显著降低数据生成成本，同时提升模型在真实世界工具调用任务中的性能。

Abstract: Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .

</details>


### [126] [Science Consultant Agent](https://arxiv.org/abs/2512.16171)
*Karthikeyan K,Philip Wu,Xin Tang,Alexandre Alves*

Main category: cs.AI

TL;DR: Science Consultant Agent是一个基于网页的AI工具，通过四个核心组件帮助从业者选择和实施最有效的AI建模策略，加速从产品经理到研究人员的开发流程。


<details>
  <summary>Details</summary>
Motivation: 为帮助AI从业者（包括产品经理、软件开发者和研究人员）更有效地选择和实施AI建模策略，解决在众多建模方法中选择合适方案的挑战，加速AI解决方案的开发过程。

Method: 采用四个核心组件：1) 问卷系统收集需求；2) 智能填充辅助输入；3) 基于研究的推荐系统提供文献支持的解决方案建议；4) 原型构建器生成初步实现。

Result: 开发了一个完整的端到端系统，通过结构化流程帮助用户从需求分析到原型生成，加速AI解决方案的开发，具体实现如图1所示。

Conclusion: Science Consultant Agent通过整合问卷、智能填充、研究指导和原型构建，为AI从业者提供了一个有效的工具，能够显著提高AI建模策略选择和实施的效率。

Abstract: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.

</details>


### [127] [PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving](https://arxiv.org/abs/2512.16214)
*Jianming Liu,Ren Zhu,Jian Xu,Kun Ding,Xu-Yao Zhang,Gaofeng Meng,Cheng-Lin Liu*

Main category: cs.AI

TL;DR: PDE-Agent：首个基于LLM驱动的多智能体协作框架，通过工具链增强实现从自然语言描述自动求解偏微分方程，无需专家知识。


<details>
  <summary>Details</summary>
Motivation: 传统PDE求解方法依赖人工设置和领域专业知识，现有PINN和DeepXDE等方法仍需要专家知识且缺乏完全自主性。需要一种能够从自然语言描述自动求解PDE的新范式。

Method: 提出PDE-Agent框架，包含两个关键创新：1）Prog-Act框架与图记忆支持的多智能体协作，通过双循环机制（局部修复和全局修订）实现动态规划和错误纠正；2）集成工具-参数分离机制的资源池，集中管理运行时工件并解决现有框架中的工具间依赖间隙。

Result: 开发了PDE-Bench多类型PDE基准测试集，并提出多层次指标评估工具协调能力。评估验证PDE-Agent在复杂多步骤、跨步骤依赖任务中表现出优越的适用性和性能。

Conclusion: 工具链增强的多智能体PDE求解新范式将推动自动化科学计算的未来发展，PDE-Agent展示了从自然语言描述自动求解PDE的可行性。

Abstract: Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.

</details>


### [128] [Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis](https://arxiv.org/abs/2512.16237)
*Zhi Helu,Huang Jingjing,Xu Wang,Xu Yangbin,Zhang Wanyue,Jiang Baoyang,Deng Shirui,Zhu Liang,Li Fangfang,Zhao Tiejun,Lin Yankai,Yao Yuan*

Main category: cs.AI

TL;DR: SPRITE框架通过模拟器和大型模型程序化合成可扩展、多样且高质量的空间推理数据，解决了传统模板方法刚性高和人工标注不可扩展的困境。


<details>
  <summary>Details</summary>
Motivation: 当前体现智能面临空间理解和推理能力的局限，现有方法存在两难：模板数据集可扩展但结构僵化，人工标注语言多样但不可扩展且计算不精确。

Method: 将真实数据生成重构为代码生成任务，利用LLMs将复杂空间问题编译为可执行程序，通过模拟器提取的高精度场景元信息进行验证，确保计算精确性和可验证性。

Result: 构建了包含3个模拟器、11k+场景、300k+图像/视频指令调优对的数据集，训练出的VLM在多个空间基准测试中表现显著提升，优于同等规模的其他开源数据集。

Conclusion: SPRITE框架通过程序化合成方法克服了传统模板方法的低多样性问题，对构建稳健、可泛化的空间智能至关重要，框架代码和完整数据集将公开以促进未来研究。

Abstract: Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.

</details>


### [129] [AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints](https://arxiv.org/abs/2512.16245)
*Aniruddha Roy,Jyoti Patel,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: AlignMerge：一种几何感知的LLM合并框架，通过显式保持对齐不变性，在合并模型时保护安全对齐性，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型合并方法（如线性权重组合、任务向量、Fisher加权平均）虽然能保持损失函数值，但会悄悄破坏模型的安全对齐性。作者认为合并不应是数值技巧，而应是围绕已对齐锚点的几何约束操作。

Method: 在指令调优基模型的局部Fisher图表中，估计对齐子空间并引入投影器P_A。优化目标函数包含三个部分：几何损失（保持与专家模型的Fisher-Rao几何距离）、对齐损失（惩罚沿对齐敏感方向的移动）和预算损失（强制执行软对齐预算）。使用解码不变的对齐质量指数（AQI）作为对齐功能度量。

Result: 在五个模型家族（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）上测试，AlignMerge在合并安全锚点和任务专家时，显著改善了对齐指标（AQI、毒性、LLM-judge对齐），同时在指令遵循、推理和帮助性方面匹配或超过最佳专家。相比Fisher soups、TIES、SafeMerge和MergeAlign等方法，表现出更小的对齐子空间漂移和更少的预算违规。

Conclusion: 该研究使对齐保持合并成为首要设计目标，并为未来基础模型的几何感知组合提供了一条路径。强调合并应尊重安全几何结构，而不是事后验证。

Abstract: Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.
  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:
  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,
  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.
  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.

</details>


### [130] [Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection](https://arxiv.org/abs/2512.16300)
*Fanrui Zhang,Qiang Zhang,Sizhuo Zhou,Jianwen Sun,Chuanhao Li,Jiaxin Ai,Yukang Feng,Yujie Zhang,Wenjie Li,Zizhen Li,Yifan Chang,Jiawei Liu,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 提出ForenAgent框架，通过多轮交互让多模态大语言模型自主生成、执行并迭代优化基于Python的低级工具，实现更灵活可解释的图像伪造检测。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测方法要么利用低级语义无关的伪影，要么依赖具有高级语义知识的多模态大语言模型。这两种信息流在范式和推理上高度异质，难以统一或有效建模其跨层次交互。

Method: 提出ForenAgent框架，采用两阶段训练流程（冷启动+强化微调），设计动态推理循环（全局感知、局部聚焦、迭代探测、整体裁决），并构建FABench数据集进行训练评估。

Result: 实验表明ForenAgent在低级工具辅助下，在挑战性IFD任务上展现出新兴的工具使用能力和反思推理能力，为通用IFD开辟了有前景的路径。

Conclusion: ForenAgent通过让MLLMs自主生成和执行低级工具，实现了更灵活可解释的伪造分析，为统一低级伪影和高级语义知识提供了有效解决方案。

Abstract: Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.

</details>


### [131] [Adaptation of Agentic AI](https://arxiv.org/abs/2512.16301)
*Pengcheng Jiang,Jiacheng Lin,Zhiyi Shi,Zifeng Wang,Luxi He,Yichen Wu,Ming Zhong,Peiyang Song,Qizheng Zhang,Heng Wang,Xueqiang Xu,Hanwen Xu,Pengrui Han,Dylan Zhang,Jiashuo Sun,Chaoqi Yang,Kun Qian,Tian Wang,Changran Hu,Manling Li,Quanzheng Li,Hao Peng,Sheng Wang,Jingbo Shang,Chao Zhang,Jiaxuan You,Liyuan Liu,Pan Lu,Yu Zhang,Heng Ji,Yejin Choi,Dawn Song,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 本文提出了一个系统框架，统一了智能体AI系统中的适应策略研究，将适应分为智能体适应和工具适应两大类，并进一步细分为不同形式，为构建更强大、高效、可靠的智能体AI系统提供概念基础和实践路线图。


<details>
  <summary>Details</summary>
Motivation: 随着基于基础模型的智能体AI系统能力不断增强、应用范围不断扩大，适应机制成为提升性能、可靠性和泛化能力的关键手段。当前研究领域快速扩展但缺乏系统性框架，需要统一视角来理解不同的适应策略及其权衡。

Method: 提出一个系统化框架，将适应策略分为两大类别：智能体适应（包括工具执行信号驱动和智能体输出信号驱动两种形式）和工具适应（包括智能体无关和智能体监督两种形式）。通过该框架分析设计空间、明确权衡，并提供实践指导。

Result: 该框架有助于澄清智能体AI中适应策略的设计空间，使各种策略的权衡变得明确，为系统设计中选择或切换策略提供实用指导。同时回顾了各类别的代表性方法，分析了其优势和局限性。

Conclusion: 本文为研究人员和实践者构建更强大、高效、可靠的智能体AI系统提供了概念基础和实践路线图，并指出了关键开放挑战和未来机遇，推动了智能体AI适应策略研究的系统化发展。

Abstract: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.

</details>


### [132] [Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference](https://arxiv.org/abs/2512.16317)
*Arther Tian,Alex Ding,Frank Chen,Alan Wu,Aaron Chan,Bruce Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种成本感知的Proof of Quality框架，将显式效率测量整合到奖励机制中，用于去中心化LLM推理的质量验证。


<details>
  <summary>Details</summary>
Motivation: 去中心化LLM推理需要透明和抗审查的AI访问，但现有验证方法难以扩展到现代模型。原始的Proof of Quality忽略了推理节点和评估节点之间的异构计算成本差异。

Method: 引入成本感知PoQ框架，将效率测量整合到奖励机制中。设计结合了真实标记级F1、轻量级学习评估器和基于GPT的判断，采用线性奖励函数平衡标准化质量和成本。

Result: 实验表明语义文本相似性双编码器比交叉编码器与真实值和GPT分数相关性更高；质量-成本分析显示池中最大模型在单位延迟质量方面最有效；蒙特卡洛模拟显示成本感知奖励方案能持续奖励高质量低成本的推理模型和高效评估器。

Conclusion: 成本感知PoQ为经济可持续的去中心化LLM推理提供了实用基础，评估器架构是关键设计选择，最大模型在质量-效率权衡中表现最佳。

Abstract: Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.
  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.

</details>


### [133] [PCIA: A Path Construction Imitation Algorithm for Global Optimization](https://arxiv.org/abs/2512.16392)
*Mohammad-Javad Rezaei,Mozafar Bag-Mohammadi*

Main category: cs.AI

TL;DR: 提出了一种新的元启发式优化算法PCIA，受人类构建和使用路径的启发，通过随机种群寻找最佳路径，在53个数学优化问题和13个约束优化问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 受人类构建和使用路径的行为启发，人类通常偏好热门交通路线，在路径关闭时通过智能混合现有路径构建新路线，并随机选择不同路径到达未知目的地。

Method: PCIA算法生成随机种群寻找最佳路径，类似于群体智能算法。每个粒子代表一条通往目的地的路径，算法模拟人类构建和使用路径的智能行为。

Result: 在53个数学优化问题和13个约束优化问题上测试，结果显示PCIA与流行和最新的元启发式算法相比具有高度竞争力。

Conclusion: PCIA是一种有效的元启发式优化算法，受人类路径构建行为的启发，在多种优化问题上表现出优异的性能。

Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.

</details>


### [134] [Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs](https://arxiv.org/abs/2512.16424)
*Nguyen Xuan-Vu,Daniel Armstrong,Milena Wehrbach,Andres M Bran,Zlatko Jončev,Philippe Schwaller*

Main category: cs.AI

TL;DR: Synthelite是一个基于大语言模型的计算机辅助合成规划框架，能够通过自然语言交互生成端到端合成路线，并灵活适应用户约束，成功率高达95%。


<details>
  <summary>Details</summary>
Motivation: 现有计算机辅助合成规划框架缺乏与人类专家交互的机制，限制了化学家洞察力的整合。需要开发能够结合专家知识和灵活适应约束的合成规划工具。

Method: 提出Synthelite框架，利用大语言模型直接提出逆合成转化。通过自然语言提示允许专家干预，利用LLMs固有的化学知识和推理能力生成端到端合成路线。

Result: 实验表明Synthelite能够灵活适应各种用户指定约束，在策略约束和起始材料约束的合成任务中达到95%的成功率。同时展示出在路线设计中考虑化学可行性的能力。

Conclusion: Synthelite既是一个实用工具，也是迈向以大语言模型为核心协调器的合成规划范式的重要一步，实现了人机协作的化学合成规划。

Abstract: Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.

</details>


### [135] [TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles](https://arxiv.org/abs/2512.16442)
*Allard Oelen,Sören Auer*

Main category: cs.AI

TL;DR: TIB AIssistant是一个AI支持的研究平台，通过多个专门助手和工具支持整个研究生命周期，生成的数据可导出为RO-Crate格式以增强可重复性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和大语言模型的广泛应用，AI支持的研究具有支持整个研究生命周期的潜力，但目前缺乏系统化的平台来整合这些功能。

Method: 开发TIB AIssistant平台，包含多个专门负责特定研究任务的助手，提供访问外部学术服务的工具，并将生成的数据存储在资产中，可导出为RO-Crate格式。

Result: 通过顺序演示助手之间的交互，展示了AIssistant如何生成研究论文草稿的各个部分，验证了平台在支持研究生命周期方面的功能。

Conclusion: TIB AIssistant为AI支持的研究奠定了基础，旨在建立一个社区维护的平台，促进研究透明度和可重复性。

Abstract: The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.

</details>


### [136] [Towards AI-Supported Research: a Vision of the TIB AIssistant](https://arxiv.org/abs/2512.16447)
*Sören Auer,Allard Oelen,Mohamad Yaser Jaradeh,Mutahira Khalid,Farhana Keya,Sasi Kiran Gaddipati,Jennifer D'Souza,Lorenz Schlüter,Amirreza Alasti,Gollam Rabby,Azanzi Jiomekong,Oliver Karras*

Main category: cs.AI

TL;DR: TIB AIssistant是一个领域无关的人机协作平台，旨在通过AI助手支持跨学科研究者在整个研究生命周期中的任务，解决AI集成到研究中的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大型语言模型的快速发展有望改变研究方式，但将AI有效整合到研究中仍面临挑战，包括领域需求差异、AI素养有限、工具协调复杂以及生成式AI在研究中的准确性不明确等问题。

Method: 提出了TIB AIssistant平台，采用模块化组件设计，包括提示和工具库、共享数据存储和灵活的编排框架，支持从构思、文献分析、方法开发到数据分析和学术写作的全研究周期任务。

Result: 描述了概念框架、系统架构和早期原型实现，证明了该方法的可行性和潜在影响。

Conclusion: TIB AIssistant平台为跨学科研究者提供了一个领域无关的人机协作平台，通过模块化组件支持整个研究生命周期，有望解决AI集成到研究中的挑战并增强学术工作流程。

Abstract: The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.

</details>


### [137] [TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries](https://arxiv.org/abs/2512.16453)
*Jiayang Yang,Chunhui Zhao,Martin Guay,Zhixing Cao*

Main category: cs.AI

TL;DR: TS2R是一个提示框架，将锂离子电池原始时间序列数据转换为结构化报告，使LLM能够在电池储能系统管理中推理、预测和决策。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解释多元时间序列数据方面具有潜力，但在实际电池储能系统运维中的应用尚未充分探索。需要一种方法将原始传感器数据转化为LLM可理解的结构化信息。

Method: TS2R通过分割、语义抽象和基于规则的解释，将短期时间动态编码为自然语言，连接低层传感器信号与高层上下文洞察。

Result: 在实验室和真实数据集上，TS2R在异常检测、荷电状态预测和充放电管理等任务中，相比视觉、嵌入和文本提示基线，在准确性、鲁棒性和可解释性方面持续提升LLM性能。

Conclusion: TS2R集成的LLM无需重新训练或架构修改即可达到专家级决策质量和预测一致性，为自适应、LLM驱动的电池智能建立了实用路径。

Abstract: Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.

</details>


### [138] [cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution](https://arxiv.org/abs/2512.16465)
*Jinwu Chen,Qidie Wu,Bin Li,Lin Ma,Xin Si,Yang Hu,Shouyi Yin,Jun Yang*

Main category: cs.AI

TL;DR: cuPilot是一个策略协调的多智能体框架，通过引入策略作为中间语义表示来优化CUDA内核，相比PyTorch在100个内核基准测试中平均加速3.09倍


<details>
  <summary>Details</summary>
Motivation: CUDA内核优化需要硬件-软件协同设计专业知识且高性能内核库具有专有性，现有LLM结合进化算法的方法由于智能体设计不佳和进化表示不匹配而性能不足

Method: 提出策略协调的多智能体框架cuPilot，引入策略作为内核进化的中间语义表示，包括策略协调进化算法、屋顶线引导提示和策略级种群初始化

Result: 在100个内核基准测试中，cuPilot生成的内核相比PyTorch平均加速3.09倍；在GEMM任务中展示了复杂优化并实现了关键硬件单元的高利用率

Conclusion: cuPilot通过策略协调的多智能体框架有效解决了CUDA内核优化问题，生成的优化内核已开源，为自动内核优化提供了新方法

Abstract: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.

</details>


### [139] [Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery](https://arxiv.org/abs/2512.16468)
*Danial Safaei,Siddartha Khastgir,Mohsen Alirezaei,Jeroen Ploeg,Son Tong,Xingyu Zhao*

Main category: cs.AI

TL;DR: 提出了一种新的系统特定保真度度量方法——决定性特征保真度（DFF），用于评估自动驾驶系统在真实和合成环境中决策所依赖的因果证据是否一致，超越了传统的像素级保真度评估。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶安全验证主要依赖合成数据的虚拟测试，但研究发现像素级保真度并不能保证系统在仿真和真实世界中的可靠迁移。关键问题在于系统是否基于相同的因果证据做出决策，而不仅仅是图像对人类"看起来真实"。现有缺乏这种基于行为的保真度度量方法。

Method: 提出了决定性特征保真度（DFF）度量方法，利用可解释AI技术识别和比较系统在匹配的真实-合成数据对上决策所依赖的决定性特征。进一步提出了基于反事实解释的实用估计器，以及DFF引导的校准方案来提升仿真器保真度。

Result: 在2126个匹配的KITTI-VirtualKITTI2数据对上的实验表明，DFF能够揭示传统输出值保真度忽略的差异。DFF引导的校准在提升决定性特征和输入级保真度的同时，不会牺牲不同系统间的输出值保真度。

Conclusion: 决定性特征保真度（DFF）提供了一种系统特定的、基于因果证据的保真度度量方法，能够更有效地评估仿真到真实世界的迁移可靠性，并为仿真器校准提供了新的指导方向。

Abstract: Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images "look real" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.

</details>


### [140] [Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network](https://arxiv.org/abs/2512.16491)
*Theresa Eimer,Lennart Schäpermeier,André Biedenkapp,Alexander Tornede,Lars Kotthoff,Pieter Leyman,Matthias Feurer,Katharina Eggensperger,Kaitlin Maile,Tanja Tornede,Anna Kozak,Ke Xue,Marcel Wever,Mitra Baratchi,Damir Pulatov,Heike Trautmann,Haniye Kashgarani,Marius Lindauer*

Main category: cs.AI

TL;DR: 该报告收集了元算法研究中的最佳实践，涵盖从研究问题提出到结果呈现的完整实验周期，为元算法领域的研究者和实践者提供指导。


<details>
  <summary>Details</summary>
Motivation: 元算法研究（如算法选择、配置和调度）依赖大量计算实验，实验设计和执行中存在多种错误源，威胁研究的可扩展性和有效性。现有最佳实践分散在不同领域，需要系统整合。

Method: 收集COSEAL社区各子领域的良好实践，涵盖完整实验周期：从研究问题提出、实验设计选择，到实验执行，再到结果分析和公正呈现。

Result: 建立了元算法研究中的当前最先进实践标准，为元算法领域的新研究者和实践者提供了系统指导。

Conclusion: 该报告系统整合了元算法研究的实验最佳实践，有助于提高研究的严谨性和可重复性，促进元算法领域的发展。

Abstract: Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.

</details>


### [141] [Scaling Laws for Energy Efficiency of Local LLMs](https://arxiv.org/abs/2512.16531)
*Ander Alvarez,Alessandro Genuardi,Nilotpal Sinha,Antonio Tiene,Samuel Mugel,Román Orús*

Main category: cs.AI

TL;DR: 本文系统研究了在CPU设备上部署本地大语言模型和视觉语言模型的性能规律，发现了文本长度与计算成本近似线性相关、视觉模型存在分辨率阈值效应，并验证了量子启发压缩技术能显著降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU主导AI部署，但大多数消费硬件（笔记本电脑、台式机、工业控制器和嵌入式系统）依赖CPU。然而，CPU专用推理在本地语言和视觉语言工作负载上的计算规律尚未得到充分探索，需要系统量化这些边缘设备的性能特征。

Method: 在两个代表性CPU层级上进行系统基准测试：MacBook Pro M2（主流笔记本电脑级）和Raspberry Pi 5（低功耗嵌入式设置）。采用基于处理器和内存使用连续采样及曲线下面积积分的统一方法，分析计算负载如何随文本长度（语言模型）和图像分辨率（视觉语言模型）变化。

Result: 发现两个经验性扩展规律：1）语言模型推理的计算成本与标记长度近似线性相关；2）视觉语言模型存在预处理驱动的"分辨率拐点"，计算量在内部分辨率阈值以上保持恒定，在阈值以下急剧下降。量子启发压缩技术可将处理器和内存使用降低高达71.9%，能耗降低高达62%，同时保持或提高语义准确性。

Conclusion: 研究系统量化了多模态CPU专用推理在本地语言和视觉语言工作负载上的扩展规律，识别出模型压缩和输入分辨率预处理作为可持续边缘推理的有效、低成本杠杆。这些发现为在资源受限的边缘设备上部署AI模型提供了实用指导。

Abstract: Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.

</details>


### [142] [Prefix Probing: Lightweight Harmful Content Detection for Large Language Models](https://arxiv.org/abs/2512.16650)
*Jirui Yang,Hengqi Guo,Zhihui Lu,Yi Zhao,Yuansen Zhang,Shijing Hu,Qiang Duan,Yinggui Wang,Tao Wei*

Main category: cs.AI

TL;DR: Prefix Probing：一种黑盒有害内容检测方法，通过比较"同意/执行"与"拒绝/安全"前缀的条件对数概率，利用前缀缓存将检测开销降低到接近首词元延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现实世界安全敏感应用中面临检测准确性、推理延迟和部署成本之间的三向权衡问题，需要一种高效且低成本的检测方法。

Method: 提出Prefix Probing方法：1）比较"同意/执行"与"拒绝/安全"开头前缀的条件对数概率；2）利用前缀缓存减少检测开销；3）设计高效前缀构造算法自动发现高信息量前缀；4）推理时只需对探测前缀进行单次对数概率计算。

Result: 实验表明，Prefix Probing的检测效果与主流外部安全模型相当，同时仅产生最小计算成本，无需额外模型部署，具有强实用性和高效性。

Conclusion: Prefix Probing通过前缀探测和缓存机制，在保持检测准确性的同时显著降低了计算开销和部署成本，为大型语言模型的安全应用提供了高效实用的解决方案。

Abstract: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of "agreement/execution" versus "refusal/safety" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.

</details>


### [143] [Comprehensive AI Literacy: The Case for Centering Human Agency](https://arxiv.org/abs/2512.16656)
*Sri Yash Tadimalla,Justin Cary,Gordon Hull,Jordan Register,Daniel Maxwell,David Pugalee,Tina Heafner*

Main category: cs.AI

TL;DR: 本文主张从功能性AI技能培训转向全面的AI素养教育，强调以人类能动性为核心，培养批判性思维和伦理决策能力，而非被动接受技术。


<details>
  <summary>Details</summary>
Motivation: 当前AI技术快速融入社会，但教育框架未能有效应对，导致危险的素养鸿沟。现有的AI教育过于关注工具的操作技能，忽视了批判性和伦理思考的培养，这种失衡需要系统性转变。

Method: 提出以人类能动性为核心的AI素养框架，包括AI素养、流利度和能力三个层次。强调将技术视为可选择而非必然采纳的对象，通过批判性思维和认识论教育，让教育者和学生成为AI使用中的自主决策者。

Result: 构建了系统性的AI素养教育框架，为教育者和学生提供了明确路径，使其能够清晰表达AI决策的意图、态度及其对学术、职业和社会的影响，实现以人为本的AI应用。

Conclusion: 真正的AI素养教育必须以人类能动性为中心，将技术视为可选择的工具而非必然命运。这需要教育系统致力于培养批判性思维和认识论理解，使所有教育利益相关者成为AI技术使用的自主决策者。

Abstract: The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.

</details>


### [144] [Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm](https://arxiv.org/abs/2512.16694)
*Wisnu Uriawan,Achmad Ajie Priyajie,Angga Gustian,Fikri Nur Hidayat,Sendi Ahmad Rafiudin,Muhamad Fikri Zaelani*

Main category: cs.AI

TL;DR: 本研究使用Apriori算法对印尼语布哈里圣训进行无监督主题分组，通过关联规则挖掘发现了礼拜、启示和圣训叙事等主题模式。


<details>
  <summary>Details</summary>
Motivation: 随着伊斯兰文本数字化的发展，迫切需要自动化圣训主题分组技术。本研究旨在利用无监督学习方法自动识别圣训中的语义关联和主题模式。

Method: 采用Apriori算法进行关联规则挖掘，数据为印尼语布哈里圣训译本，预处理包括大小写转换、标点清理、分词、停用词去除和词干提取，使用支持度、置信度和提升度参数分析关联模式。

Result: 发现了有意义的关联模式，如"rakaat-礼拜"、"verse-启示"和"hadith-故事"之间的关系，这些模式描述了礼拜、启示和圣训叙事等主题。

Conclusion: Apriori算法能够自动揭示潜在的语义关系，为数字伊斯兰研究和基于技术的学习系统发展做出贡献，证明了无监督学习方法在圣训主题分组中的有效性。

Abstract: This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.

</details>


### [145] [Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning](https://arxiv.org/abs/2512.16698)
*Mahbub E Sobhani,Md. Faiyaz Abdullah Sayeedi,Mohammad Nehad Alam,Proma Hossain Progga,Swakkhar Shatabda*

Main category: cs.AI

TL;DR: 多智能体设计在几何问题求解中并非总是最优：开源模型受益明显，闭源模型在经典基准上单智能体表现更好


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型在几何问题求解中，多智能体设计相比单智能体的实际效益尚不明确，需要系统比较验证

Method: 在四个视觉数学基准（Geometry3K、MathVerse、OlympiadBench、We-Math）上系统比较单智能体和多智能体流水线，测试开源模型（Qwen-2.5-VL）和闭源模型（Gemini-2.0-Flash）

Result: 开源模型多智能体性能显著提升：Qwen-2.5-VL (7B)在Geometry3K上提升+6.8分，(32B)提升+3.3分；闭源模型Gemini-2.0-Flash在经典基准上单智能体表现更好，仅在较新的We-Math数据集上多智能体有适度改进

Conclusion: 多智能体流水线对开源模型有明显益处，对强大的专有系统在新基准上也有帮助，但智能体分解并非普遍最优策略

Abstract: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver

</details>


### [146] [Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems](https://arxiv.org/abs/2512.16707)
*Abhisek Ganguly*

Main category: cs.AI

TL;DR: 论文形式化了算法智能的两个独立计算限制：形式不完备性和动态不可预测性，并证明它们共同限制了智能体对其自身预测能力的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究算法智能的固有计算限制，特别是形式不完备性和动态不可预测性如何共同约束智能系统对其自身预测能力的推理。

Method: 形式化两个独立的计算限制：形式不完备性（限制一致推理系统的演绎能力）和动态不可预测性（限制有限精度下的长期预测）。分析这两个极端如何共同对智能体的自我预测能力推理施加结构性限制。

Result: 证明算法智能体通常无法计算自身的最大预测范围，揭示了智能系统中推理、预测和自我分析之间的固有权衡。

Conclusion: 形式不完备性和动态不可预测性共同为算法智能设定了基本限制，阐明了智能系统中推理、预测和自我分析之间的内在权衡关系。

Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.

</details>


### [147] [Discovering and Learning Probabilistic Models of Black-Box AI Capabilities](https://arxiv.org/abs/2512.16733)
*Daniel Bramblett,Rushang Karia,Adrian Ciotinga,Ruthvick Suresh,Pulkit Verma,YooJung Choi,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 本文提出了一种使用PDDL风格表示来学习黑盒AI系统规划能力的方法，通过蒙特卡洛树搜索生成测试任务并学习符号模型，能够描述AI的能力、执行条件和概率结果。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型等黑盒AI系统越来越多地用于序列决策，需要开发高效方法来提供这些系统能力的可靠且可解释的表示，以确保其部署和操作的安全性。

Method: 使用PDDL风格表示来学习黑盒AI的规划能力，采用蒙特卡洛树搜索范式系统性地创建测试任务、获取数据并剪枝可能的符号模型假设空间。

Result: 学习到的模型能够描述黑盒AI的能力、执行条件以及执行可能结果及其相关概率。理论结果证明了学习模型的可靠性、完整性和收敛性。多个黑盒AI系统的实证结果展示了方法的范围、效率和准确性。

Conclusion: PDDL风格表示可以有效地学习和建模黑盒AI系统的规划能力，为理解这些复杂系统的决策能力提供了一种系统化且可解释的方法。

Abstract: Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.

</details>


### [148] [AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach](https://arxiv.org/abs/2512.16739)
*Yipeng Zhuang,Yifeng Guo,Yuewen Li,Yuheng Wu,Philip Leung-Ho Yu,Tingting Song,Zhiyong Wang,Kunzhong Zhou,Weifang Wang,Li Zhuang*

Main category: cs.AI

TL;DR: 提出混合机器学习与大语言模型管道，利用结构化与非结构化电子病历数据预测肺癌患者住院期间48小时和72小时内的疼痛发作，准确率分别达0.874和0.917，敏感性提升8.6%和10.4%。


<details>
  <summary>Details</summary>
Motivation: 肺癌患者中高达91%会经历突发性疼痛，需要及时干预。现有方法难以有效预测疼痛发作，无法实现主动疼痛管理。需要开发能够整合多种医疗数据、提高预测准确性和临床可解释性的工具。

Method: 提出混合机器学习与大语言模型管道：1) 机器学习模块捕捉时间性药物使用趋势；2) 大语言模型解析模糊的用药记录和自由文本临床笔记。分析266名住院患者的回顾性队列，特征包括人口统计学、肿瘤分期、生命体征和WHO分级镇痛药使用情况。

Result: 框架在48小时预测中准确率为0.874，72小时预测中准确率为0.917。大语言模型的增强使敏感性分别提高了8.6%和10.4%。整合多模态数据改善了敏感性和可解释性。

Conclusion: 这种混合方法为早期疼痛发作预测提供了临床可解释且可扩展的工具，有望提高肿瘤护理中的治疗精准度和优化资源分配。

Abstract: Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.

</details>


### [149] [CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?](https://arxiv.org/abs/2512.16755)
*Siqi Wang,Chao Liang,Yunfan Gao,Erxin Yu,Sen Li,Yushi Li,Jing Li,Haofen Wang*

Main category: cs.AI

TL;DR: CitySeeker是一个评估视觉语言模型在动态城市环境中处理隐含需求导航能力的新基准，包含8个城市的6440条轨迹，发现即使最佳模型任务完成率也只有21.1%，主要瓶颈在于长时推理错误累积、空间认知不足和经验记忆缺陷。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在显式指令导航方面取得了进展，但它们在动态城市环境中解释隐含人类需求（如"我渴了"）的能力尚未得到充分探索。需要评估模型的空间推理和决策能力，以解决实际导航中的"最后一公里"挑战。

Method: 提出了CitySeeker基准，包含8个城市的6440条轨迹，涵盖7种目标驱动场景的多样化视觉特征和隐含需求。通过实验分析模型性能瓶颈，并探索了回溯机制、丰富空间认知和基于记忆检索（BCR）等策略，这些策略受到人类认知映射中迭代观察-推理循环和自适应路径优化的启发。

Result: 实验显示即使表现最佳的模型（如Qwen2.5-VL-32B-Instruct）任务完成率也只有21.1%。主要瓶颈包括：长时推理中的错误累积、空间认知不足、经验记忆缺陷。BCR策略的分析为开发具有强大空间智能的视觉语言模型提供了可行见解。

Conclusion: CitySeeker基准揭示了当前视觉语言模型在隐含需求导航方面的显著局限性，为解决"最后一公里"导航挑战提供了重要分析框架和开发方向，强调需要增强模型的空间智能和认知能力。

Abstract: Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., "I am thirsty") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling "last-mile" navigation challenges.

</details>


### [150] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: TOGGLE是一个新颖的LLM压缩框架，使用信号时序逻辑（STL）在压缩过程中形式化地指定和执行语言属性，通过STL鲁棒性引导的贝叶斯优化探索分层量化和剪枝配置，无需重新训练或微调即可生成满足语言约束的压缩模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言任务中表现出色，但需要大量计算资源，限制了在资源受限的边缘设备上的部署。现有的压缩技术（如量化和剪枝）通常会损害关键的语言属性，并且缺乏保持模型行为的形式化保证。

Method: 提出TOGGLE框架，利用信号时序逻辑（STL）形式化地指定和执行语言属性。采用STL鲁棒性引导的贝叶斯优化，系统性地探索分层量化和剪枝配置，生成满足指定语言约束的压缩模型，无需重新训练或微调。

Result: 在四种LLM架构（GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B）上评估TOGGLE，实现了高达3.3倍的计算成本（FLOPs）降低和高达68.8%的模型大小减少，同时满足所有语言属性。

Conclusion: TOGGLE首次将形式化方法集成到LLM压缩中，实现了在边缘硬件上高效、可验证的LLM部署，为资源受限环境下的模型压缩提供了形式化保证的新途径。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [151] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 论文提出"拼凑式AGI"假说，认为通用智能可能首先通过多个子AGI智能体协调实现，而非单一AGI系统，并为此构建分布式AGI安全框架


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和对齐研究主要关注单一AGI系统的防护，忽视了多个子AGI智能体通过协调合作实现通用能力的可能性。随着具备工具使用、通信协调能力的AI智能体快速部署，这种"拼凑式AGI"假说需要认真考虑并制定相应安全保障措施

Method: 提出分布式AGI安全框架，核心是设计和实施虚拟智能体沙盒经济系统（不可渗透或半渗透），通过稳健的市场机制管理智能体间交易，配合适当的可审计性、声誉管理和监督机制来缓解集体风险

Result: 建立了一个超越单一智能体评估和对齐的安全框架，将重点从个体智能体转向智能体群体协调产生的集体风险管理和安全保障

Conclusion: "拼凑式AGI"假说应得到严肃对待，需要开发相应的安全保障和缓解措施。提出的分布式AGI安全框架为应对多个智能体协调合作可能带来的集体风险提供了系统性的解决方案

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


### [152] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 该论文提出了社会责任感堆栈（SRS），这是一个六层架构框架，用于将社会价值观嵌入AI系统作为显式约束、保障措施、行为接口、审计机制和治理流程。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统越来越多地部署在影响人类行为、机构决策和社会结果的领域，但现有的负责任AI和治理工作虽然提供了重要的规范原则，却缺乏在整个系统生命周期中可执行的工程机制。

Method: 引入社会责任感堆栈（SRS）框架，将责任建模为社会技术系统的闭环监督控制问题，整合设计时保障措施与运行时监控和机构监督。采用统一的基于约束的表述，引入安全包络和反馈解释，展示如何持续监控和执行公平性、自主性、认知负担和解释质量等指标。

Result: 通过临床决策支持、协作式自动驾驶汽车和公共部门系统的案例研究，展示了SRS如何将规范目标转化为可操作的工程和运营控制。

Conclusion: SRS框架连接了伦理学、控制理论和AI治理，为负责任、适应性强且可审计的社会技术AI系统提供了实用基础。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>


### [153] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 提出Generative Adversarial Reasoner框架，通过对抗强化学习联合训练推理器和判别器，提高LLM数学推理能力


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然具备显式推理能力，但在数学推理中仍存在计算错误、逻辑脆弱和表面合理但无效的步骤等过程错误，需要改进推理质量

Method: 提出Generative Adversarial Reasoner框架：1）计算高效的审查计划将推理链划分为逻辑完整的片段；2）判别器评估每个片段的合理性并提供结构化理由；3）对抗强化学习联合训练推理器和判别器，推理器获得逻辑一致步骤的奖励，判别器获得正确检测错误的奖励

Result: 在多个数学基准测试中取得显著提升：AIME24上，DeepSeek-R1-Distill-Qwen-7B从54.0提升到61.3（+7.3），DeepSeek-R1-Distill-Llama-8B从43.7提升到53.7（+10.0）。该方法提供密集、校准良好的步骤级奖励，提高样本效率和推理质量

Conclusion: Generative Adversarial Reasoner框架通过对抗强化学习有效提升LLM的数学推理能力，模块化判别器支持灵活奖励塑造，适用于教师蒸馏、偏好对齐和数学证明推理等多种目标

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [154] [For how long time evolution of chaotic or random systems can be predicted](https://arxiv.org/abs/2512.16186)
*Leonid Bunimovich,Kirill Kovalenko*

Main category: nlin.CD

TL;DR: 论文证明在强混沌动力系统中，有限时间预测的时间区间长度随观测精度的提高呈指数增长而非线性增长


<details>
  <summary>Details</summary>
Motivation: 传统概率论和动力系统理论主要关注时间趋于无穷的极限定理，对有限时间动态的严格数学处理存在困难。研究开放动力系统中"洞"位置对逃逸过程的影响，开启了有限时间预测的新研究方向。

Method: 采用完全不同的技术方法，分析强混沌动力系统中轨道在相空间中的输运过程，特别是研究观测精度（相空间划分）对有限时间预测区间长度的影响。

Result: 发现有限时间预测可能的时间区间长度随观测精度的提高呈指数增长，而非先前认为的线性增长。

Conclusion: 强混沌动力系统的有限时间预测能力比预期更强，观测精度的指数级改进能显著扩展可预测的时间范围，这对实际应用具有重要意义。

Abstract: Traditionally, Probability theory was dealing with limit theorems where 'limit" means that time tends to infinity. Questions about finite time dynamics (evolution) were always considered as, although important for practical applications, but untreatable rigorously (mathematically). The same attitude was in the theory of strongly chaotic dynamical systems, which evolve similarly to stochastic processes. However, a natural question on dependence of the process of escape on a position of a "hole" in the state (phase) space, which was never asked in mathematical theory of open dynamical systems, opened up a new direction of research, which was dealing with finite time predictions of evolutions of such systems. It turned out, that transport of orbits in the phase space of the "most strongly chaotic" dynamical systems has three different stages. In the first stage there is a hierarchy of the first hitting probabilities, that shows which parts of the phase space the orbits of a system, which is an equilibrium state, will be more likely to visit the first. A principal (and the most important for applications) question was how the length of this interval changes with more refinement observations of the positions of the orbits in the phase space. Surprisingly, it turned out that the length of the time interval, where finite time predictions are possible, increases (rather to be shrinking), which, at the first sight, seems to be natural. However, this increase of the length of the time interval, where finite time predictions are possible, was rather slow (just linear) with respect to the growth of precision (partition of the phase space) of observations. In the present paper it is proved (by totally different technique) that this growth is actually exponential.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [155] [Emergence of higher-order interactions in systems of coupled Kuramoto oscillators with time delay](https://arxiv.org/abs/2512.16193)
*Narumi Fujii,Keisuke Taga,Riccardo Muolo,Bob Rink,Hiroya Nakao*

Main category: nlin.AO

TL;DR: 时间延迟的成对耦合在Kuramoto振子中自然产生高阶相互作用，通过二阶展开可得到包含二体和三体相互作用的无延迟有效模型，该模型能再现原系统的双稳态和同步转变。


<details>
  <summary>Details</summary>
Motivation: 虽然高阶相互作用框架能捕捉集体行为，但现实世界中关于动力学的例子仍然稀缺。本文旨在探索时间延迟如何自然地产生高阶相互作用，为理解延迟相互作用如何塑造动力学提供新视角。

Method: 在Kuramoto振子中，将时间延迟项在耦合强度上展开到二阶，推导出包含二体和三体相互作用的有效Kuramoto模型（无延迟）。通过数值模拟验证模型有效性，并应用Ott-Antonsen ansatz获得非相干态和同步态的稳定性图。

Result: 数值模拟显示简化模型能再现原始时间延迟系统的双稳态和同步转变。应用Ott-Antonsen ansatz得到的稳定性图与原模型结果高度匹配，表明时间延迟可有效转化为高阶相互作用形式。

Conclusion: 时间延迟可以有效地重新表述为高阶相互作用的形式，这为理解延迟相互作用如何塑造动力学提供了新的视角，同时为分析复杂系统提供了更易处理的框架。

Abstract: Understanding the mechanisms that govern collective synchronization is a paramount task in nonlinear dynamics. While higher-order (many-body) interactions have recently emerged as a powerful framework for capturing collective behaviors, real-world examples regarding dynamics remain scarce. Here, we show that higher-order interactions naturally emerge from time-delayed pairwise coupling in Kuramoto oscillators. By expanding the delay term up to second order in the coupling strength, we derive an effective Kuramoto model featuring both two-body and three-body interactions, but without delay, hence, easier to be analyzed. Numerical simulations show that this reduced model can reproduce the bistability and synchronization transitions of the original time-delayed system. Furthermore, applying the Ott-Antonsen ansatz, we obtain a stability diagram for incoherent and synchronized states that closely matches the results of the original model. Our findings reveal that time delays can be effectively recast in the form of higher-order interactions, offering a new perspective on how delayed interactions shape the dynamics.

</details>


### [156] [The Universe Learning Itself: On the Evolution of Dynamics from the Big Bang to Machine Intelligence](https://arxiv.org/abs/2512.16515)
*Pradeep Singh,Mudasani Rushikesh,Bezawada Sri Sai Anurag,Balasubramanian Raman*

Main category: nlin.AO

TL;DR: 该论文提出了一个统一的动力学系统叙事框架，将宇宙从大爆炸到当代人类社会及人工智能系统的结构形成过程视为连续的动力学演化链，强调相变、对称性破缺和涌现吸引子等数学主题。


<details>
  <summary>Details</summary>
Motivation: 传统上，宇宙学、天体物理学、地球物理学、生物学、认知科学和机器学习被视为分离的领域。本文旨在打破这种学科界限，提供一个跨尺度的理论视角，将宇宙历史解读为动力学本身的演化过程。

Method: 采用统一的动力学系统框架，将不同尺度的现象视为状态空间上连续演化的动力学体系。从暴胀场动力学和原初扰动开始，追踪引力不稳定性、耗散坍缩、地球化学循环、生命起源、进化生物学、大脑认知，直至人类文化和人工智能的演化。

Result: 构建了一个连贯的跨尺度叙事框架，展示了从宇宙起源到人工智能的连续结构形成过程。强调了一系列重复出现的数学主题：不稳定性、分岔、多尺度耦合，以及在可访问状态空间的测度零子集上的约束流。

Conclusion: 宇宙历史可以理解为动力学本身的演化过程，最终（迄今为止）产生了能够建模、预测并有意扰动自身未来轨迹的生物和人工系统。这提供了一个整合不同科学领域的统一视角，而非提出新的具体模型。

Abstract: We develop a unified, dynamical-systems narrative of the universe that traces a continuous chain of structure formation from the Big Bang to contemporary human societies and their artificial learning systems. Rather than treating cosmology, astrophysics, geophysics, biology, cognition, and machine intelligence as disjoint domains, we view each as successive regimes of dynamics on ever-richer state spaces, stitched together by phase transitions, symmetry-breaking events, and emergent attractors. Starting from inflationary field dynamics and the growth of primordial perturbations, we describe how gravitational instability sculpts the cosmic web, how dissipative collapse in baryonic matter yields stars and planets, and how planetary-scale geochemical cycles define long-lived nonequilibrium attractors. Within these attractors, we frame the origin of life as the emergence of self-maintaining reaction networks, evolutionary biology as flow on high-dimensional genotype-phenotype-environment manifolds, and brains as adaptive dynamical systems operating near critical surfaces. Human culture and technology-including modern machine learning and artificial intelligence-are then interpreted as symbolic and institutional dynamics that implement and refine engineered learning flows which recursively reshape their own phase space. Throughout, we emphasize recurring mathematical motifs-instability, bifurcation, multiscale coupling, and constrained flows on measure-zero subsets of the accessible state space. Our aim is not to present any new cosmological or biological model, but a cross-scale, theoretical perspective: a way of reading the universe's history as the evolution of dynamics itself, culminating (so far) in biological and artificial systems capable of modeling, predicting, and deliberately perturbing their own future trajectories.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [157] [Spin-dependent quasiparticle lifetimes in altermagnets](https://arxiv.org/abs/2512.15859)
*Kristoffer Leraand,Kristian Mæland,Asle Sudbø*

Main category: cond-mat.str-el

TL;DR: 研究反铁磁体中自旋分裂电子能带的多体效应，分析电子与磁振子、声子及混合模式的相互作用导致的能带展宽，探讨这些效应对光谱测量中自旋分裂可分辨性的影响。


<details>
  <summary>Details</summary>
Motivation: 反铁磁体中的自旋分裂电子能带在光谱测量中可能因多体相互作用导致的能带展宽而被模糊，需要量化这些效应以评估实验可观测性。

Method: 计算电子自能，考虑电子与磁振子、声子及磁振子-声子混合模式的相互作用；以d波Lieb晶格反铁磁体为代表性示例；包含温度依赖的自能计算。

Result: 自旋分裂在光谱上仍可分辨；电子-磁振子耦合导致费米面附近上下自旋的谱函数展宽存在明显差异，而电子-声子耦合无此差异；磁弹性耦合结果与纯磁振子情况相似。

Conclusion: 研究揭示了反铁磁体中准粒子动力学特性，为理解自旋分裂系统中的多体相互作用提供了理论依据，并量化了热涨落对电子态展宽的影响。

Abstract: We investigate many-body effects on the spin-split electron bands in altermagnets by computing the electron self-energy arising from interactions with magnons, phonons, and hybridized magnon-phonon modes. These interactions lead to band broadening, which can obscure the intrinsic spin-splitting in spectroscopic measurements. We consider a $d$-wave Lieb lattice altermagnet as a representative example. Our results reveal that the spin-splitting remains spectroscopically resolvable and provide theoretical estimates of lifetime effects relevant for experimental detection. For electron-magnon coupling, we find a distinct difference between spectral function broadening for up and down spins close to the Fermi surface, which is not present in the case of electron-phonon coupling. We relate it to the spin splitting of the magnon modes in altermagnets. The results, including magneto-elastic coupling, are very similar to the pure magnon case. This provides insights into quasiparticle dynamics in altermagnets and contributes to the broader understanding of many-body interactions in spin-split systems. By including the temperature dependence of the self-energies, we also quantify how thermal fluctuations influence the broadening of the electronic states.

</details>


### [158] [Anyon Dispersion in Aharonov-Casher Bands and Implications for Twisted MoTe${}_2$](https://arxiv.org/abs/2512.15863)
*Zihan Yan,Qingchen Li,Tomohiro Soejima,Eslam Khalaf*

Main category: cond-mat.str-el

TL;DR: 该论文开发了分数量子反常霍尔态中任意子色散的理论框架，通过构造准空穴动量本征态计算色散，发现色散带宽随量子几何不均匀性和相互作用屏蔽长度增加而增大，并建立了微观拉格朗日框架解释色散机制。


<details>
  <summary>Details</summary>
Motivation: 分数量子反常霍尔态的发现为实现色散任意子相开辟了新途径，但需要理论工具来理解任意子在晶格系统中的色散行为及其物理机制。

Method: 1. 将相互作用投影到Laughlin准空穴空间；2. 构造准空穴动量本征态；3. 使用蒙特卡洛方法高效计算单准空穴色散；4. 建立基于准空穴引导中心坐标的微观拉格朗日框架。

Result: 1. 准空穴带宽随量子几何不均匀性和相互作用屏蔽长度增加而增大；2. 对于扭曲MoTe2体系，带宽约为1 meV量级；3. 色散源于相互作用产生的周期势和准空穴多体贝里相的共同作用；4. 引导中心坐标的非对易结构将周期势转化为有限色散。

Conclusion: 该理论为分数量子反常霍尔态中任意子色散提供了分析可控的描述，揭示了色散的微观机制，并建立了可推广到多准空穴系统的框架，为理解FQAH系统中的带电激发提供了微观理论。

Abstract: The discovery of fractional quantum anomalous Hall (FQAH) states in two-dimensional heterostructures has opened the door to realizing phases of dispersing anyons. Here, we develop an analytically controlled theory of anyon dispersion in FQAH states realized in ideal or Aharonov-Casher (AC) bands by projecting interactions onto the space of Laughlin quasiholes. Constructing quasihole momentum eigenstates allows efficient evaluation of the single quasihole dispersion using Monte Carlo. We find that the quasihole bandwidth grows with increasing quantum-geometry inhomogeneity of the AC band and with increasing interaction screening length. For realistic parameters relevant to the bands of twisted MoTe${}_2$, the quasihole bandwidth is of order 1 meV, suggesting that itinerant-anyon physics may play an important role in sufficiently clean samples. Furthermore, we develop a microscopic Lagrangian framework in terms of a quasihole guiding-center coordinate, which reproduces the momentum-space formula for the dispersion. This approach reveals that quasihole dispersion originates from the combined effects of an interaction-generated periodic potential, arising from non-uniform quantum geometry of the single particle bands, and the quasihole many-body Berry phase arising from the background magnetic field. The latter endows the guiding-center coordinate with a noncommutative structure, converting the periodic potential into a finite dispersion. Finally, we outline how this framework generalizes to multiple quasiholes, enabling a microscopic theory of charged excitations in FQAH systems that retains only the anyon degrees of freedom.

</details>


### [159] [Classifying one-dimensional Floquet phases through two-dimensional topological order](https://arxiv.org/abs/2512.15868)
*Campbell McLauchlan,Vedant Motamarri,Benjamin Béri*

Main category: cond-mat.str-el

TL;DR: 本文提出使用对称拓扑场论（SymTFT）方法分类1D Floquet MBL相，通过将系统视为高维拓扑序的边界，统一了Floquet相的分类框架。


<details>
  <summary>Details</summary>
Motivation: Floquet系统展现出丰富现象（如时间晶体），但Floquet MBL相的统一分类框架尚未完全建立。静态相已通过SymTFT方法成功研究，本文旨在将这一方法扩展到Floquet系统。

Method: 采用对称拓扑场论（SymTFT）方法，将1D G对称Floquet MBL系统视为G量子双（quantum double）的边界，通过考虑拉格朗日子群和边界激发来分类相。

Result: 1. 分类覆盖所有已知Floquet相，并发现新的未探索相；2. 揭示了具有边界特征的相的体特征（"对偶"时间晶体）；3. 扩展到扭曲量子双边界，发现具有非在位对称性的新时间晶体相；4. 数值模拟显示该相具有绝对稳定性，开边界条件下对对称扰动更稳定。

Conclusion: SymTFT为统一Floquet系统的相和特征提供了强大框架，能够分类已知相并发现新相，为在可编程量子设备中实现和探测这些相提供了新视角。

Abstract: Floquet systems display rich phenomena, such as time crystals, with many-body localisation (MBL) protecting the phases from heating. While several types of Floquet phases have been classified, a unified picture of Floquet MBL is still emerging. Static phases have been fruitfully studied via "symmetry topological field theory" (SymTFT), wherein the universal features of $G$-symmetric systems are elucidated by placing them on the boundary of a topological order of one dimension higher. In this work, we provide a SymTFT approach to classifying $G$-symmetric Floquet MBL phases in 1D, for $G$ a finite Abelian group with on-site unitary action. In the SymTFT, these 1D systems correspond to the boundaries of the quantum double associated to $G$, and the classification naturally arises from considering the Lagrangian subgroups and boundary excitations of the quantum double. The classification covers all known Floquet phases while uncovering others previously unexplored, along with bulk features of phases thought to have only boundary signatures. We refer to the latter phases as "dual" time crystals. For static phases, we show how anyons of the quantum double and (string) order parameters provide a natural and simple interpretation of known classification schemes. By extending our framework to the boundaries of twisted quantum doubles, we uncover a new time-crystalline phase with non-onsite symmetry, which cannot be obtained through local, symmetric Hamiltonian drives. We numerically demonstrate evidence for the absolute stability of this phase, and observe that for open boundary conditions it has greater stability to symmetric perturbations. We finally discuss perspectives on using programmable quantum devices to realise and probe the phases we discuss. Our results show that SymTFT provides a powerful approach to unifying phases and features of Floquet systems.

</details>


### [160] [Imaging Electron-Hole Asymmetry in the Quantum Melting of Generalized Wigner Crystals](https://arxiv.org/abs/2512.16050)
*Emma Berger,Michael Arumainayagam,Zhihuan Dong,Lucas Schneider,Tianle Wang,Greyson Nichols,Salman Kahn,Rwik Dutta,Gaoqiang Wang,Takashi Taniguchi,Kenji Watanabe,Mit H. Naik,Michael P. Zaletel,Feng Wang,Michael F. Crommie*

Main category: cond-mat.str-el

TL;DR: 该研究通过扫描隧道显微镜直接观测了电子掺杂的60°扭曲MoSe2双层中广义维格纳晶体和莫特绝缘体的密度驱动熔化过程，发现了电子-空穴不对称的熔化行为。


<details>
  <summary>Details</summary>
Motivation: 研究二维莫尔材料中强关联系统的相变，特别是广义维格纳晶体和莫特绝缘体在密度驱动下的熔化行为，以理解这些量子相变的基本物理机制。

Method: 使用扫描隧道显微镜（STM）对电子掺杂的近60°扭曲MoSe2双层进行成像，该材料具有三角形莫尔超晶格结构，直接观测广义维格纳晶体和莫特绝缘体的熔化过程。

Result: 观察到广义维格纳晶体熔化存在显著的电子-空穴不对称性：空穴掺杂的广义维格纳晶体产生相互作用驱动的无序态，而电子掺杂的广义维格纳晶体熔化形成离域液体状态。莫特绝缘体熔化则没有这种不对称性。

Conclusion: 电子-空穴不对称性源于莫尔超晶格破坏的粒子-空穴对称性，导致广义维格纳晶体凝聚后电子和空穴费米口袋具有不同的动量几何结构。该工作为广义维格纳晶体量子熔化转变中出现的新奇涌现相提供了直接可视化证据。

Abstract: Two-dimensional moiré materials provide a versatile platform to explore phase transitions in strongly correlated systems. Using scanning tunneling microscopy (STM) we have imaged the density-driven melting of generalized Wigner crystals (GWCs) and Mott insulators (MIs) in electron-doped, near-60° twisted MoSe2 bilayers featuring a triangular moiré superlattice. We observe striking electron-hole asymmetry in GWC melting: hole-doped GWCs yield interaction-driven disordered states whereas electron-doped GWCs melt into delocalized liquid-like states. This asymmetry arises from the broken particle-hole symmetry of the moiré superlattice, which produces electron and hole Fermi pockets with different momentum geometries upon GWC condensation. MI states melt without such asymmetry, consistent with the absence of a symmetry-breaking density modulation. This work provides direct visualization of the novel emergent phases that appear as GWCs undergo quantum melting transitions.

</details>


### [161] [Recent progress in quantum spin liquids, fractional magnetization plateaus, and unconventional superconductivity in kagome lattices](https://arxiv.org/abs/2512.16131)
*Li-Wei He,Shun-Li Yu,Jian-Xin Li*

Main category: cond-mat.str-el

TL;DR: 这篇综述总结了kagome晶格在量子多体物理研究中的最新进展，包括量子自旋液体、分数磁化平台相和非常规超导性，特别关注了AV₃Sb₅等钒基kagome超导体的研究。


<details>
  <summary>Details</summary>
Motivation: kagome晶格因其独特的几何结构成为研究量子多体物理的重要平台，特别是在探索量子自旋液体和非常规超导性方面具有重要价值。本文旨在系统总结该领域的最新研究进展，为相关研究提供全面的综述。

Method: 采用综述研究方法，首先分析最近邻kagome反铁磁海森堡模型的经典基态性质，然后介绍量子自旋液体和分数磁化平台相的实验进展。接着讨论量子自旋液体态的费米子描述、相关规范理论和变分蒙特卡洛方法。最后分析kagome系统的电子结构特征和可能的电子不稳定性，并综述AV₃Sb₅非常规超导性的实验进展。

Result: 综述了kagome晶格在量子自旋液体、分数磁化平台相和非常规超导性方面的最新研究进展，特别强调了变分蒙特卡洛方法在研究kagome反铁磁体中的应用，以及AV₃Sb₅体系中手性超导性和配对密度波的实验发现。

Conclusion: kagome晶格为探索量子多体物理提供了丰富的平台，在量子自旋液体和非常规超导性研究中取得了重要进展。未来的研究将继续深入理解这些奇异量子态的性质和机制，推动凝聚态物理的发展。

Abstract: The kagome lattice, with its unique geometric structure, has emerged as a leading platform for exploring quantum many-body physics, particularly in the study of quantum spin liquids (QSLs) and unconventional superconductivity. This review highlights recent advancements in the investigations of QSLs, fractional magnetization plateau phases in kagome antiferromagnets, and unconventional superconductivity in vanadium-based kagome superconductors. We begin by examining the classical ground-state properties of the nearest-neighbor kagome antiferromagnetic Heisenberg model and introducing recent experimental progress in the study of QSLs and fractional magnetization plateau phases. Next, we discuss the fermionic description of the QSL states, along with related gauge theory and the variational Monte Carlo (VMC) method. We then focus on discussing the VMC studies of QSLs and magnetization plateau phases in kagome antiferromagnets. For superconductivity in kagome systems, we first analyze the characteristics of the electronic structure and the possible associated electronic instabilities. Finally, we review recent experimental advances in unconventional superconductivity in AV$_3$Sb$_5$ (A = K, Rb, Cs), with a particular focus on chiral superconductivity and pairing density waves.

</details>


### [162] [Power-Law Suppression of Phonon Thermal Transport by Magnetic Excitations in a Molecular Quantum Spin Liquid](https://arxiv.org/abs/2512.16137)
*S. Fujiyama,K. Ueda,Y. Otsuka*

Main category: cond-mat.str-el

TL;DR: 该研究通过大规模从头算声子计算，分析了分子量子自旋液体X[Pd(dmit)2]2的声子特性，发现极低的平均声子速度（700 m/s）和低于10 cm^{-1}的光学模式将德拜T^3区域限制在T < 2 K。随着转移积分各向异性接近最大阻挫区域，晶格变硬，排除了晶格软化作为自旋液体态起源的可能性。通过量化实验数据中热导率的额外抑制，观察到与二维磁激发一致的行为，具有节点近似线性（狄拉克型）谱。


<details>
  <summary>Details</summary>
Motivation: 研究分子量子自旋液体X[Pd(dmit)2]2的声子特性，探究自旋液体态的物理起源，特别是检验晶格软化是否是该自旋液体态形成的关键机制。

Method: 采用大规模从头算声子计算方法，分析X[Pd(dmit)2]2的声子谱特性。通过计算声子速度、光学模式频率等参数，研究晶格动力学行为。同时结合实验数据，量化热导率的额外抑制，分析磁激发的维度特性和谱特征。

Result: 1. 发现异常低的平均声子速度（约700 m/s）和低于10 cm^{-1}的光学模式，将德拜T^3区域限制在T < 2 K以下；2. 当转移积分各向异性接近最大阻挫区域（t'/t → 1）时，晶格变硬而非软化，排除了晶格软化作为自旋液体态起源的可能性；3. 热导率实验数据的额外抑制显示出幂律行为，与二维磁激发一致，具有节点近似线性（狄拉克型）谱特征。

Conclusion: 该研究通过声子计算和实验数据分析，排除了晶格软化作为X[Pd(dmit)2]2分子量子自旋液体态起源的可能性。相反，观察到的热导率行为支持二维磁激发的存在，具有狄拉克型谱特征，为理解这类自旋液体材料的物理机制提供了重要线索。

Abstract: We present large-scale ab initio phonon calculations for the molecular quantum spin liquid X[Pd(dmit)2]2. An unusually low average phonon velocity ( 700 {m/s}) and optical modes below 10 cm^{-1} confine the Debye T^{3} regime to T < 2 K. As the transfer-integral anisotropy approaches the maximally frustrated regime (t'/t \to 1), the lattice stiffens, ruling out lattice softening as the origin of the spin-liquid state. By quantifying the additional suppression of the thermal conductivity from experimental data, we observe a power-law behavior consistent with two-dimensional magnetic excitations with a nodal, approximately linear (Dirac-like) spectrum.

</details>


### [163] [Tunable Topological Phases in an Organic One-Dimensional Mott Chain: Odd-Haldane (S = 1/2) and Haldane (S = 1)](https://arxiv.org/abs/2512.16173)
*Khalid N. Anindya,Hong Guo*

Main category: cond-mat.str-el

TL;DR: 该论文展示了一种可化学调控的有机一维链系统，能够实现两种对称性保护拓扑相：奇数Haldane相和S=1 Haldane相，为相互作用SPT物理提供了分子平台。


<details>
  <summary>Details</summary>
Motivation: 在化学现实系统中建立具有相互作用的对称性保护拓扑相仍然是一个开放挑战。研究者旨在寻找一种可化学调控的有机分子平台，实现一维相互作用SPT物理。

Method: 使用密度泛函理论确定系统处于强Mott区（U/t≈126），通过紧凑的(t,U)→J映射固定交换耦合，采用精确对角化和DMRG方法分析拓扑特征。

Result: 系统成功实现两种SPT相：S=1/2二聚化海森堡链的奇数Haldane相和S=1链的Haldane相。观察到一致的SPT指纹特征，包括量子化多体Zak相位、偶数简并纠缠谱、受保护边缘自旋以及特征性的三粒子/Haldane特征。

Conclusion: 该研究确定了一种化学可编程的分子平台，用于一维相互作用SPT物理，并为有机Haldane自旋链在纳米尺度量子器件中的应用提供了具体的光谱学途径。

Abstract: Establishing symmetry-protected topological (SPT) phases with interactions in chemically realistic systems remains an open challenge. We show that a single, synthetically plausible organic one-dimensional chain, tunable via chemical modification of its radical sites, hosts two such phases: an odd-Haldane phase of a dimerized $S=\tfrac{1}{2}$ Heisenberg chain and a Haldane phase of an $S=1$ chain realized when Hund coupling locks two $S=\tfrac{1}{2}$ spins per monomer into $S=1$. Density-functional theory places the active manifold deep in the Mott regime ($U/t\!\approx\!126$), justifying a spin-only Heisenberg description; a compact $(t,U)\!\to\!J$ mapping then fixes exchange couplings. Exact diagonalization and DMRG reveal a consistent SPT fingerprint across both phases, including a quantized many-body Zak phase, even-degenerate entanglement spectrum, protected edge spins, and characteristic triplon/Haldane features in $S^{+-}(q,ω)$. Our results identify a chemically programmable molecular platform for interacting SPT physics in one dimension and suggest concrete spectroscopic routes to organic Haldane spin chains for nanoscale quantum devices.

</details>


### [164] [Atomic-scale control of substrate-spin coupling via vertical manipulation of a 2D metal-organic framework](https://arxiv.org/abs/2512.16194)
*Benjamin Lowe,Bernard Field,Dhaneesh Kumar,Daniel Moreno Cerrada,Oleksandr Stetsovych,Julian Ceddia,Andrés Pinar Solé,Amelia Domínguez-Celorrio,Jack Hellerstedt,Sinéad M. Griffin,Pavel Jelínek,Agustin Schiffrin*

Main category: cond-mat.str-el

TL;DR: 通过扫描隧道显微镜探针垂直操纵二维莫特绝缘体Kagome金属有机框架，实现对Kondo耦合强度的可控可逆调节


<details>
  <summary>Details</summary>
Motivation: 具有受挫晶体几何结构的二维材料可以承载强关联电子，产生多种奇异的多体量子相。控制这些系统中的交换耦合对于探索量子相和潜在应用具有重要意义。

Method: 使用原子级锐利的扫描隧道显微镜探针垂直操纵Ag(111)衬底上的二维莫特绝缘体Kagome金属有机框架，通过控制MOF的吸附高度来调节Kondo耦合强度。

Result: 能够可控且可逆地改变MOF局域自旋与衬底传导电子之间的Kondo耦合强度，实现了对Kondo耦合的机械控制。

Conclusion: 这种机械控制Kondo耦合的方法可扩展到其他形式的层间交换耦合，为原子尺度设计或控制自旋电子学技术提供了可能。

Abstract: Two-dimensional (2D) materials with frustrated crystal geometries can host strongly correlated electrons, potentially leading to a range of exotic many-body quantum phases such as Mott insulators, quantum spin-liquids, and Kondo lattices. The ability to control exchange-coupling within these systems is therefore highly desirable. Here, we use an atomically sharp scanning tunneling microscope probe to vertically manipulate a 2D Mott insulating kagome metal-organic framework (MOF) featuring Kondo-screened local magnetic moments on Ag(111). We show that by controlling the adsorption height of the MOF, we can also controllably and reversibly change the strength of Kondo coupling between the MOF's local spins and the substrate's conduction electrons. This mechanical control of Kondo coupling could be extended to other forms of interlayer exchange coupling, potentially allowing for atomic-scale design or control of spintronics technologies.

</details>


### [165] [Magnetic-field-induced insulating behavior in black phosphorus under pressure](https://arxiv.org/abs/2512.16252)
*Kazuto Akiba,Yuzuki Sega,Yuichi Akahama,Yuta Seo,Tomoki Machida,Masashi Tokunaga*

Main category: cond-mat.str-el

TL;DR: 在纵向场配置下研究加压黑磷的面外磁阻，发现在无洛伦兹力情况下仍出现显著增强的磁阻效应，在半导体和半金属相中均有明显起始场，表明可能存在场诱导电子相变。


<details>
  <summary>Details</summary>
Motivation: 研究加压黑磷在纵向磁场配置下的磁阻行为，探索在无洛伦兹力情况下仍观察到的显著磁阻增强现象，以及可能存在的场诱导电子相变。

Method: 采用纵向场配置研究加压黑磷的面外磁阻，分别在1.1 GPa（半导体相）和1.3 GPa（半金属相）下进行测量，分析磁阻增强现象及其起始场特征。

Result: 在纵向场配置下观察到显著增强的磁阻效应，在半导体和半金属相中均有明显起始场；在半导体-半金属转变压力附近观察到绝缘行为，可能与激子相的出现有关；加压黑磷可在9 T以下中等磁场中实现场诱导电子相变。

Conclusion: 加压黑磷在纵向磁场配置下表现出异常的磁阻增强现象，表明可能存在场诱导电子相变，特别是在半导体-半金属转变区域可能出现的激子相，使其成为在中等磁场下研究电子相变的候选材料。

Abstract: We investigated the out-of-plane magnetoresistance of pressurized black phosphorus (BP) with a longitudinal field configuration. Despite the absence of the Lorentz force in the present configuration, we observed a significant enhancement of magnetoresistance marked with a clear onset field in both the semiconducting (1.1 GPa) and semimetallic (1.3 GPa) phases. The insulating behavior observed near the semiconductor-semimetal transitio pressure is possibly associated with emergence of an excitonic phase, which has been suggested in a recent theoretical study. BP under finely tuned pressure can be a candidate to realize the field-induced electronic phase transition in a moderate magnetic field below 9 T.

</details>


### [166] [Interfacial Strain Modulated Correlated Plasmons in La1.85Sr0.15CuO4 and Their Role in High-temperature Superconductivity](https://arxiv.org/abs/2512.16417)
*Xiongfang Liu,Shengwei Zeng,Xun Liu,Kun Han,Difan Zhou,Chi Sin Tang,Ping Yang,Mark B. H. Breese,Chuanbing Cai,Ariando Ariando,Mi Jiang,Xinmao Yin*

Main category: cond-mat.str-el

TL;DR: 通过光谱椭偏仪在超导LSCO中发现界面应变调制的关联等离子体，这种等离子体在非超导LSCO中不存在，表明长程电子关联在高温超导中起关键作用。


<details>
  <summary>Details</summary>
Motivation: 铜酸盐高温超导机制仍然是物理学中的重大挑战，界面应变是调控复杂氧化物中电子关联的有效手段，可能为控制量子相提供新途径。

Method: 使用光谱椭偏仪观测La1.85Sr0.15CuO4（LSCO）中的关联等离子体，通过详细分析确定其源于Mott关联能带内的集体激发，并结合动力学团簇近似（DCA）与量子蒙特卡洛（QMC）计算扩展哈伯德模型进行理论验证。

Result: 在超导LSCO中发现了界面应变调制的关联等离子体，这种等离子体在非超导LSCO中不存在；理论计算表明长程库仑相互作用在LSCO中起重要作用，与实验结果一致。

Conclusion: 界面应变精细调控的长程电子关联可能在超导性的出现和转变温度调节中起关键作用，为理解高温超导中的集体激发及其作用提供了新视角。

Abstract: High-temperature superconductivity in cuprate materials remains a major challenge in physics due to the complexity of their strongly correlated electronic states. Interfacial strain is a powerful lever for tuning electronic correlations in complex oxides, offering new pathways to control emergent quantum phases. Here, we report the discovery of interfacial strain modulated correlated plasmons observed exclusively in superconducting La1.85Sr0.15CuO4 (LSCO) through spectroscopic ellipsometry. This form of plasmons is absent in the non-superconducting LSCO counterparts. Detailed analysis reveals that these correlated plasmons, arising from the collective excitations within Mott-correlated bands, are driven by long-range electronic correlations in the Cu-O planes. Furthermore, long-range electronic correlations, intricately modulated by interfacial strain, may play a crucial role in the emergence of superconductivity and in tuning the transition temperature. Dynamical cluster approximation (DCA) with quantum Monte Carlo (QMC) calculations of the extended Hubbard model suggest that long-range Coulomb interactions play an important role in LSCO, showing good agreement with our experimental findings. The collective evidence from both the experimental results and theoretical findings provides new insights into the nature of collective excitations and their pivotal role in the emergence of high-temperature superconductivity.

</details>


### [167] [Moiré-modulated $Γ$ valley in twisted bilayer and twisted double-bilayer MoTe$_2$](https://arxiv.org/abs/2512.16431)
*Wanying Chen,Hongyun Zhang,Jinxi Lu,Yu Gu,Qiyun Xu,Fei Wang,Xuanxi Cai,Jiansong Li,Jiayong Xiao,Rui Chen,Kenji Watanabe,Takashi Taniguchi,Jose Avila,Pavel Dudin,Matthew D. Watson,Pu Yu,Shengwei Jiang,Wenhui Duan,Tingxin Li,Chong Wang,Shuyun Zhou*

Main category: cond-mat.str-el

TL;DR: 该研究通过直接可视化扭曲双层和扭曲双双层MoTe₂在临界扭转角附近的电子结构，揭示了莫尔超晶格对Γ和K谷能量关系的调控作用，以及Γ谷p_z能带的显著分裂现象。


<details>
  <summary>Details</summary>
Motivation: 扭曲MoTe₂在3.7°附近表现出分数量子反常霍尔效应等有趣的关联量子现象，这些现象对扭转角和莫尔超晶格敏感。然而，扭转角如何调制电子结构以及莫尔超晶格的具体影响机制尚不清楚，需要直接可视化研究。

Method: 研究采用直接可视化技术，观测扭曲双层和扭曲双双层MoTe₂在临界扭转角附近的电子结构。通过理论分析，探究了扭转角依赖的晶格弛豫（特别是界面波纹）对电子结构调制的影响。

Result: 研究发现莫尔超晶格不仅改变了扭曲双层MoTe₂中Γ和K谷的相对能量，还强烈重构了Γ谷（对扭曲双层和扭曲双双层均如此）。具体而言，Γ谷的深p_z能带表现出明显的分裂，且这种分裂随扭转角增加而系统变化。

Conclusion: 该工作直接可视化了莫尔调制的电子结构，为理解扭曲MoTe₂物理背后的晶格弛豫和层间相互作用提供了关键的光谱信息，揭示了扭转角依赖的晶格弛豫（特别是界面波纹）是电子结构调制的主要机制。

Abstract: Twisted MoTe$_2$ hosts intriguing correlated quantum phenomena including the fractional quantum anomalous Hall effect in twisted bilayer (t-BL) MoTe$_2$ near 3.7$^\circ$, which is sensitive to the twist angle and moiré superlattices. Here, we directly visualize the twist-angle-modulated electronic structure of t-BL and twisted double-bilayer (t-DBL) near this critical angle. We find that the moiré superlattice not only modifies the relative energy between $Γ$ and K valleys in t-BL MoTe$_2$, but also strongly reconstructs the $Γ$ valley for both t-BL and t-DBL. Specifically, the deep $p_z$-derived band at $Γ$ exhibits a distinct splitting that systematically varies with increasing twist angle. Theoretical analysis suggests that this modulation arises from the twist-angle-dependent lattice relaxation, especially interfacial corrugations. Our work directly visualizes the moiré-modulated electronic structure and provides key spectroscopic information of lattice relaxation and interlayer interactions underlying the physics of twisted MoTe$_2$.

</details>


### [168] [Fractional Chern insulator with higher Chern number in optical lattice](https://arxiv.org/abs/2512.16459)
*Ying-Xing Ding,Wen-Tong Li,Li-Min Zhang,Yu-Biao Wu,Duanlu Zhou,Lin Zhuang,Wu-Ming Liu*

Main category: cond-mat.str-el

TL;DR: 通过层间耦合方案在双层棋盘晶格中构建C=2的平带，实现了高阶陈数的分数量子霍尔态


<details>
  <summary>Details</summary>
Motivation: 高阶陈数平带系统能模拟更高朗道能级的物理并增强拓扑鲁棒性，但在这样的平带系统中实现高阶陈数的关联分数量子相仍具挑战性

Method: 提出层间耦合方案，将双层棋盘晶格中两个C=1的能带通过解除简并和合并拓扑指数转化为单个C=2的平带

Result: 精确对角化计算显示该工程化能带承载C=2/3和2/5两个分数量子陈绝缘体态，并提出用冷碱土金属原子在有效双层光学晶格中模拟这些态的实验方案

Conclusion: 该工作提供了构建高阶陈数平带的通用策略，为探索奇异分数量子相开辟了新途径

Abstract: Fractional Chern insulators arise in topologically nontrivial flat bands, characterized by an integer Chern number C that corresponds to the number of dissipationless edge states in the non-interacting regime. Higher Chern numbers can replicate the physics of higher Landau levels and often confer enhanced topological robustness. However, realizing correlated fractional phases with higher Chern numbers in such flat band systems remains challenging. Here, we propose an interlayer coupling scheme to generate higher Chern numbers in a flat-band system, where the interlayer coupling transforms two C = 1 bands in a bilayer checkerboard lattice into a single flat band with C = 2 by lifting their degeneracy and merging their topological indices. Exact diagonalization calculation reveals that this engineered band hosts two fractional Chern insulator states with C = 2/3 and 2/5, respectively. An experimental setup is proposed to simulate these states using cold alkaline-earth-like atoms in an effective bilayer optical lattice. Our work provides a general and widely applicable strategy for constructing higher Chern number flat bands, opening a pathway to explore exotic fractional quantum phases

</details>


### [169] [Hyperfine coupling in singlet ground state magnets](https://arxiv.org/abs/2512.16464)
*Peter Thalmeier*

Main category: cond-mat.str-el

TL;DR: 该研究探讨了核自旋超精细耦合和核四极分裂对单重态基态磁体中诱导磁矩有序的影响，发现超精细相互作用抑制量子临界点，导致电子诱导磁矩和核超精细耦合主导的磁有序之间的渐变交叉。


<details>
  <summary>Details</summary>
Motivation: 研究非Kramers f电子化合物中单重态基态磁体的磁有序行为，特别关注核自旋超精细耦合和核四极分裂对量子临界点和磁相变的影响。

Method: 通过理论分析研究超精细相互作用对单重态基态磁体磁有序的影响，包括分析临界温度对电子和核控制参数的依赖关系，以及比热在控制参数和外加磁场下的变化。

Result: 超精细相互作用抑制量子临界点，导致电子诱导磁矩和核超精细耦合主导的磁有序之间的渐变交叉；比热的三峰结构在电子诱导磁矩区逐渐转变为核磁矩有序区或强磁场下的双峰结构；发现了磁有序的重入行为和非单调临界场。

Conclusion: 核自旋超精细耦合和核四极分裂显著影响单重态基态磁体的磁有序行为，改变量子临界点性质，导致相变特征的系统性演化，为理解这类磁体的复杂磁行为提供了重要理论框架。

Abstract: The influence of hyperfine coupling to nuclear spins and of their quadrupolar splitting on the induced moment order in singlet ground state magnets is investigated. The latter are found among non-Kramers f electron compounds. Without coupling to the nuclear spins these magnets have a quantum critical point (QCP) separating paramagnetic and induced moment regime. The hyperfine interaction suppresses the QCP and leads to a gradual crossover between induced electronic and nuclear hyperfine coupling dominated magnetic order. It is shown how the critical temperature depends on the electronic and nuclear control parameters including the nuclear spin size and its possible nuclear quadrupole splitting. In particular the dependence of the specific heat on the control parameters and applied field is investigated for ferro- and antiferromagnetic order. It is shown that the three peak structure in the electronic induced moment regime gradually changes to a two-peak structure in the hyperfine coupling dominated nuclear moment order regime or for increasing field strength. Most importantly the possibility of a reentrance behaviour of magnetic order or likewise nonmonotonic critical fields due to hyperfine coupling influence is demonstrated. Finally the systematic evolution of the phase diagram under the influence of nuclear quadrupole coupling is clarified.

</details>


### [170] [Theory of exciton polarons in 2D Wigner crystals](https://arxiv.org/abs/2512.16651)
*Haydn S. Adlong,Eugen Dizer,Richard Schmidt,Atac Imamoglu,Arthur Christianen*

Main category: cond-mat.str-el

TL;DR: 该研究开发了激子与维格纳晶体中局域电子相互作用的微观理论模型，解释了TMDs中激子-极化子光谱的实验观测，揭示了强关联电子系统的特征。


<details>
  <summary>Details</summary>
Motivation: 单层过渡金属二硫化物（TMDs）为实现维格纳晶体提供了平台，并能通过激子光谱进行检测。然而，需要建立激子与维格纳晶体中局域电子相互作用的微观理论模型，以解释实验观测到的激子-极化子光谱特征。

Method: 开发了激子与维格纳晶体中局域电子相互作用的微观理论模型，包括电子的振动运动。该模型考虑了激子-乌姆克拉普特征和更高能带的吸引极化子共振。

Result: 理论模型不仅重现了先前观测到的激子-乌姆克拉普特征，还解释并重现了最近实验报告的高能带吸引极化子共振。模型进一步揭示，在WSe₂和WS₂中观察到的两个强度相等且平行的吸引极化子，是电子系统中强关联的特征。

Conclusion: 研究结果表明，考虑电子相互作用对于重现和解释TMDs中的激子-极化子光谱至关重要。该工作为理解TMDs中维格纳晶体的激子光谱提供了理论基础。

Abstract: Monolayer transition-metal dichalcogenides (TMDs) provide a platform for realizing Wigner crystals and enable their detection via exciton spectroscopy. We develop a microscopic theoretical model for excitons interacting with the localized electrons of the Wigner crystal, including their vibrational motion. In addition to the previously observed exciton-Umklapp feature, the theory reproduces and explains the higher-band attractive-polaron resonances recently reported experimentally. Our model further uncovers that the appearance of two equal-strength and parallel attractive polarons, as commonly observed in WSe$_2$ and WS$_2$, is a signature of strong correlations in the electronic system. Altogether, our results demonstrate that accounting for electronic interactions is essential to reproduce and interpret the exciton-polaron spectra of TMDs.

</details>


### [171] [Strain-Controlled Magnetic Phase Transitions through Anisotropic Exchange Interactions: A Combined DFT and Monte Carlo Study](https://arxiv.org/abs/2512.16765)
*Sudip Mandal,Mihir Ranjan Sahoo,Kalpataru Pradhan*

Main category: cond-mat.str-el

TL;DR: 该研究结合密度泛函理论和半经典蒙特卡洛方法，系统研究了外延应变如何通过调控磁交换耦合各向异性来驱动磁相变，以BiFeO₃为模型体系揭示了应变控制磁性材料的设计原理。


<details>
  <summary>Details</summary>
Motivation: 探索外延应变作为非化学调控手段，如何通过操纵自旋、电荷和晶格自由度之间的耦合来调控功能材料的磁性，特别是理解应变如何驱动磁相变并控制竞争磁相。

Method: 采用双管齐下的方法：1) 使用密度泛函理论(DFT)计算BiFeO₃模型体系，分析应变引起的结构畸变如何导致磁交换耦合各向异性；2) 基于DFT结果构建三维半满单带Hubbard模型，通过各向异性最近邻和次近邻跳跃过程引入应变，采用半经典蒙特卡洛(s-MC)方法在非微扰区域构建基态相图。

Result: DFT计算显示外延应变导致磁交换耦合各向异性，c轴与ab平面交换耦合不同，从立方到四方晶格的微妙结构畸变驱动了从G型到C型反铁磁相的转变。s-MC相图表明：压缩应变驱动从G型到C型反铁磁绝缘体的转变，而拉伸应变抑制C型反铁磁序，有利于A型反铁磁相的形成。

Conclusion: 应变是通过调控关联体系中交换耦合机制来控制竞争磁相的有力调谐参数，为应变控制材料的设计提供了重要见解，展示了结构畸变与磁序变化之间的直接联系。

Abstract: Epitaxial strain provides a powerful, non-chemical route to tune the properties of functional materials by manipulating the coupling between spin, charge, and lattice degrees of freedom. Using density functional theory (DFT) calculations and $\rm BiFeO_3$ as a model system, we first demonstrate how epitaxial strain exactly leads to anisotropic magnetic interactions where the exchange coupling along the $c$-axis differs from that in the $ab$-plane. We show that subtle structural modifications, specifically the distortion from a cubic to a tetragonal lattice, drive a magnetic phase transition from a G-type to a C-type antiferromagnetic (AF) phase. The anisotropy in magnetic interactions, which becomes prominent in the lower symmetry tetragonal phase, provides a direct link between the structural distortion and the potential change in magnetic ordering. For a more comprehensive study, we next investigate the role of strain in driving magnetic phase transitions within a half-filled one-band Hubbard model in three dimensions. In this framework, strain is introduced through anisotropic hopping processes between nearest- and next-nearest-neighbor sites, inspired by the DFT calculations. Using a semiclassical Monte Carlo (s-MC) approach, we construct ground state phase diagrams in the nonperturbative regime, which show how uniaxial strain stabilizes distinct magnetic ground states: Compressive strain drives a transition from a G-type to a C-type AF insulator, whereas tensile strain suppresses the C-type AF order, favoring an A-type AF phase. Overall, our combined DFT and s-MC calculations highlight that strain is a powerful tuning parameter for controlling competing magnetic phases by governing exchange coupling mechanisms in correlated systems, offering valuable insights for the design of strain-controlled materials.

</details>


### [172] [Two-dimensional coherent spectroscopy of CoNb$_2$O$_6$](https://arxiv.org/abs/2512.16829)
*Yoshito Watanabe,Simon Trebst,Ciarán Hickey*

Main category: cond-mat.str-el

TL;DR: 该研究通过二维相干光谱理论分析CoNb₂O₆量子磁体，揭示了自旋子解禁闭的明确特征，为太赫兹二维相干光谱实验提供了具体预测。


<details>
  <summary>Details</summary>
Motivation: 随着太赫兹技术的发展，二维相干光谱已能探测meV能区的非线性响应，这为研究磁材料中的准粒子激发提供了新途径。然而，目前实验主要关注经典磁体，在量子磁体中的实际应用尚未建立。本研究旨在通过理论分析CoNb₂O₆这一准一维伊辛磁体，探索其分数化自旋子的二维相干光谱特征。

Method: 研究基于有效的S=1/2哈密顿量，采用四自旋子近似方法，从精确可解的一维横向场伊辛模型出发，逐步添加相互作用来构建CoNb₂O₆的微观模型。特别地，在1d-TFIM基础上添加键依赖的交错YZ相互作用，形成了TFIM+YZ模型，该模型能够再现完整材料哈密顿量的关键光谱特征。

Result: 研究发现了一系列束缚态，包括一个与常见的双自旋子束缚态不同的四自旋子束缚态。更重要的是，研究揭示了在低温有序相之上存在自旋子解禁闭的明确二维相干光谱特征。当引入禁闭势时，二维频率空间中的尖锐自旋子回波特征会被抑制，这些特征被认为反映了分数化激发的底层连续谱。

Conclusion: 该研究为未来在CoNb₂O₆及相关准一维量子磁体上进行太赫兹二维相干光谱实验提供了具体的理论预测和明确的目标，有望揭示传统线性响应探针无法探测的多体现象。

Abstract: With recent advances in terahertz (THz) sources and detection, two-dimensional coherent spectroscopy (2DCS), which allows to probe nonlinear responses in a two-frequency plane, now reaches the meV regime relevant for quasiparticle excitations in magnetic materials. This opens a promising route to reveal many-body phenomena that evade linear-response probes. To date most experimental applications have focused on classical magnets, and a solid demonstration in a quantum magnet has yet to be established. Here we present a theoretical study of 2DCS in CoNb$_2$O$_6$, a quasi-one-dimensional Ising magnet that is believed to host fractionalized spinons which at low temperatures are confined by weak interchain coupling. Our analysis, which builds on an effective $S=1/2$ Hamiltonian is found to reveal unambiguous 2DCS signatures of spinon deconfinement above the low-temperature ordered phase. Using a four-spinon approximation, we track these 2DCS signatures by sequentially building a faithful microscopic model for CoNb$_2$O$_6$, starting from the exactly solvable one-dimensional transverse-field Ising model (1$d$ TFIM) and successively adding interactions to capture its key low-energy physics. In particular, adding a bond-dependent staggered YZ interaction to the 1$d$-TFIM already reproduces many key spectral features of the full material Hamiltonian. Within this TFIM+YZ model, we find a series of bound states, including a four-spinon bound state that is distinct from the familiar two-spinon bound states. We further find that introducing a confinement potential suppresses sharp spinon-echo features in the two-frequency space, which are thought to reflect an underlying continuum of fractionalized excitations. Our results provide concrete predictions and clear targets for future THz 2DCS experiments on CoNb$_2$O$_6$ and related quasi-one-dimensional quantum magnets.

</details>


### [173] [Optimal array geometries for kinetic magnetism and Nagaoka polarons](https://arxiv.org/abs/2512.16834)
*N. Hernandez-Cepeda,Sergio E. Ulloa*

Main category: cond-mat.str-el

TL;DR: 该研究通过图论分析量子点阵列的连通性特征，揭示了Nagaoka铁磁性与代数连通度、Katz中心性的关系，并发现磁场通量可以破坏铁磁性甚至产生反铁磁态。


<details>
  <summary>Details</summary>
Motivation: 虽然量子点平台已能观测Nagaoka铁磁性，但优化集群连通性特征以获得最大自旋基态及其对磁场的鲁棒性尚未探索。研究旨在建立连通性特征与铁磁性之间的定量关系。

Method: 采用哈伯德哈密顿量的精确对角化方法，结合图论分析，使用代数连通度(λ₂)和Katz中心性(KC)描述系统连通性，研究磁场通量对系统的影响。

Result: 发现NFM起始点与代数连通度的平方相关(t_c/U≃λ₂²)，最优几何中大的λ₂和低的KC波动增强t_c/U并扩展NFM相。垂直磁场引入Aharonov-Bohm相位，临界通量会破坏NFM，π通量相位产生反铁磁关联的基态（反Nagaoka态）。

Conclusion: NFM和极化子形成可以通过阵列连通性(λ₂和KC)预测，磁场通量的引入会反直觉地破坏系统中的动力学铁磁性，为量子点阵列的设计和控制提供了理论指导。

Abstract: Quantum dot (QD) platforms have enabled the direct observation of Nagaoka ferromagnetism (NFM) in small arrays and non-infinite interaction strength. However, optimizing the cluster connectivity characteristics that yield a ground state with maximal spin and their robustness against magnetic fields remains unexplored. Employing exact diagonalization of the Hubbard Hamiltonian, we find a connection between the existence of kinetic ferromagnetism and graph theory descriptions. Algebraic connectivity ($λ_2$) and Katz centrality (KC) are shown to be related to the spin-correlation over the system. In square arrays, the onset of NFM is found to be $t_c/U\simeq λ_2^2$. In optimal cluster geometries, large $λ_2$ and low KC fluctuation per site are found to enhance $t_c/U$, extending the NFM phase while diminishing the strength of spin correlation clouds. A perpendicular magnetic field introduces Aharonov-Bohm phases, and a critical flux for which NFM is destroyed. We further find that tuning the flux phase to $π$ results in a ground state that exhibits antiferromagnetic correlations (counter-Nagaoka state). Our results illustrate how NFM and polaron formation can be predicted from the array's connectivity ($λ_2$ and KC), and how the introduction of flux results in the counterintuitive destruction of kinetic ferromagnetism in the system.

</details>


### [174] [Revival Dynamics from Equilibrium States: Scars from Chords in SYK](https://arxiv.org/abs/2512.16836)
*Debarghya Chakraborty,Dario Rosa*

Main category: cond-mat.str-el

TL;DR: 提出在二分系统中构建量子多体疤痕态的新框架，通过Krylov构造支持等间距能级塔，实现有限时间复苏动力学


<details>
  <summary>Details</summary>
Motivation: 开发一种新框架来构建具有完美相关性的二分系统中的量子多体疤痕态，研究其动力学特性

Method: 使用Krylov构造方法构建相互作用项，支持等间距能量本征态塔；在双标度SYK模型的两侧弦态中近似实现该框架

Result: 系统在一般平衡态的纯化初始化时出现有限时间复苏；动力学具有普适性，与各分区哈密顿量具体细节基本无关；对局域在单个SYK拷贝谱上的波包发现刚性运动

Conclusion: 该框架成功构建了二分系统中的量子多体疤痕态，数值模拟与理论预测高度一致，为研究量子多体疤痕动力学提供了新途径

Abstract: We develop a novel framework to build quantum many-body scar states in bipartite systems characterized by perfect correlation between the Hamiltonians governing the two sides. By means of a Krylov construction, we build an interaction term which supports a tower of equally-spaced energy eigenstates. This gives rise to finite-time revivals whenever the system is initialized in a purification of a generic equilibrium state. The dynamics is universally characterized, and is largely independent of the specific details of the Hamiltonians defining the individual partitions. By considering the two-sided chord states of the double-scaled SYK model, we find an approximate realization of this framework. We analytically study the revival dynamics, finding rigid motion for wavepackets localized on the spectrum of a single SYK copy. These findings are tested numerically for systems of finite size, showing excellent agreement with the analytical predictions.

</details>


### [175] [Wiedemann-Franz violation and thermal Hall effect in kagome metal TbCr6Ge6](https://arxiv.org/abs/2512.16868)
*Jhinkyu Choi,Mohan B. Neupane,L. H. Vilela-Leão,Bishnu P. Belbase,Arjun Unnikrishnan,Syeda Neha Zaidi,Jukka I. Väyrynen,Arnab Banerjee*

Main category: cond-mat.str-el

TL;DR: 该研究通过Wiedemann-Franz定律框架研究kagome金属TbCr6Ge6的热霍尔效应，发现铁磁转变导致热-电荷输运解耦，洛伦兹比显著偏离理论值，表明电荷中性激发的重要贡献。


<details>
  <summary>Details</summary>
Motivation: 热霍尔效应已成为探测量子材料中奇异激发的重要工具，但需要直接研究kagome金属中热与电荷的响应关系，以理解电荷中性载流子的作用。

Method: 使用Wiedemann-Franz定律框架研究铁磁性稀土1-6-6化合物TbCr6Ge6，测量纵向和横向洛伦兹比L_{xx,xy} = κ_{xx,xy} / (T σ_{xx,xy})，分析其随温度和磁场的变化。

Result: 在铁磁转变处观察到Wiedemann-Franz定律的显著破坏，洛伦兹比强烈偏离Sommerfeld值L_0；低温下L_{xx}和L_{xy}均被抑制，横向洛伦兹比出现符号变化，表明热与电荷输运解耦。

Conclusion: TbCr6Ge6提供了一个可调控的金属平台，其中交换驱动的铁磁性控制纵向和横向热响应，使Wiedemann-Franz行为在实验可及的温度和磁场范围内发生可控偏离，揭示了电荷中性激发的重要作用。

Abstract: The thermal Hall effect has emerged as a powerful probe of exotic excitations in correlated quantum materials, providing access to charge-neutral heat carriers that remain invisible to electrical transport. To directly examine how heat and charge respond in relation within a kagome metal, we investigate the ferrimagnetic rare-earth 1-6-6 compound TbCr6Ge6 using the Wiedemann-Franz (WF) framework. We observe a dramatic breakdown of the WF law across the ferrimagnetic transition, where both longitudinal and transverse Lorenz ratios, L_{xx,xy} = κ_{xx,xy} / (T σ_{xx,xy}), deviate strongly from the Sommerfeld value L_0. After a partial recovery toward L_0 near 5-7 K, the Lorenz ratios are sharply suppressed well below L_0 despite a metallic charge response. We further find a pronounced low-temperature suppression of both L_{xx} and L_{xy} and a sign-changing transverse Lorenz ratio, indicating a clear decoupling between heat and charge transport and signaling substantial contributions from charge-neutral excitations whose Berry-curvature-driven transverse response evolves with temperature and magnetic field. TbCr6Ge6 thus provides a tunable metallic platform in which exchange-driven ferrimagnetism governs both longitudinal and transverse thermal responses, enabling controlled departures from Wiedemann-Franz behavior over an experimentally accessible temperature and field range.

</details>


### [176] [An exciton interacting with the phonons of an electronic Wigner crystal](https://arxiv.org/abs/2512.16888)
*Jens Havgaard Nyhegn,Esben Rohan Christensen,Georg M. Bruun*

Main category: cond-mat.str-el

TL;DR: 该研究开发了激子与维格纳晶体电子相互作用的场论描述，包括激子-声子耦合，揭示了激子运动导致两种散射过程，形成激子-声子极化子，其能量偏移和阻尼受电子密度非平凡影响。


<details>
  <summary>Details</summary>
Motivation: 二维维格纳晶体已被观测到，其关键信号是光学激子光谱中出现由静态维格纳晶体周期性势产生的倒逆分支。然而维格纳晶体的振动会产生无能隙声子谱，可能影响激子谱，因此需要研究激子与维格纳晶体电子相互作用，包括激子-声子耦合的影响。

Method: 开发了激子与形成维格纳晶体的电子相互作用的场论描述，包括激子-声子耦合。使用非微扰自洽玻恩近似分析激子运动导致的两种散射过程：激子发射声子但保持在相同布洛赫带内（带内散射）或改变其带（带间散射）。

Result: 激子-声子耦合的重要性随激子-电子相互作用强度相对于典型声子能量平方的比例而变化。散射过程导致形成极化子（由布洛赫态激子与维格纳晶体声子组成）。极化子的能量偏移和阻尼以非平凡方式依赖于电子密度，阻尼特别受极化子能量是否在无能隙声子散射连续谱内的影响。

Conclusion: 激子-声子耦合对观测到的激子光谱性质有重要影响，通过形成极化子并改变其能量和阻尼特性，这些效应最终会影响维格纳晶体的光谱观测信号。

Abstract: With the advent of atomically thin and tunable van der Waals materials, a two-dimensional electronic Wigner crystal has recently been observed. The smoking gun signal was the appearance of an umklapp branch in optical exciton spectroscopy coming from the periodic potential generated by the Wigner crystal assumed to be static. Vibrations of the Wigner crystal however leads to a gapless phonon spectrum, which may affect the exciton spectrum. To explore this, we develop a field theoretical description of an exciton interacting with electrons forming a Wigner crystal including the coupling to the phonons. We show that importance of the exciton-phonon coupling scales with the exciton-electron interaction strength relative to the typical phonon energy squared. The motion of the exciton leads to two kinds of scattering processes, where the exciton emits a phonon either staying within the same Bloch band (intraband scattering) or changing its band (interband scattering). Using a non-perturbative self-consistent Born approximation, we demonstrate that these scattering processes lead to the formation of quasiparticles (polarons) consisting of the exciton in Bloch states dressed by Wigner crystal phonons. The energy shift and damping of these polarons depend on the electron density in a non-trivial way since it affects both the exciton-phonon interaction strength, as well as the phonon and exciton spectra. In particular, the damping is strongly affected by whether the polaron energy is inside the gapless phonon scattering continuum or not. Using these results, we finally analyse their effects on the observed spectral properties of the exciton.

</details>
